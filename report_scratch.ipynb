{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b59f34",
   "metadata": {},
   "source": [
    "If you want to run the experiments, change this parameter to the amount of samples you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6087f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES: int | None = 100  # value <= 7500; None defaults to 7500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982bd36",
   "metadata": {},
   "source": [
    "todo: \n",
    "- mettere che usiamo 1 step di learning per tpt e 1 per tnt, come da paper\n",
    "- gli iperparametri son oquelli dei paper, for fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a80678",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we focus on **Test-Time Adaptation (TTA)**, which has recently gained traction due to its ability to enhance model performance without requiring access to training data.\n",
    "\n",
    "In this project, we focus on **TTA for image classification**, particularly using **CLIP** [[2](#ref-clip2021)] with **TPT** [[3](#ref-tpt2022)]. Our approach involves adapting the model on **single-image test instances**, with the model being reset to its pre-trained state after each instance. This resembles **TTIA**, keeping the constraint of no retention of prior test-time knowledge (between batches, so between images).\n",
    "\n",
    "<!--- visualize image using html formatting, so that i can scale it properly -->\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"img/tpt.png\" alt=\"Test-Time Prompt Tuning (TPT) for CLIP\" title=\"Test-Time Prompt Tuning (TPT) for CLIP\" width=\"600\" class=\"center\"/>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ce17b",
   "metadata": {},
   "source": [
    "## A. TTIA\n",
    "\n",
    "> **Definition**: \"_Test-Time Instance Adaption, TTIA_ Given a classifier $f_\\mathcal{S}$ learned on the source domain $\\mathcal{D_s}$, and an unlabeled target instance $x_t \\in \\mathcal{D_T}$ under distribution shift, _test-time instance adaption_ aims to leverage the labeled knowledge implied in $\\mathcal{f_S}$ to infer the label of $x_t$ adaptively\" [[1](#ref-liang2025)]. In other words, TTIA aims to adapt the classifier $f_\\mathcal{S}$ to the target instance $x_t$ by leveraging the knowledge of the source domain $\\mathcal{D_S}$. [[1](#ref-liang2025)]\n",
    "\n",
    "TTIA differs from TTBA in that single-instance adaption is performed, instead of batch-wise adaption, giving an example the difference is between classifying a single frame of a video and classifying a sequence of frames. In both methods no memory of the previous test-time knowledge is retained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8fb59",
   "metadata": {},
   "source": [
    "## B. Project Overview\n",
    "\n",
    "We aim to reproduce TPT results on ImageNetA.\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "1. Introduction\n",
    "1. Baselines - ZeroShot CLIP\n",
    "1. Reproducing TPT\n",
    "   - Reproduce TPT + simplified CoOp (without pretraining) (**Our contribution**)\n",
    "   - Using OpenAI weights and OpenCLIP implementation\n",
    "     - Compare zero-shot CLIP OpenAI (weights and implementation) with OpenCLIP (weights and implementation)\n",
    "   - Using `Kornia` instead of `AugMix` / `torchvision.transforms` (**Extra: Our contribution**)\n",
    "     - Recreate the AugMix pipeline in Kornia\n",
    "     - Kornia is faster and can directly run on the GPU\n",
    "     - Benchmarking the difference\n",
    "1. Trying to get better at TTA (**Our contribution**)\n",
    "   - A. Top 10%\n",
    "   - B. TPT with Top 10%\n",
    "   - D. TNT (Recreate the paper)\n",
    "   - E. TNT with Top 10%\n",
    "   - F. Image Masking\n",
    "   - G. PCA Guided Image Augmentation \n",
    "1. Reproducing TPT + CoOp with full CoOp pretraining (on ImageNetV2)\n",
    "   - Using OpenAI CLIP (both implementation and weights)\n",
    "   - So that we can compare it with TPT + CoOp without pretraining (`a photo of a` initialization).\n",
    "1. Results and Conclusion\n",
    "1. Future Work\n",
    "1. Bibliography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53528b02",
   "metadata": {},
   "source": [
    "## C. Reproducibility\n",
    "\n",
    "The project is designed to be reproducible. Code is also available on [GitHub]() as standard `python` files.\n",
    "\n",
    "For reproducibility seeding is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3f986",
   "metadata": {},
   "source": [
    "## D. Data\n",
    "\n",
    "Get datasets data, create datasets and dataloader. Seeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49384ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "# TODO: add packages\n",
    "%pip install torch==2.3.0 notebook==7.1.3 torchvision==0.18.0 matplotlib==3.8.4 openai-clip==1.0.1 kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efcaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "\n",
    "# Get datasets (ImageNet-A and ImageNetV2)\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n",
    "\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1TR1hrs9tV6rh_W-hDqRv6jH3KE-Hxsw2/view?usp=sharing\n",
    "\n",
    "!tar -xvf imagenetv2-matched-frequency-format-val.tar.gz -C datasets\n",
    "# json metadata of the datasets\n",
    "!curl https://raw.githubusercontent.com/modestyachts/ImageNetV2/refs/heads/master/data/metadata/class_info.json -o datasets/imagenetv2-matched-frequency-format-val/class_info.json\n",
    "\n",
    "!tar -xvf imagenet-a.tar -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ettore/.conda/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import json\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "import torch.nn.functional as F\n",
    "from open_clip.transformer import text_global_pool\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import open_clip\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4da38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding and reproducibility\n",
    "\n",
    "torch.manual_seed(456)\n",
    "torch.cuda.manual_seed(456)\n",
    "torch.randn(456).to(\"cuda\")\n",
    "np.random.seed(42)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "# https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bb8ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c8ab23",
   "metadata": {},
   "source": [
    "# 2. Baselines - ZeroShot CLIP\n",
    "\n",
    "Tested baselines are with both OpenAI CLIP (OpenAI weights) and OpenCLIP (OpenAI weights). This is because OpenAI CLIP cannot be profiled using [Scalene](https://github.com/plasma-umass/scalene) because of a bug with `torch.jit` ([ISSUE](https://github.com/plasma-umass/scalene/issues/908#issuecomment-2933035958)). Profiling was mostly done to improve the performance of the code.\n",
    "\n",
    "Kornia was used to try reduce latency in the image augmentation step, as it is a GPU-accelerated library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f4ef3",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader\n",
    "\n",
    "The dataset was made so that images are loaded directly as tensors and the image augmentation is done with Kornia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f0da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the ImageNet-A dataset.\n",
    "\n",
    "    ----\n",
    "\n",
    "    The dataset is organized into subdirectories, each named with a class code (e.g., \"n01614925\").\n",
    "    Each subdirectory contains images belonging to that class. The dataset also includes a README.txt file that maps class codes to human-readable names.\n",
    "\n",
    "    The dataset is expected to be structured as follows:\n",
    "    ```\n",
    "    datasets/imagenet-a/\n",
    "        n01440764/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        n01614925/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        ...\n",
    "        README.txt\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir=\"datasets/imagenet-a\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__download_if_needed()\n",
    "\n",
    "        # Load mapping from class codes (e.g., \"n01614925\") to human-readable names\n",
    "        readme_path = os.path.join(root_dir, \"README.txt\")\n",
    "        self.class_code_to_label = self._load_class_mapping(readme_path)\n",
    "\n",
    "        # Filter valid class directories that match the mapping\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_label\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Map class codes to indices\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image file paths and corresponding labels\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "        # Inverse mapping from label index to class name\n",
    "        self.idx_to_label = {\n",
    "            idx: self.class_code_to_label[code]\n",
    "            for code, idx in self.class_code_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def __download_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if the dataset is already downloaded. If not, download it.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found at {self.root_dir}. Please download it first.\"\n",
    "            )\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        \"\"\"\n",
    "        Load class code to human-readable name mapping from README.txt.\n",
    "        Skips header lines and parses lines in format: 'n01440764 tench'.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as file:\n",
    "            lines = file.readlines()[12:]  # Skip first 12 header lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        \"\"\"\n",
    "        Walk through each class directory to gather image paths and corresponding labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_dir = os.path.join(self.root_dir, class_code)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    samples.append((image_path, label))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load image and return dictionary containing image, label index, and class name.\n",
    "\n",
    "        Returns:\n",
    "            image (tensor)\n",
    "            label (tensor)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path).float() / 255.0\n",
    "\n",
    "        if image.shape[0] == 1:  # Grayscale → RGB\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        elif image.shape[0] == 4:  # RGBA → RGB\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        elif image.shape[0] != 3:\n",
    "            raise ValueError(f\"Unsupported number of channels: {image.shape[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"\n",
    "        Get human-readable class name for a given index.\n",
    "        \"\"\"\n",
    "        return self.idx_to_label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3e23a",
   "metadata": {},
   "source": [
    "## Faster \"AugMix\"\n",
    "\n",
    "TODO: add motivation and benchmark\n",
    "\n",
    "TPT and other TTA techniques, as already seen, use image augmentation to improve the performance of the model. Usually torchvision's AugMix [[18](#ref-augmix)] is used, but it's quite slow. For this reason it has been rewritten using kornia [[19](#ref-kornia)] which is more efficient and can be more easily customized and extended. It follows it's implementation and a small performance test to compare it with the original one.\n",
    "\n",
    "The usage of Kornia for AugMix is a way to speed up the process, as it's differentiable its gradients are suppressed. Performance wise performing random augmentation on images is not \"well parallelizable\" on a GPU, still some performance improvements can be achieved and the data can be kept on the GPU, avoiding _useless_ data transfers. Better performances when performing e.g. random cropping, resizing are expected.\n",
    "\n",
    "1. Kornia performances are generally better than torchvision's, as can be seen in [Kornia's Benchmark](https://kornia.readthedocs.io/en/stable/augmentation.html#benchmark).\n",
    "1. It has been noted that there are training [performance (model wise, accuracy) issues](https://discuss.pytorch.org/t/significant-difference-in-performance-between-torchvision-and-kornia-image-augmentations/97596) when comparing Kornia and torchvision, this is not covered in this project.\n",
    "1. Kornia's AugMix is not that well optimized as new memory allocation is performed on the device, instead of reusing a pre-allocated tensor. This can be done for future work, but it is not a priority for this project.\n",
    "1. Kornia operatoins are differentiable, so the gradients are propagated through the augmentations, which is not needed for TTA. Of course the _transformation_ class (`ImageTransform`) has a `torch.no_grad()` to avoid having unwanted gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459e067",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f030102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made to compare AugMix efficiency with Kornia.\n",
    "# Note that TPT says that uses AugMix, but in reality it uses Random Crop. Even tho AugMix was used\n",
    "# and discarded later.\n",
    "class AugMixKornia(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        severity: int = 3,\n",
    "        width: int = 3,\n",
    "        depth: int = -1,\n",
    "        alpha: float = 1.0,\n",
    "        mixture_width: int = 3,\n",
    "        chain_depth: int = 3,\n",
    "        all_ops: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AugMix implementation using Kornia with closer fidelity to the original paper.\n",
    "\n",
    "        Args:\n",
    "            severity: Severity level of augmentations (1-10)\n",
    "            width: Width of augmentation chain (not used directly, kept for compatibility)\n",
    "            depth: Depth of augmentation chain (-1 for random between 1-3)\n",
    "            alpha: Dirichlet distribution parameter for mixing weights\n",
    "            mixture_width: Number of augmentation chains to mix\n",
    "            chain_depth: Number of operations in each chain\n",
    "            all_ops: Whether to use all augmentation operations\n",
    "            device: Device to run on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.severity = severity\n",
    "        self.alpha = alpha\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth if depth <= 0 else depth\n",
    "        self.all_ops = all_ops\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define augmentation operations\n",
    "        self.augmentations = self._get_augmentations()\n",
    "\n",
    "    def _get_augmentations(self) -> List[nn.Module]:\n",
    "        \"\"\"Create a list of augmentation operations that will be randomly applied\"\"\"\n",
    "        severity_factor = self.severity / 10.0\n",
    "\n",
    "        if self.all_ops:\n",
    "            # Full set of augmentations similar to original AugMix\n",
    "            return [\n",
    "                # AutoContrast\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                # Equalize\n",
    "                Ke.equalize,\n",
    "                # Posterize\n",
    "                K.RandomPosterize(bits=max(1, 8 - self.severity), p=1.0),\n",
    "                # Rotate\n",
    "                K.RandomRotation(\n",
    "                    degrees=(-30 * severity_factor, 30 * severity_factor), p=1.0\n",
    "                ),\n",
    "                # Solarize\n",
    "                K.RandomSolarize(\n",
    "                    thresholds=0.5, additions=(0.0, 0.1 * self.severity), p=1.0\n",
    "                ),\n",
    "                # Shear\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    shear=(-15 * severity_factor, 15 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # Translate\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.1 * severity_factor, 0.1 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # ColorJitter\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity,\n",
    "                    contrast=0.1 * self.severity,\n",
    "                    saturation=0.1 * self.severity,\n",
    "                    hue=0.1,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Simplified version\n",
    "            return [\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                Ke.equalize,\n",
    "                K.RandomAffine(\n",
    "                    degrees=(-15 * severity_factor, 15 * severity_factor), p=1.0\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _apply_augmentation_chain(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a random sequence of augmentations to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented image tensor (C, H, W)\n",
    "        \"\"\"\n",
    "        # Randomly select augmentations for this chain\n",
    "        op_indices = np.random.choice(\n",
    "            len(self.augmentations), size=self.chain_depth, replace=True\n",
    "        )\n",
    "\n",
    "        augmented = image  # Don't clone immediately\n",
    "        for op_idx in op_indices:\n",
    "            augmented = self.augmentations[op_idx](augmented)\n",
    "\n",
    "        return augmented.squeeze(0)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply AugMix to a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: Input batch of images (B, C, H, W) or (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented batch (same shape as input)\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            # Input validation\n",
    "            if not isinstance(images, torch.Tensor):\n",
    "                images = K.image_to_tensor(images)\n",
    "\n",
    "            if images.dim() == 3:\n",
    "                images = images.unsqueeze(0)\n",
    "\n",
    "            # Move to device if needed\n",
    "            if images.device != self.device:\n",
    "                images = images.to(self.device)\n",
    "\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            # Sample mixing weights from Dirichlet distribution\n",
    "            weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet(\n",
    "                        [self.alpha] * self.mixture_width, size=batch_size\n",
    "                    )\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, mixture_width)\n",
    "\n",
    "            # Sample weights for mixing with original\n",
    "            mix_weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet([self.alpha, self.alpha], size=batch_size)\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, 2)\n",
    "\n",
    "            # Generate augmented versions for each mixture component\n",
    "            # Pre-allocate memory for augmented versions\n",
    "            augmented = torch.empty(\n",
    "                (self.mixture_width, batch_size, *images.shape[1:]), device=self.device\n",
    "            )\n",
    "\n",
    "            for i in range(self.mixture_width):\n",
    "                augmented[i] = self._apply_augmentation_chain(images)\n",
    "\n",
    "            # Weighted sum of augmented versions\n",
    "            mixed = torch.einsum(\"mbchw,bm->bchw\", augmented, weights).to(self.device)\n",
    "\n",
    "            # Final mix with original image\n",
    "            result = (\n",
    "                mix_weights[:, 0:1, None, None] * images\n",
    "                + mix_weights[:, 1:2, None, None] * mixed\n",
    "            )\n",
    "\n",
    "            result = result.squeeze(0) if result.shape[0] == 1 else result\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Simple, yet effective, rancom crop using Kornia.\n",
    "def kornia_random_crop(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies random crop to a batch of images using Kornia's RandomResizedCrop.\n",
    "    Preserves the original image size while randomly cropping a portion.\n",
    "    \"\"\"\n",
    "    b, c, h, w = images.shape\n",
    "\n",
    "    # Create random crop transform that:\n",
    "    # 1. Crops between 50% and 100% of original area\n",
    "    # 2. Maintains original aspect ratio\n",
    "    # 3. Resizes back to original dimensions\n",
    "    transform = K.RandomResizedCrop(\n",
    "        size=(h, w),\n",
    "        # scale=(0.5, 1.0),  # Crop between 50% and 100% of original area\n",
    "        # ratio=(1.0, 1.0),  # Maintain original aspect ratio\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "        same_on_batch=False,  # Different crop for each image in batch\n",
    "    )\n",
    "\n",
    "    return transform(images)\n",
    "\n",
    "\n",
    "# Preprocessing pipeline using Kornia for CLIP models.\n",
    "# mean and std are extracted from the `preprocess` of ViT-B/16 model (OpenAI weights).\n",
    "# Note that mean and std are the same also for ViT-B/32.\n",
    "kornia_preprocess = nn.Sequential(\n",
    "    K.SmallestMaxSize(\n",
    "        224,\n",
    "        resample=kornia.constants.Resample.BICUBIC,  # type:ignore\n",
    "    ),\n",
    "    K.CenterCrop(\n",
    "        size=(224, 224),\n",
    "        resample=kornia.constants.Resample.BICUBIC,  # type:ignore\n",
    "    ),\n",
    "    kornia.enhance.Normalize(\n",
    "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]),\n",
    "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b1d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper for image transformations\n",
    "# The custom transformation can be either AugMixKornia or kornia_random_crop, or any\n",
    "# function (or class with __call__ method) with the same signature.\n",
    "class ImageTransform(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_transform, custom_transform=None, n_views=63, device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_transform = model_transform\n",
    "        self.custom_transform = custom_transform\n",
    "        self.n_views = n_views\n",
    "        self.device = device\n",
    "\n",
    "        self.eval()\n",
    "        # self.model_transform.eval()\n",
    "        # self.custom_transform.eval() if custom_transform is not None else None\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model transform and custom transform to the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            image = image.to(self.device)\n",
    "\n",
    "            if self.custom_transform is not None:\n",
    "                with torch.no_grad():\n",
    "                    views = torch.empty(\n",
    "                        (self.n_views + 1, *image.shape), device=self.device\n",
    "                    )\n",
    "                    views[:-1] = self.custom_transform(\n",
    "                        image.repeat(self.n_views, 1, 1, 1)\n",
    "                    )\n",
    "                    views[-1] = image\n",
    "                    return self.model_transform(views)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    return self.model_transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a364b",
   "metadata": {},
   "source": [
    "## AugMix and AugMixKornia Comparison\n",
    "\n",
    "Simple benchmark to compare the performance of the two implementations of AugMix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b2c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "image = torchvision.io.read_image(\n",
    "    \"datasets/imagenet-a/n01641577/0.038738_agama _ newt_0.7465035.jpg\"\n",
    ")\n",
    "\n",
    "n_times = 100\n",
    "n_augmentations = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae606954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:37<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision latency: 977.58 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Original AugMix (torchvision_\n",
    "# The `preprocess` comes from `clip.load(\"ViT-B/32\")`\n",
    "# The idea is to augment the same image `n` times (as if they were different batches) and get the average latency.\n",
    "# The code follows a straightforward approach to augment the image as done for TPT: [B, C, H, W], where B is\n",
    "# composed by `n` augmentation images and 1 original image.\n",
    "# Output for both methods is a tensor of shape [B, C, H, W] where B = n + 1.\n",
    "\n",
    "preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(\n",
    "            size=224,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BICUBIC,\n",
    "            max_size=None,\n",
    "            antialias=True,\n",
    "        ),\n",
    "        torchvision.transforms.CenterCrop(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "            std=(0.26862954, 0.26130258, 0.27577711),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmix = torchvision.transforms.AugMix()\n",
    "\n",
    "transform = torchvision.transforms.Compose([augmix, preprocess])\n",
    "\n",
    "# Compose needs PIL images, so we need to convert it\n",
    "# Image casting is not benched as it could easily be done in the dataloader\n",
    "# by reading the image with PIL instead of torch.\n",
    "pil_image = torchvision.transforms.functional.to_pil_image(image)  # type: ignore\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(n_times)):\n",
    "    transformed = [transform(pil_image) for _ in range(n_augmentations)]\n",
    "    transformed = torch.stack(transformed)\n",
    "end = time.time()\n",
    "\n",
    "torchvision_latency = (end - start) / n_times * 1000\n",
    "print(f\"torchvision latency: {torchvision_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce50c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:01<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency per image: 618.8325 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Timing could be done with cuda events, but for simplicity we use time.time()\n",
    "\n",
    "kornia_augmix = AugMixKornia()\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(n_times)):\n",
    "    images = image.float().div(255).repeat(63, 1, 1, 1)\n",
    "    views = kornia_augmix(images)\n",
    "    views = kornia_preprocess(views)\n",
    "end = time.time()\n",
    "\n",
    "kornia_latency = (end - start) / n_times * 1000\n",
    "print(f\"Latency per image: {kornia_latency:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde4288",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "**Table 1**: Performance comparison between torchvision and kornia implementations of AugMix. Test performed on a single NVIDIA RTX 4060 (140W, performance mode). This was tested also on CPU and the results were similar, thus showing that the performance difference is not due to the GPU, but rather to the implementation of the augmentations (and performing full AugMix on the GPU is, of course, not well parallelizable).\n",
    "\n",
    "It's worth noting that the performance of Kornia is better than torchvision's, as expected. The speedup is not huge, mostly because AugMix performs random operations on the images, which is not well parallelizable on a GPU (warp divergence) Still, the data is kept on the GPU, avoiding multiple data copies and transfers.\n",
    "\n",
    "E.g. augmenting with AugMix has the generation of the augmented images on the CPU, then data is copied to the GPU, so the copy is e.g. 64 images at once. While with Kornia only the original image is copied on the GPU, then augmentations are done on device. This inherently speeds up the process.\n",
    "\n",
    "Note: results vary a lot depending if it's running on a notebook or pure python. \\*notebooks are not reliable.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| **Implementation** | **Time (ms)** | **Speedup** |\n",
    "| :----------------: | :-----------: | :---------: |\n",
    "|    torchvision     |    742.37     |     1.0     |\n",
    "|       kornia       |    552.19     |    1.35     |\n",
    "|      $\\Delta$      |    190.18     |    0.35     |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47470fd6",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "The preprocessing done with Kornia needs to be the same as the one done with torchvision (`torchvision.transforms.Compose` saw above). This is verified by comparing the output of the two implementations.\n",
    "\n",
    "Warning: Differences are not noticable by eye, but they are there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ecdab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pil_image = torchvision.transforms.functional.to_pil_image(image)  # type: ignore\n",
    "\n",
    "preprocessed_torchvision = preprocess(pil_image)\n",
    "\n",
    "preprocessed_kornia = kornia_preprocess(image.float().div(255)).squeeze(0)\n",
    "\n",
    "# Visuaize both on the same plot and row\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n",
    "\n",
    "axes[0].imshow(preprocessed_torchvision.permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title(\"AugMix (torchvision)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(preprocessed_kornia.permute(1, 2, 0).cpu().numpy())\n",
    "axes[1].set_title(\"AugMix (kornia)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee85829",
   "metadata": {},
   "source": [
    "## Image-A Builder\n",
    "\n",
    "Build the dataset and dataloader with image-agumentation at data-loading time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40682281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImagenetA(\n",
    "    augmenter: ImageTransform,\n",
    "    root_dir=\"datasets/imagenet-a\",\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the ImageNet-A dataset. Defaults to 1 element per batch.\n",
    "    Non modifiable. No shuffling.\n",
    "    Args:\n",
    "        augmenter (callable):\n",
    "        root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (DataLoader): DataLoader for the ImageNet-A dataset.\n",
    "        dataset (ImageNetADataset): The underlying dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle the batch of images and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images = batch[0][0]\n",
    "\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        labels = batch[0][1]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    dataset = ImageNetADataset(root_dir=root_dir, transform=augmenter)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5cebbb",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "This is as simple benchmark function to easily and reliably test the performance of different TTA implementations. The idea is that images are fed in the same way to the model. The `forward` method of the model internally manages every detail of the TTA (image augmentation excluded).\n",
    "\n",
    "Timing is done with cuda events, only on the `forward` method as we are mostly interested in the inference time of the model, not the data loading/augmentation time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae12ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str,\n",
    "    comment: str,\n",
    "    reduce: Optional[int | None] = None,\n",
    "    visualize: Optional[bool] = False,\n",
    "):\n",
    "    \"\"\"Benchmark the model on the dataset.\n",
    "\n",
    "    The model must return logits.\n",
    "    \"\"\"\n",
    "    today_now = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "\n",
    "    board = SummaryWriter(log_dir=f\"runs/{today_now}/{comment}\")\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    times = []\n",
    "\n",
    "    total_tqdm = reduce if reduce is not None else len(dataloader)\n",
    "    # ░▒█\n",
    "    # ascii=\" ▖▘▝▗▚▞█\"\n",
    "    # ascii=' >='\n",
    "    start_event, end_event = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n",
    "        enable_timing=True\n",
    "    )\n",
    "    for image, label in tqdm(dataloader, total=total_tqdm, ascii=\" ▖▘▝▗▚▞█\"):\n",
    "        image = image.to(device)\n",
    "\n",
    "        start_event.record()  # type: ignore\n",
    "        pred_class = model(image)\n",
    "        end_event.record()  # type: ignore\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        times.append(start_event.elapsed_time(end_event))\n",
    "\n",
    "        # del image\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total += 1\n",
    "        correct += int((pred_class == label))\n",
    "\n",
    "        if reduce:\n",
    "            if total > reduce:\n",
    "                break\n",
    "\n",
    "        # break\n",
    "        board.add_scalar(\"accuracy\", correct / total, total)\n",
    "        board.add_scalar(\"dbg/label/predict_class\", pred_class, total)\n",
    "        board.add_scalar(\"dbg/label/label\", label, total)\n",
    "\n",
    "        running_accuracy = correct / total\n",
    "\n",
    "        if visualize:\n",
    "            print(f\"[{label} || {pred_class}] | Acc: [{running_accuracy*100:.2f}%]\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    latency = (np.array(times).sum() / total).item()  # ms\n",
    "\n",
    "    board.add_scalar(\"metrics/latency (ms)\", latency)\n",
    "    board.add_scalar(\"metrics/accuracy\", accuracy)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Latency: {latency:.2f} ms\")\n",
    "\n",
    "    return accuracy, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aeb05e",
   "metadata": {},
   "source": [
    "## Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfe6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_random_crop,\n",
    "    n_views=63,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "dataloader, dataset = ImagenetA(augmenter, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f2da2",
   "metadata": {},
   "source": [
    "## Baseline - ZeroShot CLIP (OpenAI implementation and weights)\n",
    "\n",
    "Simple baseline with CLIP using the OpenAI implementation and weights. The model is used in zero-shot: no training, no fine-tuning, no TTA. The model is used as it is, with a prompt of `a photo of a {class}` for each class in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88b879",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0b72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: clip.model.CLIP,  # type:ignore\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [clip.tokenize(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Returns the predicted class.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(x)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        return int(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22e73e",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.48%\n",
      "Latency: 182.67 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5247524752475248, 182.6725933717029)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model, _ = clip.load(\"ViT-B/16\", device=DEVICE, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=N_SAMPLES,\n",
    "    comment=\"baseline OpenAI\",\n",
    "    visualize=False,\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del clip_model\n",
    "del wrapper_clip\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d6fcd",
   "metadata": {},
   "source": [
    "## Baseline - ZeroShot CLIP (OpenCLIP implementation and OpenAI weights)\n",
    "\n",
    "Simple baseline with CLIP using OpenCLIP implementation and OpenAI weights. The model is used in zero-shot as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574c920",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2118de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        # Precompute text features\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "        return int(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528e0d7",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.47%\n",
      "Latency: 181.56 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    "    jit=False,\n",
    ")\n",
    "clip_model.eval()  #  type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  #  type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=N_SAMPLES,\n",
    "    comment=\"baseline OpenClip\",\n",
    "    visualize=False,\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "del clip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72efb6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4771e795",
   "metadata": {},
   "source": [
    "# 3. Reproducing TPT\n",
    "\n",
    "TPT + \"one-shot\" Coop inspiration\n",
    "\n",
    "The idea is to verify if TPT + CoOp with OpenCLIP implementation of CLIP and OpenAI weights and Kornia as image augmentator is equivalent to the original TPT + CoOp with OpenAI clip and weights and torchvision as image augmentator.\n",
    "\n",
    "Kornia was used to try reduce latency in the image augmentation step, as it generally faster than torchvision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed47cd",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "- `TPTPromptLearner`: a simple TPT prompt learner, which has a similar amount of learnable parameters as the original TPT's prompt learner, but with a simpler and more readable implementation. The idea is to simplify how the prompt learner is structured: instead of having the `class_token_position` that can be in the `end`, `middle` or `front` as in the original TPT + CoOp model, we simply split the prompt is `pre` and `post` prompts, which are then concatenated with the current class token (embedded ofc). One thing to note is that the prompt initialization, as seen in CoOp [[4](#ref-coop2021)], is done with \"a photo of a {}\", without any pre-training, as performances are close.\n",
    "- `TPTModel`: CLIP model with TPT+CoOp prompt learner.\n",
    "- `TPT`: awesome wrapper that makes possible to manage the finetuning and reset of the model after each image, it's \"invisible\" to the user and painless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75682192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: from 5_tpt.py\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: open_clip.model.CLIP,\n",
    "        arch: CLIPModels = CLIPModels.ViTB32,  # type: ignore\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer(arch)\n",
    "\n",
    "        self.__init_ctx_from_prompt(\n",
    "            tokenizer=tokenizer,\n",
    "            token_embedding=clip_model.token_embedding,\n",
    "            base_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    def __init_ctx_from_prompt(\n",
    "        self, tokenizer, token_embedding, base_prompt: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[tokenizer.sot_token_id]]).to(self.device)\n",
    "        c_token_eot = torch.tensor([[tokenizer.eot_token_id]]).to(self.device)\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = tokenizer(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = tokenizer(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = tokenizer(self.class_names).to(self.device)\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = tokenizer(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = token_embedding(c_token_sot)\n",
    "            self.embedded_eot = token_embedding(c_token_eot)\n",
    "            self.embedded_pad = token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            # [num_valid_tokens, 512]\n",
    "            clean_embed = embedded_class_names[i][mask[i]]\n",
    "            clean_embeddings.append(\n",
    "                clean_embed.unsqueeze(0)\n",
    "            )  # [1, num_valid_tokens, 512]\n",
    "\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "\n",
    "        for i, embed in enumerate(clean_embeddings):\n",
    "            self.register_buffer(f\"class_embed_{i}\", embed)\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)  # type: ignore\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)  # type: ignore\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # embeddeD_max_len: 77\n",
    "            # embedded_prefix: torch.Size([4, 512])\n",
    "            # embedded_class_names: torch.Size([1, 1, 512])\n",
    "            # embedded_suffix: torch.Size([0, 512]\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - getattr(self, f\"class_embed_{i}\").shape[1]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            ) - 2  # # -2 for SOT and EOT\n",
    "\n",
    "            # embedded sot shape: torch.Size([1, 1, 512])\n",
    "            # embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            # embedded class names shape: torch.Size([1, 1, 1, 512])\n",
    "            # embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            # embedded eot shape: torch.Size([1, 1, 512])\n",
    "            # effective padding shape: torch.Size([1, 70, 512])\n",
    "            # Prompt shape: torch.Size([1, 77, 512])\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    # self.embedded_class_names[i],\n",
    "                    getattr(self, f\"class_embed_{i}\"),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    self.embedded_pad.repeat(1, padding_size, 1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "        with torch.no_grad():\n",
    "            self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "            self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        arch: CLIPModels,\n",
    "        pretrained: str,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: open_clip.model.CLIP\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(  # type:ignore\n",
    "            model_name=arch,  # type:ignore\n",
    "            pretrained=pretrained,\n",
    "            device=device,\n",
    "            force_quick_gelu=True,\n",
    "        )\n",
    "\n",
    "        self.model = clip_model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)  # type:ignore\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.visual: open_clip.transformer.VisionTransformer = (  # type:ignore\n",
    "            clip_model.visual\n",
    "        )  # type:ignore\n",
    "        self.visual.eval()\n",
    "\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.attn_mask = clip_model.attn_mask\n",
    "        self.text_pool_type = clip_model.text_pool_type\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            arch=arch, class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "    def _pool(self, x: torch.Tensor):\n",
    "        if self.visual.attn_pool is not None:\n",
    "            if self.visual.attn_pool_contrastive is not None:\n",
    "                # This is untested, WIP pooling that should match paper\n",
    "                x = self.visual.ln_post(\n",
    "                    x\n",
    "                )  # TBD LN first or separate one after each pool?\n",
    "                tokens = self.visual.attn_pool(x)\n",
    "                if self.visual.attn_pool_type == \"parallel\":\n",
    "                    pooled = self.visual.attn_pool_contrastive(x)\n",
    "                else:\n",
    "                    assert self.visual.attn_pool_type == \"cascade\"\n",
    "                    pooled = self.visual.attn_pool_contrastive(tokens)\n",
    "            else:\n",
    "                # this is the original OpenCLIP CoCa setup, does not match paper\n",
    "                x = self.visual.attn_pool(x)\n",
    "                x = self.visual.ln_post(x)\n",
    "                pooled, tokens = self.visual._global_pool(x)\n",
    "        elif self.visual.final_ln_after_pool:\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "            pooled = self.visual.ln_post(pooled)\n",
    "        else:\n",
    "            x = self.visual.ln_post(x)\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "\n",
    "        return pooled, tokens, x\n",
    "\n",
    "    def _forward_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.visual._embeds(x)\n",
    "        x = self.visual.transformer(x)\n",
    "\n",
    "        pooled, tokens, x = self._pool(x)\n",
    "\n",
    "        if self.visual.proj is not None:\n",
    "            pooled = pooled @ self.visual.proj\n",
    "        if self.visual.output_tokens:\n",
    "            return pooled, tokens, x  # type:ignore\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def __encode_image(\n",
    "        self, image, normalize: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pooled_pre_norm = self._forward_image(image)\n",
    "        return (\n",
    "            F.normalize(pooled_pre_norm, dim=-1) if normalize else pooled_pre_norm\n",
    "        )  # type:ignore\n",
    "\n",
    "    def __encode_text(self, text=None, normalize: bool = False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.prompt_learner().to(cast_dtype)\n",
    "\n",
    "        text = self.prompt_learner.tokenized_initial_full_prompt\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = text_global_pool(x, text, self.text_pool_type)  # type:ignore\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                x = self.text_projection(x)\n",
    "            else:\n",
    "                x = x @ self.text_projection\n",
    "\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.__encode_image(image, normalize=True)\n",
    "        text_features = self.__encode_text(normalize=True)\n",
    "\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, image_features\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the prompt learner to its initial state.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.reset()\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "        model = TPTModel(\n",
    "            class_names=class_names,\n",
    "            arch=arch,\n",
    "            pretrained=pretrained,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # # # TEST - learnable layer norm\n",
    "        # self.model.visual.ln_post.requires_grad_(True)\n",
    "        # self.model.ln_final.requires_grad_(True)\n",
    "\n",
    "        # Get all trainable parameters (filter by requires_grad)\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # Initialize optimizer with trainable parameters\n",
    "        self.optim = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim_init = deepcopy(self.optim.state_dict())\n",
    "\n",
    "        # Initialize backup lists\n",
    "        self.ln_backup = {\n",
    "            \"weights\": [],  # For gamma (scale)\n",
    "            \"biases\": [],  # For beta (shift)\n",
    "        }\n",
    "\n",
    "        # Backup all LN params in text encoder\n",
    "        for block in self.model.transformer.resblocks:  # type:ignore\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_1.weight.data.detach().clone()\n",
    "            )  # gamma for ln_1\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_1.bias.data.detach().clone()\n",
    "            )  # beta for ln_1\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_2.weight.data.detach().clone()\n",
    "            )  # gamma for ln_2\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_2.bias.data.detach().clone()\n",
    "            )  # beta for ln_2\n",
    "\n",
    "        # Backup final LN\n",
    "        self.ln_backup[\"weights\"].append(\n",
    "            self.model.ln_final.weight.data.detach().clone()\n",
    "        )\n",
    "        self.ln_backup[\"biases\"].append(self.model.ln_final.bias.data.detach().clone())\n",
    "\n",
    "    def set_tta_steps(self, tta_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the number of TTA steps.\n",
    "\n",
    "        Args:\n",
    "            tta_steps (int): Number of TTA steps.\n",
    "        \"\"\"\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            # input = input[-1].unsqueeze(0)\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        return pred_class\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Full reset of prompt learner and optimizer state\"\"\"\n",
    "        # 1. Reset prompt embeddings\n",
    "        for p in self.model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        self.model.reset()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "        #     self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            # Reset LN params in text encoder\n",
    "            for block in self.model.transformer.resblocks:  # type:ignore\n",
    "                block.ln_1.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_1.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "                block.ln_2.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_2.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "\n",
    "            # # Reset final LN\n",
    "            self.model.ln_final.weight.data.copy_(self.ln_backup[\"weights\"][-1].clone())\n",
    "            self.model.ln_final.bias.data.copy_(self.ln_backup[\"biases\"][-1].clone())\n",
    "\n",
    "        # # 2. Reset optimizer state\n",
    "        self.optim.load_state_dict(deepcopy(self.optim_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6b1e",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4d07d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.38%\n",
      "Latency: 807.33 ms\n"
     ]
    }
   ],
   "source": [
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=N_SAMPLES, comment=\"TPT\", visualize=False\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bedaa1",
   "metadata": {},
   "source": [
    "# 4. Trying to get better at TTA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4ee7b",
   "metadata": {},
   "source": [
    "TPT with CoOp is quite slow due to the finetuning of the prompt. The idea is to try to get better or similar performances getting inspiration form TPT and other TTA methods, but, possibly, without any weight updates (as backpropagation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea8336",
   "metadata": {},
   "source": [
    "## Common Base Model\n",
    "\n",
    "As the main differences mostly rely in the forward method, this skeleton is provided to avoid code duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2b862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipBaseline(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        # self.logit_scale = model.log\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf9ddf",
   "metadata": {},
   "source": [
    "## A. Top 10% 🚀\n",
    "\n",
    "The idea is to remove the prompt learner (CoOp style) from the TPT model and use the most confident samples logits (top 1%) and the original image ones, average them and use the average logits as the final prediction.\n",
    "\n",
    "We want to keep the most confident samples logits, as they are the ones that are more likely to be correct, and average them with the original image logits, to avoid getting a too biased prediction and \"losing context\".\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "pred_class = int(marginal_prob.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "- `initial_logits`: the logits of the original image\n",
    "\n",
    "We expect this to be way faster than TPT (as no finetuning is done) and to have slightly better performances, as we are using the most confident samples logits, which are more likely to be correct, and keeping the original image logits to avoid getting a _too biased_ prediction.\n",
    "\n",
    "- Why the \"biased prediction\"? Because the augmentations are random crop, so the model might be biased towards the augmented images, which might not be representative of the original image. By averaging the logits, we can mitigate this bias and get a more reliable prediction.\n",
    "- Is this the best method to do this? No, but it's a good starting point, it's simple and it works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e6552",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac33959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(ClipBaseline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__(model, class_labels, prompt, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            initial_image_features = image_features[-1:]\n",
    "            filtered_image_features = image_features[:-1:]\n",
    "\n",
    "            # filter logits\n",
    "            initial_logits = (\n",
    "                self.logit_scale * initial_image_features @ self.text_features.t()\n",
    "            )\n",
    "            filtered_logits = (\n",
    "                self.logit_scale * filtered_image_features @ self.text_features.t()\n",
    "            )\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                # filtered_logits, top=1 / filtered_logits.shape[0]\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            # selected_logits = selected_logits.mean(0, keepdim=True)\n",
    "\n",
    "            # final_logits = selected_logits\n",
    "            final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = int(marginal_prob.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06fd48",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64e46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.53%\n",
      "Latency: 183.95 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=N_SAMPLES, comment=\"Top10\", visualize=False\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "del clip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271eab2",
   "metadata": {},
   "source": [
    "## B. TPT with Top 10\n",
    "\n",
    "The idea is pretty simple: use the top 10 + original image logits as specified above, but on TPT w/ CoOp.\n",
    "\n",
    "The implementation is straightforward: override the `forward` method of the `TPT` class (the one which manages the finetuning of the `TPTModel`), so that it uses the top 10 + original image logits instead of the prompt learner. Note that the finetuning of the model is kept as is, only the final prediction is changed.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e0c42",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9733ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPTTop10(TPT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            pretrained=pretrained,\n",
    "            arch=arch,\n",
    "            class_names=class_names,\n",
    "            tta_steps=tta_steps,\n",
    "            lr=lr,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b7c13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wrapper_clip\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d361620",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a630d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.45%\n",
      "Latency: 806.66 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wrapper_clip = TPTTop10(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=N_SAMPLES,\n",
    "    comment=\"TPT-Top10\",\n",
    "    visualize=False,\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del TPT\n",
    "del TPTTop10\n",
    "del wrapper_clip\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c39c8e",
   "metadata": {},
   "source": [
    "## C. TNT 🧨\n",
    "\n",
    "TODO: add amount of learnable parameters wrt TPT\n",
    "\n",
    "Implementation of TNT [[6](#ref-tnt2023)]. It's pretty straightforward, it's CLIP with learnable random noise on the augmented images (noise shape: `(3, height, width)`).\n",
    "\n",
    "We expect this to be slightly faster than TPT, and have slightly better performances, as in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20b237",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34397430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNT(ClipBaseline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model, class_labels=class_labels, prompt=prompt, device=device\n",
    "        )\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.tnt_steps = tnt_steps\n",
    "        self.top_k = top_k\n",
    "        self.eps = epsilon\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.temperature = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.noise = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()  #  type:ignore\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)[-1:]\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            probs = F.softmax(logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc6818",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f7e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.50%\n",
      "Latency: 647.98 ms\n"
     ]
    }
   ],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNT(\n",
    "    clip_model,  # type:ignore\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=N_SAMPLES, comment=\"TnT\", visualize=False\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "del clip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c3bf4",
   "metadata": {},
   "source": [
    "## D. TNT + Top10\n",
    "\n",
    "It's the same as TNT, but with the top 1 + original image logits as final prediction.\n",
    "\n",
    "We expect this to be slightly more accurate than TNT.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54950f2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076fe156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNTTop10(TNT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            class_labels=class_labels,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            tnt_steps=tnt_steps,\n",
    "            top_k=top_k,\n",
    "            epsilon=epsilon,\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()  # type:ignore\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdb029",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80caa2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a ClipSkeleton instance\u001b[39;00m\n\u001b[1;32m     14\u001b[0m wrapper_clip \u001b[38;5;241m=\u001b[39m TNTTop10(\n\u001b[1;32m     15\u001b[0m     clip_model,  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     class_labels\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mclass_code_to_label,\n\u001b[1;32m     17\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     18\u001b[0m     tnt_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTnT-Top10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Cleaning\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m wrapper_clip\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mbench\u001b[0;34m(model, dataloader, device, comment, reduce, visualize)\u001b[0m\n\u001b[1;32m     30\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m start_event\u001b[38;5;241m.\u001b[39mrecord()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m pred_class \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m end_event\u001b[38;5;241m.\u001b[39mrecord()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 46\u001b[0m, in \u001b[0;36mTNTTop10.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtnt_steps):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     x_aug \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogit_scale \u001b[38;5;241m*\u001b[39m image_features \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_features\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Select top-k logits\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/open_clip/model.py:279\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 279\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/open_clip/transformer.py:827\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    826\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embeds(x)\n\u001b[0;32m--> 827\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;66;03m# x.shape: torch.Size([64, 197, 768])\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     pooled, tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool(x)\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/open_clip/transformer.py:504\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    502\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 504\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first:\n\u001b[1;32m    507\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/open_clip/transformer.py:267\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    265\u001b[0m v_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1_kv(v_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln_1_kv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    266\u001b[0m x \u001b[38;5;241m=\u001b[39m q_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(q_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(q_x), k_x\u001b[38;5;241m=\u001b[39mk_x, v_x\u001b[38;5;241m=\u001b[39mv_x, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask))\n\u001b[0;32m--> 267\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/open_clip/transformer.py:35\u001b[0m, in \u001b[0;36mQuickGELU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.702\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNTTop10(\n",
    "    clip_model,  # type:ignore\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=N_SAMPLES, comment=\"TnT-Top10\", visualize=False\n",
    ")\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "del clip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d266d47",
   "metadata": {},
   "source": [
    "## E. Test Time Clustering (TTC)\n",
    "\n",
    "The idea here is to apply a clustering algorithm (Spectral Clustering) to the test image and its augmented views. We keep just the cluster containing the original image for the final prediction. In this way, we keep only the images similar to the original image, while filtering out the augmented views that maybe changed the semantic content of the image too much. The prediction is made with the averaged probabilities of the images in the selected cluster.\n",
    "\n",
    "Spectral clustering groups image features based on their pairwise cosine similarities by treating them as nodes in a graph, where edges represent similarity. The model computes a similarity (affinity) matrix, converts it into a graph Laplacian, and uses its eigenvectors to embed the data in a lower-dimensional space. K-means clustering is then applied in this space to assign images to clusters. We use spectral clustering, instead of other algorithms (e.g. K-means), because it allows us to work with similarities. Opposed to just the spatial positions.\n",
    "\n",
    "Additionally, the number of cluster is decided dynamically using the silhoutte score. The silhouette score measures how well a data point fits within its assigned cluster compared to other clusters. It ranges from -1 to 1, where a high score indicates that the point is well-matched to its own cluster and poorly matched to neighboring clusters. Specifically, it considers the average distance to points in the same cluster (intra-cluster) versus the nearest other cluster (inter-cluster). Averaging these scores across all points gives an overall measure of clustering quality, with higher values suggesting more distinct and well-separated clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b1848",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ba848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        n_clusters: int = 3,\n",
    "        dynamic_clusters: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.dynamic_clusters = dynamic_clusters\n",
    "\n",
    "        # Precompute text features\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "\n",
    "    @staticmethod\n",
    "    def best_cluster_count(affinity_matrix, max_clusters=8):\n",
    "        best_score = -1\n",
    "        best_n = 2  # Start from 2 since 1 gives undefined silhouette\n",
    "        for nr_clusters in range(2, max_clusters + 1):\n",
    "            clustering = SpectralClustering(n_clusters=nr_clusters, affinity='precomputed', random_state=42)\n",
    "            labels = clustering.fit_predict(affinity_matrix)\n",
    "            if len(set(labels)) == 1:  # Silhouette score is undefined for 1 cluster\n",
    "                continue\n",
    "            try:\n",
    "                score = silhouette_score(affinity_matrix, labels, metric=\"precomputed\")\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_n = nr_clusters\n",
    "            except Exception:\n",
    "                continue  # Skip cases where silhouette can't be computed\n",
    "        return best_n # Return the best_n found or the initial 2 if loop completes without finding a better score\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            # Cosine similarity matrix\n",
    "            sim_matrix = image_features @ image_features.T\n",
    "            affinity = sim_matrix.detach().cpu().numpy()\n",
    "            affinity[affinity < 0] = 0.0  # Threshold negative similarities\n",
    "\n",
    "            # If dynamic clustering is enabled, determine optimal cluster count and update clustering model\n",
    "            if self.dynamic_clusters:\n",
    "                best_n = self.best_cluster_count(affinity)\n",
    "                self.clustering = SpectralClustering(n_clusters=best_n, affinity='precomputed', random_state=42)\n",
    "\n",
    "            # Perform clustering and find features in the same cluster as the original image (last image)\n",
    "            cluster_labels = self.clustering.fit_predict(affinity)\n",
    "            original_cluster = cluster_labels[-1]\n",
    "            selected_indices = [i for i, lbl in enumerate(cluster_labels) if lbl == original_cluster]\n",
    "\n",
    "            # Classify based on features in the same cluster\n",
    "            selected_features = image_features[selected_indices]\n",
    "            logits = self.logit_scale * selected_features @ self.text_features.T\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "\n",
    "            return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6c073",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE, dynamic_clusters=True\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=N_SAMPLES, comment=\"spectral_clust_dyn\", visualize=False)\n",
    "\n",
    "# Cleaning\n",
    "del wrapper_clip\n",
    "del clip_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46857ace",
   "metadata": {},
   "source": [
    "## F. Image Masking 🤦‍♂️\n",
    "\n",
    "Taking inspiration from the idea of masking the image, as done in [DinoV2](https://arxiv.org/abs/2304.07193), for visualizing the foreground and the background, we can try to mask the image (and its augmented views) and use the masked image(s) to make the final predictions, using the most confident sample logits (top 10%). \n",
    "\n",
    "We don't expect much improvement from this method, as CLIP is not trained to work with masked images, nor it's trained to split foreground and background. Also the CLIP model we are using is ViT based, so, by being a transformer-based architecture it shouldn't need a \"guide\" on what to focus on the image, and these maskings are probably going to just reduce the context and the relevant predictions. But it can be interesting to see how it performs and to visualize how CLIP (doesn't) separate foreground and background.\n",
    "\n",
    "As CLIP doesn't have a specific foreground and background separation, we use two masks:\n",
    "- positive: thresholded $> 0.6$ \n",
    "- negative: thresholded $< 0.4$ (as in DinoV2)\n",
    "\n",
    "🤿\n",
    "The visual encoder of a model sometimes could be distracted when watching to the whole image.\n",
    "So a bit inspired by attention mechanisms, we wanted to try systems to augment TPT by masking features unused by the visual encoder.\n",
    "Once generated the augmentations of the image we make generate a mask for each augmentation. \n",
    "Since we are working with the VIT based CLIP, our features are linked to the patches used for the visual tokens.\n",
    "The idea is to obscure what are the unused patches and see what are the results.\n",
    "In some of the images, the central object of the image has less activations then the rest of the image. So to avoid that the central image is masked out we also generate the negative of the mask.\n",
    "\n",
    "Many methods were developed, they performed the same (or pretty close) the most notable are the following:\n",
    "### F.1\n",
    "1. image_features, tokens = model.visual(x)\n",
    "2. perform pca on patch_embeddings\n",
    "3. apply masks on the images\n",
    "    - Both positive and negative masks\n",
    "4. encode and get logits again\n",
    "5. keep top-k\n",
    "6. average logits and get the final prediction\n",
    "### F.2\n",
    "1. image_features, tokens = model.visual(x)\n",
    "2. keep top-k\n",
    "3. apply masks on the images\n",
    "    - Both positive and negative masks\n",
    "4. encode and get logits again\n",
    "5. average logits and get the final prediction *there's also a version that uses the logits of the original image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca4b90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "727f43a0",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(ClipBaseline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__(model, class_labels, prompt, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad():  # , torch.autocast(\"cuda\"):\n",
    "            clip_model.visual.output_tokens = True  # type: ignore\n",
    "\n",
    "            image_features, tokens = self.model.visual(x)\n",
    "\n",
    "            # Calcualte PCA (n_components=1) of each of the selected tokens\n",
    "            # Assuming patch_embeddings is a tensor of shape [64, 50, 768]\n",
    "            batch_size, num_patches, feat_dim = tokens.shape\n",
    "            PATCH_RES = int(num_patches**0.5)  # Assuming num_patches is a square number\n",
    "\n",
    "            # Center the data (subtract mean)\n",
    "            patch_embeddings_centered = tokens - tokens.mean(dim=1, keepdim=True)\n",
    "\n",
    "            U, S, V = torch.pca_lowrank(patch_embeddings_centered, q=1, center=False)\n",
    "\n",
    "            # Project the data (get the first principal component scores)\n",
    "            fg_scores = torch.matmul(\n",
    "                patch_embeddings_centered, V[:, :, -1:]\n",
    "            )  # [64, 50, 1]\n",
    "\n",
    "            # Min-max scaling per image\n",
    "            min_vals = fg_scores.min(dim=1, keepdim=True)[0]\n",
    "            max_vals = fg_scores.max(dim=1, keepdim=True)[0]\n",
    "            fg_scores_scaled = (fg_scores - min_vals) / (\n",
    "                max_vals - min_vals + 1e-8\n",
    "            )  # [64, 50, 1]\n",
    "\n",
    "            # Reshape to [64, PATCH_RES, PATCH_RES]\n",
    "            fg_scores_reshaped = fg_scores_scaled.view(batch_size, PATCH_RES, PATCH_RES)\n",
    "\n",
    "            # Create mask\n",
    "            positive_mask = fg_scores_reshaped > 0.6\n",
    "            negative_mask = fg_scores_reshaped < 0.4\n",
    "\n",
    "            positive_masked = x.clone()\n",
    "            negative_masked = positive_masked.clone()\n",
    "\n",
    "            positive_mask = F.interpolate(\n",
    "                positive_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "                size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "                mode=\"nearest\",\n",
    "            )\n",
    "\n",
    "            negative_mask = F.interpolate(\n",
    "                negative_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "                size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "                mode=\"nearest\",\n",
    "            )\n",
    "\n",
    "            positive_masked = positive_masked * positive_mask\n",
    "            negative_masked = negative_masked * (1 - negative_mask)\n",
    "\n",
    "            # Calculate the logits for the positive and negative masked images\n",
    "            clip_model.visual.output_tokens = False  # type: ignore\n",
    "\n",
    "            pos_logits = self.model.encode_image(positive_masked, normalize=True)\n",
    "            neg_logits = self.model.encode_image(negative_masked, normalize=True)\n",
    "\n",
    "            pos_logits = torch.cat((pos_logits, image_features[-1:]), dim=0)\n",
    "            neg_logits = torch.cat((neg_logits, image_features[-1:]), dim=0)\n",
    "\n",
    "            # Calculate the cosine similarity between the positive and negative logits\n",
    "            pos_logits = pos_logits @ self.text_features.T * self.logit_scale\n",
    "            neg_logits = neg_logits @ self.text_features.T * self.logit_scale\n",
    "\n",
    "            all_logits = torch.cat((pos_logits, neg_logits), 0)\n",
    "            selected_logits, idx = self.select_confident_samples(\n",
    "                logits=all_logits, top=0.1\n",
    "            )\n",
    "\n",
    "            all_marginal_prob = F.softmax(selected_logits, dim=1).mean(0)\n",
    "\n",
    "            pred_class = int(all_marginal_prob.argmax().item())\n",
    "        # # type 2\n",
    "        # with torch.no_grad():  # , torch.autocast(\"cuda\"):\n",
    "        #     clip_model.visual.output_tokens = True  # type: ignore\n",
    "\n",
    "        #     # Patch embedding: (batch_size, num_patches + 1, embed_dim)\n",
    "        #     image_features, tokens = self.model.visual(x)\n",
    "\n",
    "        #     image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        #     filtered_logits = (\n",
    "        #         self.logit_scale * image_features[:-1:] @ self.text_features.T\n",
    "        #     )\n",
    "\n",
    "        #     selected_logits, idx = self.select_confident_samples(\n",
    "        #         logits=filtered_logits, top=0.1\n",
    "        #     )\n",
    "\n",
    "        #     # selected_tokens = torch.cat((tokens[idx], tokens[-1:]), dim=0) # semplice\n",
    "        #     selected_tokens = tokens[idx]  # coso_separato\n",
    "\n",
    "        #     # Calcualte PCA (n_components=1) of each of the selected tokens\n",
    "        #     # Assuming patch_embeddings is a tensor of shape [64, 50, 768]\n",
    "        #     batch_size, num_patches, feat_dim = selected_tokens.shape\n",
    "        #     PATCH_RES = int(num_patches**0.5)  # Assuming num_patches is a square number\n",
    "\n",
    "        #     # Center the data (subtract mean)\n",
    "        #     patch_embeddings_centered = selected_tokens - selected_tokens.mean(\n",
    "        #         dim=1, keepdim=True\n",
    "        #     )\n",
    "\n",
    "        #     U, S, V = torch.pca_lowrank(patch_embeddings_centered, q=1, center=False)\n",
    "\n",
    "        #     # Project the data (get the first principal component scores)\n",
    "        #     fg_scores = torch.matmul(\n",
    "        #         patch_embeddings_centered, V[:, :, -1:]\n",
    "        #     )  # [64, 50, 1]\n",
    "\n",
    "        #     # Min-max scaling per image\n",
    "        #     min_vals = fg_scores.min(dim=1, keepdim=True)[0]\n",
    "        #     max_vals = fg_scores.max(dim=1, keepdim=True)[0]\n",
    "        #     fg_scores_scaled = (fg_scores - min_vals) / (\n",
    "        #         max_vals - min_vals + 1e-8\n",
    "        #     )  # [64, 50, 1]\n",
    "\n",
    "        #     # Reshape to [64, PATCH_RES, PATCH_RES]\n",
    "        #     fg_scores_reshaped = fg_scores_scaled.view(batch_size, PATCH_RES, PATCH_RES)\n",
    "\n",
    "        #     # Create mask\n",
    "        #     positive_mask = fg_scores_reshaped > 0.6\n",
    "        #     negative_mask = fg_scores_reshaped < 0.4\n",
    "\n",
    "        #     # Apply mask on the input (x) on the specific indices\n",
    "        #     positive_masked = x[idx].clone()\n",
    "        #     negative_masked = positive_masked.clone()\n",
    "\n",
    "        #     positive_mask = F.interpolate(\n",
    "        #         positive_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "        #         size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "        #         mode=\"nearest\",\n",
    "        #     )\n",
    "\n",
    "        #     negative_mask = F.interpolate(\n",
    "        #         negative_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "        #         size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "        #         mode=\"nearest\",\n",
    "        #     )\n",
    "\n",
    "        #     positive_masked_p = positive_masked * positive_mask  # .bool()\n",
    "        #     negative_masked_p = negative_masked * (1 - positive_mask)  # .bool()\n",
    "        #     positive_masked_n = positive_masked * negative_mask  # .bool()\n",
    "        #     negative_masked_n = negative_masked * (1 - negative_mask)  # .bool()\n",
    "\n",
    "        #     # Calculate the logits for the positive and negative masked images\n",
    "        #     clip_model.visual.output_tokens = False  # type: ignore\n",
    "\n",
    "        #     masked = torch.cat(\n",
    "        #         (\n",
    "        #             positive_masked_p,\n",
    "        #             negative_masked_p,\n",
    "        #             positive_masked_n,\n",
    "        #             negative_masked_n,\n",
    "        #             # x[-1:],\n",
    "        #         ),\n",
    "        #         dim=0,\n",
    "        #     )\n",
    "\n",
    "        #     logits = self.model.encode_image(masked, normalize=True)\n",
    "        #     logits = logits @ self.text_features.T * self.logit_scale\n",
    "        #     logits = self.select_confident_samples(logits, top=0.1)[0]\n",
    "\n",
    "        #     marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "        #     pred_class = int(marginal_prob.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fd4f3",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeebc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type: ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=N_SAMPLES,\n",
    "    comment=\"pca_no_bool_nuovo_senza_originale\",\n",
    "    visualize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3210f1",
   "metadata": {},
   "source": [
    "## G. PCA Guided Image Augmentation\n",
    "\n",
    "As F. we take inspiratoin from [DinoV2](https://arxiv.org/abs/2304.07193) and use the PCA of the \"best\" patches of the image to guide the augmentation. Random crops, centered on random patches are used to augment the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3509dd",
   "metadata": {},
   "source": [
    "# 5. Reproducing TPT + CoOp with full CoOp pretraining (on ImageNetV2)\n",
    "We want to reproduce the results of TPT + CoOp with full CoOp pretraining (on ImageNetV2) to compare it with our TPT + CoOp inspired implementation (without pretraining, full TTA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac182b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa13371",
   "metadata": {},
   "source": [
    "# 6. Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64904839",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "| **Implementation**                     | **Accuracy**  | **Time (ms)** |\n",
    "| :----------------                      | :-----------: | :---------:   |\n",
    "| CLIP (OpenAI)                          | 49.37         | **4**         |\n",
    "| CLIP (OpenCLIP)                        | 49.39         | 7             |\n",
    "| TPT + CoOp (pre-trained on ImageNetV2) | 55.00         | -             |\n",
    "| TPT + CoOp (Our)                       | 57.13         | 725           |\n",
    "| Top 10%                                | **59.00**     | 163           |\n",
    "| TPT + Top 10%                          | 59.12         | 713           |\n",
    "| TNT                                    |               |               |\n",
    "| TNT + Top 10%                          |               |               |\n",
    "| Test Time Clustering                   | 54.76         | 211           |\n",
    "| Image Masking (method F.1)             | 54.91         | 1000          |\n",
    "| Image Masking (method F.2)             | 55.35         | 499           |\n",
    "| PCA Guided Image Augmentation          |               |               |\n",
    "\n",
    "</div>\n",
    "\n",
    "$\\star$ experiments run on a single Laptop NVIDIA RTX 4060 (140W, performance mode).\n",
    "$\\star$ time represents the average time to perform the inference on a single image (model only), excluding data loading and augmentation time. Recoded with cuda events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63d36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18f9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c95ce3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
