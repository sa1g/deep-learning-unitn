{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from [OpenAI's Zero-Shot Prediction example](https://github.com/openai/CLIP?tab=readme-ov-file#zero-shot-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _ = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import ImageNetA\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "dataset = ImageNetA(\"dataset/imagenet-a\", _transform, n_px=model.visual.input_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training, testing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "training_sample = int(num_samples * 0.5 + 1)\n",
    "validation_sample = int(num_samples * 0.25)\n",
    "test_sample = num_samples - training_sample - validation_sample\n",
    "\n",
    "training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [training_sample, validation_sample, test_sample])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "Ground truch: {'stingray'}\n",
      "        stingray: 16.53%\n",
      "american bullfrog: 11.72%\n",
      "     cowboy boot: 9.27%\n",
      "            newt: 7.11%\n",
      "   manhole cover: 5.80%\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "image, class_id = dataset[0]\n",
    "image_input = image.unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in dataset.classes.values()]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "print(f\"Ground truch: {dataset.idx_to_class(class_id.item())}\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{list(dataset.classes.values())[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One image at a time (SLOW - don't run it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up device and load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Assuming test_dataset is already defined and a DataLoader is ready\n",
    "batch_size = 32  # Set the batch size according to your hardware capabilities\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for images, class_ids in dataloader:\n",
    "    # Preprocess images and move them to the appropriate device\n",
    "    image_inputs = images.to(device)\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in dataset.classes.values()]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Normalize features\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarities between images and text features\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Pick the top 5 most similar labels for each image in the batch\n",
    "    values, indices = similarity.topk(1, dim=-1)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        # Print the result for each image in the batch\n",
    "        print(\"\\nTop predictions:\\n\")\n",
    "        print(f\"Ground truth: {dataset.idx_to_class(class_ids[i].item())}\")\n",
    "        for value, index in zip(values[i], indices[i]):\n",
    "            print(f\"{list(dataset.classes.values())[index]:>16s}: {100 * value.item():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It works (Batches: FAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 29.62% (555/1874 correct predictions)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up device and load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Assuming test_dataset is already defined and a DataLoader is ready\n",
    "batch_size = 256  # Set the batch size according to your hardware capabilities\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Prepare text inputs once outside the loop\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in dataset.classes.values()]).to(device)\n",
    "\n",
    "# Create a tensor that maps the indices to class IDs\n",
    "class_keys_tensor = torch.tensor(list(dataset.classes.keys())).to(device)\n",
    "\n",
    "# Variables to track the number of correct predictions and total predictions\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for images, class_ids in dataloader:\n",
    "    # Preprocess images and move them to the appropriate device\n",
    "    image_inputs = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode image and text inputs\n",
    "        image_features = model.encode_image(image_inputs)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarities between images and text features\n",
    "        similarity = image_features @ text_features.T\n",
    "\n",
    "        # Get the index of the top 1 most similar label for each image in the batch\n",
    "        predicted_indices = similarity.argmax(dim=-1)\n",
    "\n",
    "    # Map predicted indices to actual class IDs\n",
    "    predicted_class_ids = class_keys_tensor[predicted_indices]\n",
    "\n",
    "    # Calculate the number of correct predictions in the batch\n",
    "    correct_predictions += (predicted_class_ids == class_ids.to(device)).sum().item()\n",
    "    total_predictions += class_ids.size(0)\n",
    "\n",
    "# Calculate and print the overall accuracy\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}% ({correct_predictions}/{total_predictions} correct predictions)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
