{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fe1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    cols = 8\n",
    "    rows = (images.shape[0] + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols * 2, rows * 2))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        if img.dtype.kind == \"f\":  # float\n",
    "            img = img.clip(0, 1)  # ensure in [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a03d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import torchvision\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "image = torchvision.io.read_image(\n",
    "    \"datasets/imagenet-a/n01641577/0.038738_agama _ newt_0.7465035.jpg\"\n",
    ")\n",
    "\n",
    "n_times = 100\n",
    "n_augmentations = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b038bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "\n",
    "\n",
    "class AugMixKornia(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        severity: int = 3,\n",
    "        width: int = 3,\n",
    "        depth: int = -1,\n",
    "        alpha: float = 1.0,\n",
    "        mixture_width: int = 3,\n",
    "        chain_depth: int = 3,\n",
    "        all_ops: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AugMix implementation using Kornia with closer fidelity to the original paper.\n",
    "\n",
    "        Args:\n",
    "            severity: Severity level of augmentations (1-10)\n",
    "            width: Width of augmentation chain (not used directly, kept for compatibility)\n",
    "            depth: Depth of augmentation chain (-1 for random between 1-3)\n",
    "            alpha: Dirichlet distribution parameter for mixing weights\n",
    "            mixture_width: Number of augmentation chains to mix\n",
    "            chain_depth: Number of operations in each chain\n",
    "            all_ops: Whether to use all augmentation operations\n",
    "            device: Device to run on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.severity = severity\n",
    "        self.alpha = alpha\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth if depth <= 0 else depth\n",
    "        self.all_ops = all_ops\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define augmentation operations\n",
    "        self.augmentations = self._get_augmentations()\n",
    "\n",
    "    def _get_augmentations(self) -> List[nn.Module]:\n",
    "        \"\"\"Create a list of augmentation operations that will be randomly applied\"\"\"\n",
    "        severity_factor = self.severity / 10.0\n",
    "\n",
    "        if self.all_ops:\n",
    "            # Full set of augmentations similar to original AugMix\n",
    "            return [\n",
    "                # AutoContrast\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                # Equalize\n",
    "                Ke.equalize,\n",
    "                # Posterize\n",
    "                K.RandomPosterize(bits=max(1, 8 - self.severity), p=1.0),\n",
    "                # Rotate\n",
    "                K.RandomRotation(\n",
    "                    degrees=(-30 * severity_factor, 30 * severity_factor), p=1.0\n",
    "                ),\n",
    "                # Solarize\n",
    "                K.RandomSolarize(\n",
    "                    thresholds=0.5, additions=(0.0, 0.1 * self.severity), p=1.0\n",
    "                ),\n",
    "                # Shear\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    shear=(-15 * severity_factor, 15 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # Translate\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.1 * severity_factor, 0.1 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # ColorJitter\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity,\n",
    "                    contrast=0.1 * self.severity,\n",
    "                    saturation=0.1 * self.severity,\n",
    "                    hue=0.1,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Simplified version\n",
    "            return [\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                Ke.equalize,\n",
    "                K.RandomAffine(\n",
    "                    degrees=(-15 * severity_factor, 15 * severity_factor), p=1.0\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _apply_augmentation_chain(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a random sequence of augmentations to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented image tensor (C, H, W)\n",
    "        \"\"\"\n",
    "        # Randomly select augmentations for this chain\n",
    "        op_indices = np.random.choice(\n",
    "            len(self.augmentations), size=self.chain_depth, replace=True\n",
    "        )\n",
    "\n",
    "        augmented = image  # Don't clone immediately\n",
    "        for op_idx in op_indices:\n",
    "            augmented = self.augmentations[op_idx](augmented)\n",
    "\n",
    "        return augmented.squeeze(0)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply AugMix to a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: Input batch of images (B, C, H, W) or (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented batch (same shape as input)\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if not isinstance(images, torch.Tensor):\n",
    "            images = K.image_to_tensor(images)\n",
    "\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        # Move to device if needed\n",
    "        if images.device != self.device:\n",
    "            images = images.to(self.device)\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Sample mixing weights from Dirichlet distribution\n",
    "        weights = (\n",
    "            torch.from_numpy(\n",
    "                np.random.dirichlet([self.alpha] * self.mixture_width, size=batch_size)\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )  # Shape (B, mixture_width)\n",
    "\n",
    "        # Sample weights for mixing with original\n",
    "        mix_weights = (\n",
    "            torch.from_numpy(\n",
    "                np.random.dirichlet([self.alpha, self.alpha], size=batch_size)\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )  # Shape (B, 2)\n",
    "\n",
    "        # Generate augmented versions for each mixture component\n",
    "        # Pre-allocate memory for augmented versions\n",
    "        augmented = torch.empty(\n",
    "            (self.mixture_width, batch_size, *images.shape[1:]), device=self.device\n",
    "        )\n",
    "\n",
    "        for i in range(self.mixture_width):\n",
    "            augmented[i] = self._apply_augmentation_chain(images)\n",
    "\n",
    "        # Weighted sum of augmented versions\n",
    "        mixed = torch.einsum(\"mbchw,bm->bchw\", augmented, weights).to(self.device)\n",
    "\n",
    "        # Final mix with original image\n",
    "        result = (\n",
    "            mix_weights[:, 0:1, None, None] * images\n",
    "            + mix_weights[:, 1:2, None, None] * mixed\n",
    "        )\n",
    "\n",
    "        return result.squeeze(0) if result.shape[0] == 1 else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e196e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia.constants\n",
    "\n",
    "\n",
    "kornia_preprocess = nn.Sequential(\n",
    "    K.SmallestMaxSize(\n",
    "        224,\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    K.CenterCrop(\n",
    "        size=(224, 224),\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    kornia.enhance.Normalize(\n",
    "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]),\n",
    "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "kornia_augmix = AugMixKornia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1622623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageNetADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the ImageNet-A dataset.\n",
    "\n",
    "    Set the `transform` parameter so that images work with your model.\n",
    "    Example usage:\n",
    "    ```python\n",
    "        model, transform = clip.load(\"ViT-B/32\")\n",
    "        dataset = ImageNetADataset(<path>, transform=transform)\n",
    "    ```\n",
    "    ----\n",
    "\n",
    "    The dataset is organized into subdirectories, each named with a class code (e.g., \"n01614925\").\n",
    "    Each subdirectory contains images belonging to that class. The dataset also includes a README.txt file that maps class codes to human-readable names.\n",
    "\n",
    "    The dataset is expected to be structured as follows:\n",
    "    ```\n",
    "    datasets/imagenet-a/\n",
    "        n01440764/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        n01614925/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        ...\n",
    "        README.txt\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir=\"datasets/imagenet-a\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__download_if_needed()\n",
    "\n",
    "        # Load mapping from class codes (e.g., \"n01614925\") to human-readable names\n",
    "        readme_path = os.path.join(root_dir, \"README.txt\")\n",
    "        self.class_code_to_label = self._load_class_mapping(readme_path)\n",
    "\n",
    "        # Filter valid class directories that match the mapping\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_label\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Map class codes to indices\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image file paths and corresponding labels\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "        # Inverse mapping from label index to class name\n",
    "        self.idx_to_label = {\n",
    "            idx: self.class_code_to_label[code]\n",
    "            for code, idx in self.class_code_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def __download_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if the dataset is already downloaded. If not, download it.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found at {self.root_dir}. Please download it first.\"\n",
    "            )\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        \"\"\"\n",
    "        Load class code to human-readable name mapping from README.txt.\n",
    "        Skips header lines and parses lines in format: 'n01440764 tench'.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as file:\n",
    "            lines = file.readlines()[12:]  # Skip first 12 header lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        \"\"\"\n",
    "        Walk through each class directory to gather image paths and corresponding labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_dir = os.path.join(self.root_dir, class_code)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    samples.append((image_path, label))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load image and return dictionary containing image, label index, and class name.\n",
    "\n",
    "        Returns:\n",
    "            image (tensor)\n",
    "            label (tensor)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path).float() / 255.0\n",
    "\n",
    "        if image.shape[0] == 1:  # Grayscale → RGB\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        elif image.shape[0] == 4:  # RGBA → RGB\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        elif image.shape[0] != 3:\n",
    "            raise ValueError(f\"Unsupported number of channels: {image.shape[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"\n",
    "        Get human-readable class name for a given index.\n",
    "        \"\"\"\n",
    "        return self.idx_to_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119b4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform(nn.Module):\n",
    "    def __init__(self, model_transform, custom_transform=None, n_views=63, device=None):\n",
    "        super().__init__()\n",
    "        self.model_transform = model_transform\n",
    "        self.custom_transform = custom_transform\n",
    "        self.n_views = n_views\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.eval()\n",
    "        # self.model_transform.eval()\n",
    "        # self.custom_transform.eval() if custom_transform is not None else None\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model transform and custom transform to the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            image = image.to(self.device)\n",
    "\n",
    "            if self.custom_transform is not None:\n",
    "                views = image.repeat(self.n_views, 1, 1, 1)\n",
    "                views = self.custom_transform(views)\n",
    "                views = torch.cat([views, image.unsqueeze(0)], dim=0)\n",
    "                views = self.model_transform(views)\n",
    "\n",
    "                return views\n",
    "            else:\n",
    "                return self.model_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf552d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ResnetA(\n",
    "    augmenter: ImageTransform,\n",
    "    root_dir=\"datasets/imagenet-a\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the ImageNet-A dataset. Defaults to 1 element per batch.\n",
    "    Non modifiable. No shuffling.\n",
    "    Args:\n",
    "        augmenter (callable):\n",
    "        root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (DataLoader): DataLoader for the ImageNet-A dataset.\n",
    "        dataset (ImageNetADataset): The underlying dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle the batch of images and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images = batch[0][0]\n",
    "\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        labels = batch[0][1]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    dataset = ImageNetADataset(root_dir=root_dir, transform=augmenter)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388175a2",
   "metadata": {},
   "source": [
    "# So\n",
    "\n",
    "1. base model\n",
    "2. tpt model\n",
    "3. prompt learning (tpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c254c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_augmix,\n",
    "    n_views=63,\n",
    ")\n",
    "\n",
    "dataloader, dataset = ResnetA(augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f72c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 151279360\n",
      "Trainable parameters: 2048\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import clip\n",
    "import clip.model\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "    # You can add more, but the `kornia_preprocess` should be modified accordingly\n",
    "    # ViTB16: str = \"ViT-B/16\"\n",
    "    # RN50: str = \"RN50\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: clip.model.CLIP,\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.dtype = clip_model.visual.conv1.weight.dtype\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "        self.token_embedding.requires_grad_(False)\n",
    "\n",
    "        self.__init_ctx_from_prompt(base_prompt=base_prompt)\n",
    "\n",
    "    def __init_ctx_from_prompt(self, base_prompt: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[49406]]).to(self.device)  # SOT\n",
    "        c_token_eot = torch.tensor([[49407]]).to(self.device)  # EOT\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = clip.tokenize(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = clip.tokenize(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = clip.tokenize(self.class_names).to(self.device)\n",
    "\n",
    "        # self.tokenized_class_names_len = torch.argmax(\n",
    "        #     (tokenized_class_names == 0).int(), dim=1, keepdim=True\n",
    "        # )\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = clip.tokenize(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        )\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = self.token_embedding(c_token_sot)\n",
    "            self.embedded_eot = self.token_embedding(c_token_eot)\n",
    "            self.embedded_pad = self.token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = self.token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = self.token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = self.token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            clean_embed = embedded_class_names[i][mask[i]]  # [num_valid_tokens, 512]\n",
    "            clean_embeddings.append(\n",
    "                clean_embed.unsqueeze(0)\n",
    "            )  # [1, num_valid_tokens, 512]\n",
    "\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "\n",
    "        for i, embed in enumerate(clean_embeddings):\n",
    "            self.register_buffer(f\"class_embed_{i}\", embed)\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # embeddeD_max_len: 77\n",
    "            # embedded_prefix: torch.Size([4, 512])\n",
    "            # embedded_class_names: torch.Size([1, 1, 512])\n",
    "            # embedded_suffix: torch.Size([0, 512]\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - getattr(self, f\"class_embed_{i}\").shape[1]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            ) - 2  # # -2 for SOT and EOT\n",
    "\n",
    "            ## embedded sot shape: torch.Size([1, 1, 512])\n",
    "            ## embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            ## embedded class names shape: torch.Size([1, 1, 1, 512])\n",
    "            ## embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            ## embedded eot shape: torch.Size([1, 1, 512])\n",
    "            ## effective padding shape: torch.Size([1, 70, 512])\n",
    "            ## Prompt shape: torch.Size([1, 77, 512])\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    # self.embedded_class_names[i],\n",
    "                    getattr(self, f\"class_embed_{i}\"),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    self.embedded_pad.repeat(1, padding_size, 1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "        with torch.no_grad():\n",
    "            self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "            self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, class_names: List[str], arch: CLIPModels = CLIPModels.ViTB32, device=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(arch, device=self.device)\n",
    "\n",
    "        self.dtype = clip_model.visual.conv1.weight.dtype\n",
    "        # self.clip = clip_model\n",
    "        self.image_encoder = clip_model.visual\n",
    "\n",
    "        self.logit_scale = clip_model.logit_scale.data\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "        #\n",
    "\n",
    "    def __encode_text(\n",
    "        self, tokenized_prompt: torch.Tensor, embedded_prompt: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the text prompt using the CLIP model.\n",
    "            The tokenizer is external.\n",
    "\n",
    "        Source: CLIP source code. model.py#L343\n",
    "        \"\"\"\n",
    "        x = embedded_prompt + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLP -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompt.argmax(dim=-1)]\n",
    "            @ self.text_projection\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the image using the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            image_features (torch.Tensor): Normalized encoded image features.\n",
    "        \"\"\"\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    def forward(self, image: torch.Tensor, is_image: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "            is_image (bool): whether the input is an iamge or already image_features.\n",
    "                If False, the input is assumed to be already image features.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "        if is_image:\n",
    "            with torch.no_grad():\n",
    "                image_features = self.encode_image(image)\n",
    "        else:\n",
    "            image_features = image\n",
    "\n",
    "        embedded_prompt = self.prompt_learner().type(self.dtype)\n",
    "\n",
    "        txt_features = self.__encode_text(\n",
    "            self.prompt_learner.tokenized_initial_full_prompt, embedded_prompt\n",
    "        )\n",
    "        txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ txt_features.t()\n",
    "\n",
    "        if is_image:\n",
    "            return logits, image_features\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the prompt learner to its initial state.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.reset()\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        arch: CLIPModels = CLIPModels.ViTB32,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "        model = TPTModel(\n",
    "            class_names=class_names,\n",
    "            arch=arch,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Get all trainable parameters (filter by requires_grad)\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # Initialize optimizer with trainable parameters\n",
    "        self.optim = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim_init = deepcopy(self.optim.state_dict())\n",
    "\n",
    "    def set_tta_steps(self, tta_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the number of TTA steps.\n",
    "\n",
    "        Args:\n",
    "            tta_steps (int): Number of TTA steps.\n",
    "        \"\"\"\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # manage Prompt learner finetuning\n",
    "        # so do n iterations with fine tuning\n",
    "        # then return the single prediction\n",
    "        # loss, etc, are 100% internal\n",
    "\n",
    "        selected_idx = None\n",
    "        for _ in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "            # print(f\"Loss: {loss.item():.3f}\")\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # take only the last image of the input\n",
    "                input = input[-1].unsqueeze(0)\n",
    "                logits, _ = self.model(input)\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def __select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def __avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def __reset(self) -> None:\n",
    "        \"\"\"Full reset of prompt learner and optimizer state\"\"\"\n",
    "        # 1. Reset prompt embeddings\n",
    "        self.model.reset()\n",
    "\n",
    "        # # 2. Reset optimizer state\n",
    "        self.optim.load_state_dict(deepcopy(self.optim_init))\n",
    "\n",
    "        # # 3. Reset gradient scaler if using AMP\n",
    "        # if hasattr(self, \"scaler\"):\n",
    "        #     self.scaler.load_state_dict(torch.cuda.amp.GradScaler().state_dict())\n",
    "\n",
    "\n",
    "my_tpt = TPT(class_names=dataset.class_code_to_label.values(), tta_steps=0, lr=0.005)\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in my_tpt.parameters())}\")\n",
    "print(\n",
    "    f\"Trainable parameters: {sum(p.numel() for p in my_tpt.parameters() if p.requires_grad)}\"\n",
    ")\n",
    "\n",
    "# for image, label in tqdm(dataloader):\n",
    "#     l = my_tpt(image)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5111343",
   "metadata": {},
   "source": [
    "For TPT, we initialize the prompt as the default hand-crafted one “a photo\n",
    "of a\", and optimize the corresponding 4 tokens in the text input embedding space based on a single\n",
    "test image. We augment a single test image 63 times using random resized crops and construct a\n",
    "batch of 64 images, including the original one. Among the 64 predictions, we select the top 10%\n",
    "(ρ=0.1) confident samples (lowest 10% in self-entropy) and compute the entropy of the averaged\n",
    "probability of the selected predictions (i.e., marginal entropy). We optimize the prompt to minimize\n",
    "the marginal entropy for 1 step, using the AdamW optimizer with a learning rate of 0.005.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ea841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/7500 [00:14<2:04:21,  1.00it/s]"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def bench(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str,\n",
    "    reduce: int | None = None,\n",
    "):\n",
    "    \"\"\"Benchmark the model on the dataset.\n",
    "\n",
    "    The model must return logits.\n",
    "    \"\"\"\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for image, label in tqdm(dataloader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        logits = model(image)\n",
    "\n",
    "        # pred_class = logits.argmax(dim=-1)\n",
    "        marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "        pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        total += 1\n",
    "        correct += int((pred_class == label).max().item())\n",
    "\n",
    "        if reduce:\n",
    "            if total > reduce:\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    latency = (end - start) / total  # ms\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Latency: {latency * 1000:.2f} ms\")\n",
    "\n",
    "    return accuracy, latency\n",
    "\n",
    "my_tpt.set_tta_steps(1)\n",
    "\n",
    "accuracy, latency = bench(my_tpt, dataloader, device, reduce=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb2f019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maccuracy\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922335d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
