{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78fe1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    cols = 8\n",
    "    rows = (images.shape[0] + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols * 2, rows * 2))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        if img.dtype.kind == \"f\":  # float\n",
    "            img = img.clip(0, 1)  # ensure in [0, 1]\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a03d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "import torchvision\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "image = torchvision.io.read_image(\n",
    "    \"datasets/imagenet-a/n01641577/0.038738_agama _ newt_0.7465035.jpg\"\n",
    ")\n",
    "\n",
    "n_times = 100\n",
    "n_augmentations = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b038bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "\n",
    "\n",
    "class AugMixKornia(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        severity: int = 3,\n",
    "        width: int = 3,\n",
    "        depth: int = -1,\n",
    "        alpha: float = 1.0,\n",
    "        mixture_width: int = 3,\n",
    "        chain_depth: int = 3,\n",
    "        all_ops: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AugMix implementation using Kornia with closer fidelity to the original paper.\n",
    "\n",
    "        Args:\n",
    "            severity: Severity level of augmentations (1-10)\n",
    "            width: Width of augmentation chain (not used directly, kept for compatibility)\n",
    "            depth: Depth of augmentation chain (-1 for random between 1-3)\n",
    "            alpha: Dirichlet distribution parameter for mixing weights\n",
    "            mixture_width: Number of augmentation chains to mix\n",
    "            chain_depth: Number of operations in each chain\n",
    "            all_ops: Whether to use all augmentation operations\n",
    "            device: Device to run on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.severity = severity\n",
    "        self.alpha = alpha\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth if depth <= 0 else depth\n",
    "        self.all_ops = all_ops\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define augmentation operations\n",
    "        self.augmentations = self._get_augmentations()\n",
    "\n",
    "    def _get_augmentations(self) -> List[nn.Module]:\n",
    "        \"\"\"Create a list of augmentation operations that will be randomly applied\"\"\"\n",
    "        severity_factor = self.severity / 10.0\n",
    "\n",
    "        if self.all_ops:\n",
    "            # Full set of augmentations similar to original AugMix\n",
    "            return [\n",
    "                # AutoContrast\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                # Equalize\n",
    "                Ke.equalize,\n",
    "                # Posterize\n",
    "                K.RandomPosterize(bits=max(1, 8 - self.severity), p=1.0),\n",
    "                # Rotate\n",
    "                K.RandomRotation(\n",
    "                    degrees=(-30 * severity_factor, 30 * severity_factor), p=1.0\n",
    "                ),\n",
    "                # Solarize\n",
    "                K.RandomSolarize(\n",
    "                    thresholds=0.5, additions=(0.0, 0.1 * self.severity), p=1.0\n",
    "                ),\n",
    "                # Shear\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    shear=(-15 * severity_factor, 15 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # Translate\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.1 * severity_factor, 0.1 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # ColorJitter\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity,\n",
    "                    contrast=0.1 * self.severity,\n",
    "                    saturation=0.1 * self.severity,\n",
    "                    hue=0.1,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Simplified version\n",
    "            return [\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                Ke.equalize,\n",
    "                K.RandomAffine(\n",
    "                    degrees=(-15 * severity_factor, 15 * severity_factor), p=1.0\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _apply_augmentation_chain(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a random sequence of augmentations to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented image tensor (C, H, W)\n",
    "        \"\"\"\n",
    "        # Randomly select augmentations for this chain\n",
    "        op_indices = np.random.choice(\n",
    "            len(self.augmentations), size=self.chain_depth, replace=True\n",
    "        )\n",
    "\n",
    "        augmented = image  # Don't clone immediately\n",
    "        for op_idx in op_indices:\n",
    "            augmented = self.augmentations[op_idx](augmented)\n",
    "\n",
    "        return augmented.squeeze(0)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply AugMix to a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: Input batch of images (B, C, H, W) or (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented batch (same shape as input)\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if not isinstance(images, torch.Tensor):\n",
    "            images = K.image_to_tensor(images)\n",
    "\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        # Move to device if needed\n",
    "        if images.device != self.device:\n",
    "            images = images.to(self.device)\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Sample mixing weights from Dirichlet distribution\n",
    "        weights = (\n",
    "            torch.from_numpy(\n",
    "                np.random.dirichlet([self.alpha] * self.mixture_width, size=batch_size)\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )  # Shape (B, mixture_width)\n",
    "\n",
    "        # Sample weights for mixing with original\n",
    "        mix_weights = (\n",
    "            torch.from_numpy(\n",
    "                np.random.dirichlet([self.alpha, self.alpha], size=batch_size)\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )  # Shape (B, 2)\n",
    "\n",
    "        # Generate augmented versions for each mixture component\n",
    "        # Pre-allocate memory for augmented versions\n",
    "        augmented = torch.empty(\n",
    "            (self.mixture_width, batch_size, *images.shape[1:]), device=self.device\n",
    "        )\n",
    "\n",
    "        for i in range(self.mixture_width):\n",
    "            augmented[i] = self._apply_augmentation_chain(images)\n",
    "\n",
    "        # Weighted sum of augmented versions\n",
    "        mixed = torch.einsum(\"mbchw,bm->bchw\", augmented, weights).to(self.device)\n",
    "\n",
    "        # Final mix with original image\n",
    "        result = (\n",
    "            mix_weights[:, 0:1, None, None] * images\n",
    "            + mix_weights[:, 1:2, None, None] * mixed\n",
    "        )\n",
    "\n",
    "        return result.squeeze(0) if result.shape[0] == 1 else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e196e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia.constants\n",
    "\n",
    "\n",
    "kornia_preprocess = nn.Sequential(\n",
    "    K.SmallestMaxSize(\n",
    "        224,\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    K.CenterCrop(\n",
    "        size=(224, 224),\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    kornia.enhance.Normalize(\n",
    "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]),\n",
    "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "kornia_augmix = AugMixKornia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1622623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageNetADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the ImageNet-A dataset.\n",
    "\n",
    "    Set the `transform` parameter so that images work with your model.\n",
    "    Example usage:\n",
    "    ```python\n",
    "        model, transform = clip.load(\"ViT-B/32\")\n",
    "        dataset = ImageNetADataset(<path>, transform=transform)\n",
    "    ```\n",
    "    ----\n",
    "\n",
    "    The dataset is organized into subdirectories, each named with a class code (e.g., \"n01614925\").\n",
    "    Each subdirectory contains images belonging to that class. The dataset also includes a README.txt file that maps class codes to human-readable names.\n",
    "\n",
    "    The dataset is expected to be structured as follows:\n",
    "    ```\n",
    "    datasets/imagenet-a/\n",
    "        n01440764/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        n01614925/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        ...\n",
    "        README.txt\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir=\"datasets/imagenet-a\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__download_if_needed()\n",
    "\n",
    "        # Load mapping from class codes (e.g., \"n01614925\") to human-readable names\n",
    "        readme_path = os.path.join(root_dir, \"README.txt\")\n",
    "        self.class_code_to_label = self._load_class_mapping(readme_path)\n",
    "\n",
    "        # Filter valid class directories that match the mapping\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_label\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Map class codes to indices\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image file paths and corresponding labels\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "        # Inverse mapping from label index to class name\n",
    "        self.idx_to_label = {\n",
    "            idx: self.class_code_to_label[code]\n",
    "            for code, idx in self.class_code_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def __download_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if the dataset is already downloaded. If not, download it.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found at {self.root_dir}. Please download it first.\"\n",
    "            )\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        \"\"\"\n",
    "        Load class code to human-readable name mapping from README.txt.\n",
    "        Skips header lines and parses lines in format: 'n01440764 tench'.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as file:\n",
    "            lines = file.readlines()[12:]  # Skip first 12 header lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        \"\"\"\n",
    "        Walk through each class directory to gather image paths and corresponding labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_dir = os.path.join(self.root_dir, class_code)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    samples.append((image_path, label))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load image and return dictionary containing image, label index, and class name.\n",
    "\n",
    "        Returns:\n",
    "            image (tensor)\n",
    "            label (tensor)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path).float() / 255.0\n",
    "\n",
    "        if image.shape[0] == 1:  # Grayscale → RGB\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        elif image.shape[0] == 4:  # RGBA → RGB\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        elif image.shape[0] != 3:\n",
    "            raise ValueError(f\"Unsupported number of channels: {image.shape[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"\n",
    "        Get human-readable class name for a given index.\n",
    "        \"\"\"\n",
    "        return self.idx_to_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119b4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform(nn.Module):\n",
    "    def __init__(self, model_transform, custom_transform=None, n_views=63, device=None):\n",
    "        super().__init__()\n",
    "        self.model_transform = model_transform\n",
    "        self.custom_transform = custom_transform\n",
    "        self.n_views = n_views\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.eval()\n",
    "        # self.model_transform.eval()\n",
    "        # self.custom_transform.eval() if custom_transform is not None else None\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model transform and custom transform to the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            image = image.to(self.device)\n",
    "\n",
    "            if self.custom_transform is not None:\n",
    "                views = image.repeat(self.n_views, 1, 1, 1)\n",
    "                views = self.custom_transform(views)\n",
    "                views = torch.cat([views, image.unsqueeze(0)], dim=0)\n",
    "                views = self.model_transform(views)\n",
    "\n",
    "                return views\n",
    "            else:\n",
    "                return self.model_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf552d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def ResnetA(\n",
    "    augmenter: ImageTransform,\n",
    "    root_dir=\"datasets/imagenet-a\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the ImageNet-A dataset. Defaults to 1 element per batch.\n",
    "    Non modifiable. No shuffling.\n",
    "    Args:\n",
    "        augmenter (callable):\n",
    "        root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (DataLoader): DataLoader for the ImageNet-A dataset.\n",
    "        dataset (ImageNetADataset): The underlying dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle the batch of images and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images = batch[0][0]\n",
    "\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        labels = batch[0][1]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    dataset = ImageNetADataset(root_dir=root_dir, transform=augmenter)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388175a2",
   "metadata": {},
   "source": [
    "# So\n",
    "\n",
    "1. base model\n",
    "2. tpt model\n",
    "3. prompt learning (tpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c254c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_augmix,\n",
    "    n_views=63,\n",
    ")\n",
    "\n",
    "dataloader, dataset = ResnetA(augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f72c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([7, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([3, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import clip\n",
    "import clip.model\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "    # You can add more, but the `kornia_preprocess` should be modified accordingly\n",
    "    # ViTB16: str = \"ViT-B/16\"\n",
    "    # RN50: str = \"RN50\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: clip.model.CLIP,\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.dtype = clip_model.visual.conv1.weight.dtype\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "        self.__init_ctx_from_prompt(base_prompt=base_prompt)\n",
    "\n",
    "    def __init_ctx_from_prompt(self, base_prompt: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[49406]]).to(self.device)  # SOT\n",
    "        c_token_eot = torch.tensor([[49407]]).to(self.device)  # EOT\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = clip.tokenize(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = clip.tokenize(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = clip.tokenize(self.class_names).to(self.device)\n",
    "\n",
    "        # self.tokenized_class_names_len = torch.argmax(\n",
    "        #     (tokenized_class_names == 0).int(), dim=1, keepdim=True\n",
    "        # )\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = clip.tokenize(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        )\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = self.token_embedding(c_token_sot)\n",
    "            self.embedded_eot = self.token_embedding(c_token_eot)\n",
    "            self.embedded_pad = self.token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = self.token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = self.token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = self.token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            clean_embed = embedded_class_names[i][mask[i]]  # [num_valid_tokens, 512]\n",
    "            clean_embeddings.append(clean_embed)\n",
    "            print(clean_embed.shape)\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # Padding size: tensor([-950], device='cuda:0')\n",
    "            # embedded class names shape: torch.Size([200, 77, 512])\n",
    "            # embedded prefix shape: torch.Size([4, 512])\n",
    "            # Tokenized class names len: tensor([3], device='cuda:0')\n",
    "            # embedded suffix shape: torch.Size([0, 512])\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - self.tokenized_class_names_len[i]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            )\n",
    "\n",
    "            # embedded sot shape: torch.Size([1, 1, 512])\n",
    "            # embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            # embedded class names shape: torch.Size([1, 3, 77, 512])\n",
    "            # embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            # embedded eot shape: torch.Size([1, 1, 512])\n",
    "\n",
    "            leng = self.tokenized_class_names_len[i]\n",
    "\n",
    "            print(f\"embedded sot shape: {self.embedded_sot.shape}\")\n",
    "            print(f\"embedded prefix shape: {self.embedded_prefix.unsqueeze(0).shape}\")\n",
    "            print(\n",
    "                f\"embedded class names shape: {self.embedded_class_names[i : leng].unsqueeze(0).shape}\"\n",
    "            )\n",
    "            print(f\"embedded suffix shape: {self.embedded_suffix.unsqueeze(0).shape}\")\n",
    "            print(f\"embedded eot shape: {self.embedded_eot.shape}\")\n",
    "\n",
    "            # break\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    self.embedded_class_names[i:leng].unsqueeze(0),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    # self.embedded_pad.repeat(1, padding_size),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            print(f\"Prompt shape: {prompt.shape}\")\n",
    "\n",
    "            break\n",
    "            prompts.append(prompt)\n",
    "        # prompts = torch.stack(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "\n",
    "        self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "        self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, class_names: List[str], arch: CLIPModels = CLIPModels.ViTB32, device=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(arch, device=self.device)\n",
    "\n",
    "        self.dtype = clip_model.visual.conv1.weight.dtype\n",
    "        # self.clip = clip_model\n",
    "        self.image_encoder = clip_model.visual\n",
    "\n",
    "        self.logit_scale = clip_model.logit_scale.data\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "        self.tmp_token_embedding = clip_model.token_embedding\n",
    "        tmp_prompt = torch.cat(\n",
    "            [clip.tokenize(f\"a photo of a {c}\") for c in class_names]\n",
    "        ).to(device)\n",
    "\n",
    "        # print(f\"DIOCAN DEVICE: {self.device}\")\n",
    "        self.tmp_prompt = tmp_prompt.to(\"cuda\")\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "        #\n",
    "\n",
    "    def __encode_text(\n",
    "        self, tokenized_prompt: torch.Tensor, embedded_prompt: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the text prompt using the CLIP model.\n",
    "            The tokenizer is external.\n",
    "\n",
    "        Source: CLIP source code. model.py#L343\n",
    "        \"\"\"\n",
    "        x = embedded_prompt + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLP -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompt.argmax(dim=-1)]\n",
    "            @ self.text_projection\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the image using the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            image_features (torch.Tensor): Normalized encoded image features.\n",
    "        \"\"\"\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    def __select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def __avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def clip_inference(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            image_features = self.__encode_image(image)\n",
    "\n",
    "        embedded_prompt = self.tmp_token_embedding(self.tmp_prompt).type(self.dtype)\n",
    "\n",
    "        txt_features = self.__encode_text(self.tmp_prompt, embedded_prompt)\n",
    "        txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ txt_features.t()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def predict(self, input):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "my_tpt = TPT(class_names=dataset.class_code_to_label.values())\n",
    "# my_tpt = TPT()\n",
    "# print(f\"Total parameters: {sum(p.numel() for p in my_tpt.parameters())}\")\n",
    "# print(\n",
    "#     f\"Trainable parameters: {sum(p.numel() for p in my_tpt.parameters() if p.requires_grad)}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0839f03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tpt.prompt_learner.embedded_pad[:, :70]\n",
    "\n",
    "my_tpt.prompt_learner.embedded_eot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0636b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 77, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tpt.tmp_token_embedding(my_tpt.tmp_prompt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc555525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded sot shape: torch.Size([1, 1, 512])\n",
      "embedded prefix shape: torch.Size([1, 4, 512])\n",
      "embedded class names shape: torch.Size([1, 3, 77, 512])\n",
      "embedded suffix shape: torch.Size([1, 0, 512])\n",
      "embedded eot shape: torch.Size([1, 1, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmy_tpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 138\u001b[0m, in \u001b[0;36mTPTPromptLearner.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedded eot shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedded_eot\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_sot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_prefix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_class_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mleng\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_suffix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded_eot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# self.embedded_pad.repeat(1, padding_size),\u001b[39;49;00m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 4"
     ]
    }
   ],
   "source": [
    "my_tpt.prompt_learner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0862fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7500 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in tqdm(dataloader):\n",
    "    image = image.to(device)\n",
    "    logits = my_tpt.clip_inference(image)\n",
    "\n",
    "    print(logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965bd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49406, 49407,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], dtype=torch.int32)\n",
      "tensor(49406, dtype=torch.int32)\n",
      "tensor(49407, dtype=torch.int32)\n",
      "tensor(0, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "extractor = clip.tokenize(\"\")[0]\n",
    "pad_token = extractor[3]\n",
    "sot_token = extractor[0]\n",
    "eot_token = extractor[1]\n",
    "\n",
    "print(extractor)\n",
    "print(extractor[0])\n",
    "print(extractor[1])\n",
    "print(extractor[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8210e888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 320, 1125,  539,  320], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_prefix = clip.tokenize(\"a photo of a\")\n",
    "\n",
    "tokenized_prefix1 = tokenized_prefix[\n",
    "    (tokenized_prefix != sot_token)\n",
    "    & (tokenized_prefix != eot_token)\n",
    "    & (tokenized_prefix != pad_token)\n",
    "]\n",
    "\n",
    "\n",
    "print(tokenized_prefix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32dbee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"a photo of a [CLS]\"\n",
    "class_names = dataset.class_code_to_label.values()\n",
    "\n",
    "\n",
    "# Split the base prompt into prefix and suffix\n",
    "promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "# \"Clean\" PAD, SOT and EOT tokens\n",
    "c_token_sot = torch.tensor([[49406]])  # SOT\n",
    "c_token_eot = torch.tensor([[49407]])  # EOT\n",
    "c_token_pad = torch.tensor([[0]])  # PAD\n",
    "\n",
    "# Tokenize prefix, suffix and class names\n",
    "tokenized_prefix = clip.tokenize(promt_prefix)\n",
    "tokenized_suffix = clip.tokenize(promt_suffix)\n",
    "\n",
    "# remove PAD, SOT and EOT tokens\n",
    "# Extract \"clean\" tokens\n",
    "c_tokenized_prefix = tokenized_prefix[\n",
    "    (tokenized_prefix != c_token_sot)\n",
    "    & (tokenized_prefix != c_token_eot)\n",
    "    & (tokenized_prefix != c_token_pad)\n",
    "]\n",
    "c_tokenized_suffix = tokenized_suffix[\n",
    "    (tokenized_suffix != c_token_sot)\n",
    "    & (tokenized_suffix != c_token_eot)\n",
    "    & (tokenized_suffix != c_token_pad)\n",
    "]\n",
    "\n",
    "tokenized_class_names = clip.tokenize(class_names)\n",
    "tokenized_class_names_len = torch.argmax(\n",
    "    (tokenized_class_names == 0).int(), dim=1, keepdim=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e252837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1])\n",
      "torch.Size([200, 77])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_class_names_len.shape)\n",
    "print(tokenized_class_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298b3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1d59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
