{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9629e639",
   "metadata": {},
   "source": [
    "# Clip + Coop + TPT Delivery\n",
    "\n",
    "*Group Composition*\n",
    "- Emmanuele V. Coppola\n",
    "- Ivan Doná\n",
    "- Ettore Saggiorato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f30275",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb18f6",
   "metadata": {},
   "source": [
    "#### Installing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef3aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/openai/CLIP.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-62iros0r\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m fatal: unable to access 'https://github.com/openai/CLIP.git/': Could not resolve host: github.com\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/openai/CLIP.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-62iros0r\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ftfy regex tqdm scikit-learn scikit-image\n",
    "!pip install -q git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e292b95",
   "metadata": {},
   "source": [
    "#### Download and extract the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060c9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08708732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com\n"
     ]
    }
   ],
   "source": [
    "#Download metadata for class labelling\n",
    "!curl https://raw.githubusercontent.com/modestyachts/ImageNetV2/refs/heads/master/data/metadata/class_info.json -o datasets/imagenetv2-matched-frequency-format-val/class_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775e88b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘datasets’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: imagenet-a.tar: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!mkdir datasets\n",
    "!tar -xvf imagenet-a.tar -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e3036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clip\n",
    "import torch\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3358e0",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ec6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducibility steps by https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "\n",
    "def show_augmented_images(dataset, num_images=16, label=\"augmented\"):\n",
    "    \"\"\"\n",
    "    Show a grid of augmented images from the dataset.\n",
    "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
    "    \"\"\"\n",
    "    assert int(\n",
    "        num_images ** 0.5) ** 2 == num_images, \"num_images must be a perfect square\"\n",
    "\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        img_tensor, _ = dataset[i]\n",
    "        images.append(img_tensor)\n",
    "\n",
    "    # Make a grid of images\n",
    "    grid = torchvision.utils.make_grid(\n",
    "        images, nrow=int(num_images ** 0.5), padding=2)\n",
    "    npimg = grid.numpy()\n",
    "\n",
    "    # Convert and display\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{num_images} Augmented Versions (including original) of {label}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_augmented_images_from_id_list(dataset, id_list, title=\"\"):\n",
    "    \"\"\"\n",
    "    Show a grid of augmented images from the dataset.\n",
    "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for i in id_list:\n",
    "        img_tensor, _ = dataset[i]\n",
    "        images.append(img_tensor)\n",
    "\n",
    "    # Make a grid of images\n",
    "    grid = torchvision.utils.make_grid(\n",
    "        images, nrow=int(len(id_list) ** 0.5), padding=2)\n",
    "    npimg = grid.numpy()\n",
    "\n",
    "    # Convert and display\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{len(id_list)} Augmented Versions: {title}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_tensor_images(tensor_images):\n",
    "    \"\"\"\n",
    "    Show a grid of augmented images from the dataset.\n",
    "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a grid of images\n",
    "    grid = torchvision.utils.make_grid(\n",
    "        tensor_images, nrow=int(len(tensor_images) ** 0.5), padding=2)\n",
    "    npimg = grid.numpy()\n",
    "\n",
    "    # Convert and display\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{len(tensor_images)} Augmented Versions')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def LINE():\n",
    "    return sys._getframe(1).f_lineno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635b59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fc3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = _Tokenizer()\n",
    "vis_net, basic_image_transformations = clip.load('ViT-B/16', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe32c6a",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1a2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    def __init__(self, root_dir='datasets/imagenet-a', transform=basic_image_transformations):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load class code to name mapping from README.txt\n",
    "        self.class_code_to_name = self._load_class_mapping(\n",
    "            os.path.join(root_dir, 'README.txt'))\n",
    "\n",
    "        # Map class codes to integer labels\n",
    "        self.class_codes = sorted([\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and d in self.class_code_to_name\n",
    "        ])\n",
    "        self.class_code_to_idx = {code: idx for idx,\n",
    "                                  code in enumerate(self.class_codes)}\n",
    "\n",
    "        # Collect all image paths and labels\n",
    "        self.samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_folder = os.path.join(root_dir, class_code)\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    self.samples.append((path, label))\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        mapping = {}\n",
    "        with open(readme_path, 'r') as f:\n",
    "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1adb36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    def __init__(self, root_dir='datasets/imagenetv2-matched-frequency-format-val', transform=basic_image_transformations, use_imagenet_a_classes = True, imagenet_a = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_imagenet_a = use_imagenet_a_classes\n",
    "        \n",
    "        if use_imagenet_a_classes:\n",
    "            assert type(imagenet_a) == ImageNetA, \"imagenet_a_classes set to TRUE without passing imagenet_a object\"\n",
    "            imagenet_a_class_code_to_idx = imagenet_a.class_code_to_idx\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, 'class_info.json'), use_imagenet_a_classes, imagenet_a_class_code_to_idx)\n",
    "            self.class_code_to_name = copy.deepcopy(imagenet_a.class_code_to_name)\n",
    "            \n",
    "        else:\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, 'class_info.json'), use_imagenet_a_classes,None)\n",
    "            self.class_code_to_name = {\n",
    "                idx: self.v2id_to_info[\"label\"] for idx in self.v2id_to_info.keys()}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.samples = []\n",
    "        for v2_class_code in self.v2id_to_info.keys():\n",
    "            class_folder = os.path.join(root_dir, str(v2_class_code))\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    if use_imagenet_a_classes:\n",
    "                        self.samples.append((path, self.v2id_to_info[v2_class_code][\"label_id\"]))\n",
    "                    else:\n",
    "                        self.samples.append((path, v2_class_code))\n",
    "        \n",
    "    def _load_class_mapping(self, infofile_path, use_imagenet_a_classes, imagenet_a_class_code_to_idx:dict[str, int]):\n",
    "        mapping = {}\n",
    "        with open(infofile_path) as f:\n",
    "            data = json.load(f)\n",
    "            for idx,item in enumerate(data):\n",
    "                if use_imagenet_a_classes:\n",
    "                    if item[\"wnid\"] in imagenet_a_class_code_to_idx.keys():\n",
    "                        mapping[item[\"cid\"]] = {\"label_id\": imagenet_a_class_code_to_idx[item[\"wnid\"]],\n",
    "                                                \"ia_code\": item[\"wnid\"], \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\")}\n",
    "                else:\n",
    "                    mapping[item[\"cid\"]] = {\"label\":item[\"synset\"][0].lower().replace(\n",
    "                        \" \", \"_\")}\n",
    "        return mapping\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf95115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(dataset, train_percentage=0.5, validation_percentage=0.25):\n",
    "    # Load data\n",
    "\n",
    "    # Create train validation and test samples\n",
    "    num_samples = len(dataset)\n",
    "    training_sample = int(num_samples * train_percentage + 1)\n",
    "    validation_sample = int(num_samples * validation_percentage)\n",
    "    test_sample = num_samples - training_sample - validation_sample\n",
    "\n",
    "    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [training_sample, validation_sample, test_sample])\n",
    "\n",
    "    return (training_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "\n",
    "def get_data(training_dataset, validation_dataset, test_dataset, batch_size=64, transform=None, num_workers=8):\n",
    "    \"\"\"\n",
    "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
    "    \"\"\"\n",
    "\n",
    "    if not transform:\n",
    "        transform = torchvision.transforms.Compose(\n",
    "            [torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
    "        generator=g)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
    "        generator=g)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
    "        generator=g)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def embed_dataset_classnames(dataset:ImageNetA, model, templates=[\"a photo of a {}.\"]):\n",
    "    \"\"\"\n",
    "    Embed the classnames in the prompt template.\n",
    "    Return the classnames and the normalized textual features.\n",
    "    \"\"\"\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    classnames = dataset.class_code_to_name.values()\n",
    "\n",
    "    texts_z_views = []\n",
    "    for template in templates:\n",
    "        descriptions = [template.format(c) for c in classnames]\n",
    "        text_tokens = clip.tokenize(descriptions).to(DEVICE)\n",
    "\n",
    "        # Get the normalized textual features\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "            texts_z_views.append(texts_z)\n",
    "\n",
    "    # Evaluate the mean representation\n",
    "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
    "\n",
    "    # Renormalise\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return classnames, texts_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51898b3",
   "metadata": {},
   "source": [
    "#### Initialize datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a6a94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = ImageNetA()\n",
    "dataset_v2 = ImageNetV2(imagenet_a=dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3f1e7",
   "metadata": {},
   "source": [
    "### basic Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4110b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(model_class, dataset):\n",
    "    classnames, _ = embed_dataset_classnames(\n",
    "        dataset, vis_net)\n",
    "    n_ctx = 4\n",
    "    ctx_init = \"\"\n",
    "    class_token_position = \"end\"\n",
    "    csc = False\n",
    "    coop_net = model_class(\n",
    "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
    "    ).to(DEVICE)\n",
    "    return coop_net\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model.load_state_dict(torch.load(\n",
    "        \"./working_directory/model.pth\", weights_only=True))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf776eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(\n",
    "            dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31f6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Basic mechanics are taken from the Lab Number 3 of 2024/2025 accademic year\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(\n",
    "                clip_model.token_embedding.weight.device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        print(classnames)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        # print(\"+++\")\n",
    "        # print(\"Prompts:\")\n",
    "        # for p in prompts:\n",
    "        #     print(p)\n",
    "        # print(\"+++\")\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(\n",
    "            clip_model.token_embedding.weight.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\n",
    "            \"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "        self.ctx_checkpoint = ctx_vectors.detach().clone()\n",
    "\n",
    "    def reset_ctx(self):  # https://discuss.pytorch.org/t/reset-model-weights/19180\n",
    "        with torch.no_grad():\n",
    "            self.ctx.copy_(self.ctx_checkpoint)\n",
    "\n",
    "        self.ctx.requires_grad = True\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        with torch.no_grad():\n",
    "            self.ctx_checkpoint.copy_(self.ctx)\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i: i + 1, :, :]\n",
    "                class_i = suffix[i: i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i: i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i: i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i: i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i: i + 1, :, :]\n",
    "                class_i = suffix[i: i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i: i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i: i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d8ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip.model\n",
    "\n",
    "\n",
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model\n",
    "\n",
    "        self.prompt_learner = PromptLearner(\n",
    "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def reset_ctx(self):\n",
    "        self.prompt_learner.reset_ctx()\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        self.prompt_learner.set_ctx_checkpoint()\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / \\\n",
    "            image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / \\\n",
    "            text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01ad09",
   "metadata": {},
   "source": [
    "### COOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeed0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(net: OurCLIP, data_loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, cost_function, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Training step (for CoOp).\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(data_loader, desc=\"Training\", position=0,\n",
    "                leave=True, total=len(data_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        \"\"\" print(f\"input shape {inputs.shape}\")\n",
    "        print(f\"input type {inputs.dtype}\")\n",
    "        print(f\"labels{targets}\") \"\"\"\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        # max() returns (maximum_value, index_of_maximum_value)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(),\n",
    "                         train_acc=cumulative_accuracy / samples * 100)\n",
    "        pbar.update(1)\n",
    "        del inputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(data_loader, desc=\"Testing\", position=0,\n",
    "                leave=True, total=len(data_loader))\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(inputs)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753140db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coop(\n",
    "    net,\n",
    "    dataset_splits,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "    skip_test=False\n",
    "):\n",
    "    \"\"\"\n",
    "    @param: dataset_class\n",
    "    @param: dataset_splits tuple that contains (training, validation, test) \"\"\"\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        dataset_splits[0], dataset_splits[1], dataset_splits[2], transform=basic_image_transformations, batch_size=batch_size)\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {\"params\": net.parameters()}\n",
    "    ], lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    # Define the cost function\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # Computes evaluation results before training\n",
    "    if not skip_test:\n",
    "        print(\"Before training:\")\n",
    "        train_loss, train_accuracy = test_step(\n",
    "            net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    if not skip_test:\n",
    "        print(\"After training:\")\n",
    "\n",
    "        train_loss, train_accuracy = test_step(\n",
    "            net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7f2fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' coop_net = new_model(OurCLIP, dataset_v2)\\nsplitted_datasets = get_dataset_split(dataset_v2)\\nmain_coop(coop_net, splitted_datasets, batch_size=16, skip_test = True)\\ntorch.save(coop_net.state_dict(), \"./working_directory/model_coop.pth\") '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" coop_net = new_model(OurCLIP, dataset_v2)\n",
    "splitted_datasets = get_dataset_split(dataset_v2)\n",
    "main_coop(coop_net, splitted_datasets, batch_size=16, skip_test = True)\n",
    "torch.save(coop_net.state_dict(), \"./working_directory/model_coop.pth\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fd578",
   "metadata": {},
   "source": [
    "### TPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928c01c",
   "metadata": {},
   "source": [
    "##### Augmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd3fe273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AugMix-style transform\n",
    "def get_augmix_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Basic original transform (non-augmented)\n",
    "original_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Wrapper Dataset that takes a base dataset + index of the sample to augment\n",
    "\n",
    "\n",
    "class AugmentSingleSampleDataset(Dataset):\n",
    "    def __init__(self, base_dataset, sample_idx, num_augments=63):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sample_idx = sample_idx\n",
    "        self.num_augments = num_augments\n",
    "        self.augmix_transform = get_augmix_transform()\n",
    "        self.original_transform = original_transform\n",
    "\n",
    "        # Extract the image once to avoid loading it 64 times\n",
    "        image, label = self.base_dataset[self.sample_idx]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            self.image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            self.image = image\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_augments + 1  # 63 augments + 1 original\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            image = self.original_transform(self.image)\n",
    "        else:\n",
    "            image = self.augmix_transform(self.image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf01fee",
   "metadata": {},
   "source": [
    "##### TPT Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c8a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_confident_samples(logits, top_p):\n",
    "    \"\"\"\n",
    "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
    "    \"\"\"\n",
    "    assert 0 <= top_p < 1, \"The value must be between 0 and 1\"\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[\n",
    "        :int(batch_entropy.size()[0] * top_p)]\n",
    "    return logits[idx], idx\n",
    "\n",
    "\n",
    "def compute_avg_entropy(outputs):\n",
    "    \"\"\"\n",
    "    Compute marginal entropy of samples and return the average.\n",
    "    \"\"\"\n",
    "    # Calculate probabilities from logits\n",
    "    probs = outputs.softmax(dim=1)\n",
    "    # To avoid log(0), clamp probabilities to a minimum value\n",
    "    probs = probs.clamp(min=1e-9)\n",
    "    entropy = -(probs * probs.log()).sum(dim=1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfdf2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_tpt(net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16):\n",
    "    \"\"\"\n",
    "    @param net takes a OurClip model type\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(range(len(dataset)), desc=\"TPT_testing\", position=0,\n",
    "                leave=True, total=len(dataset))\n",
    "    # Iterate over the indices of the test set\n",
    "    try:\n",
    "        for sample_idx in pbar:  # Iterate through indices\n",
    "            net.reset_ctx()\n",
    "            optimizer.load_state_dict(optimizer_state_dict)\n",
    "            # print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
    "\n",
    "            # Create augmented dataset for the current sample\n",
    "            aug_data = AugmentSingleSampleDataset(\n",
    "                dataset, sample_idx, num_augments=num_aug)  # Pass the base dataset and index\n",
    "\n",
    "            # Create a DataLoader for the augmented samples of this single image\n",
    "            aug_dataloader = torch.utils.data.DataLoader(\n",
    "                aug_data, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker,\n",
    "                generator=g)\n",
    "\n",
    "            # Process the augmented images for this sample\n",
    "            all_outputs = []\n",
    "            for images, labels in aug_dataloader:\n",
    "                try:\n",
    "                    with autocast():\n",
    "                        # print(f\"size batch {len(images)}\")\n",
    "                        images = images.to(DEVICE)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        outputs = net(images)  # Use the provided net\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        # cpu_outputs = outputs.to(\"cpu\")\n",
    "                        all_outputs.append(outputs)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "                        \"\"\" del images\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        print(torch.cuda.mem_get_info(), LINE()) \"\"\"\n",
    "\n",
    "                except:\n",
    "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                    raise\n",
    "\n",
    "            # Get the original label for this sample\n",
    "            original_image, target = dataset[sample_idx]\n",
    "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
    "            # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "            # Make target a tensor and move to device\n",
    "            target = torch.tensor([target]).to(DEVICE)\n",
    "\n",
    "            # Concatenate outputs from all batches for this sample\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "            # Select confident samples and compute average entropy\n",
    "            top_outputs, _ = select_confident_samples(all_outputs, 0.2)\n",
    "            loss = compute_avg_entropy(top_outputs)\n",
    "            # Loss computation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(original_image)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += original_image.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        del all_outputs\n",
    "        del aug_data\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c28d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_test(net, dataset:Dataset, run_name=\"tpt1\", num_aug=63, batch_size=64):\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    net.set_ctx_checkpoint()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "        # print(f\"{name}is in {param.requires_grad}\")\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
    "    # , weight_decay=wd, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)\n",
    "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    print(\"Test tpt:\")\n",
    "    test_loss, test_accuracy = test_step_tpt(\n",
    "        net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=num_aug, batch_size=batch_size)\n",
    "\n",
    "    # Closes the logger\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b5b3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a generic context\n",
      "Initial context: 'X X X X'\n",
      "Number of context words (tokens): 4\n",
      "dict_values(['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn'])\n"
     ]
    }
   ],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_a)\n",
    "coop_net = load_model(coop_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a41b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6794510336, 8212709376) 11\n",
      "(6794510336, 8212709376) 18\n",
      "Test tpt:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPT_testing: 100%|██████████| 7500/7500 [1:25:34<00:00,  1.46it/s, test_acc=55.2]\n"
     ]
    }
   ],
   "source": [
    "tpt_test(coop_net, dataset_a, batch_size=64, num_aug=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353289c",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f008305",
   "metadata": {},
   "source": [
    "### Film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60df16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLMModulation(nn.Module):\n",
    "    def __init__(self, text_dim, image_dim):\n",
    "        super().__init__()\n",
    "        self.film = nn.Sequential(\n",
    "            nn.Linear(text_dim, image_dim * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_feat, text_feat):\n",
    "        gamma_beta = self.film(text_feat)  # [B, 2 * C]\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=-1)  # [B, C] each\n",
    "        return gamma * image_feat + beta\n",
    "\n",
    "\n",
    "class FilmCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "\n",
    "        self.prompt_learner = PromptLearner(\n",
    "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "        self.film_modulation = FiLMModulation(\n",
    "            text_dim=512, image_dim=512)  # 512 is CLIP's embedding dim\n",
    "\n",
    "    def reset_ctx(self):\n",
    "        self.prompt_learner.reset_ctx()\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        self.prompt_learner.set_ctx_checkpoint()\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)  # [B, 512]\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(\n",
    "            prompts, tokenized_prompts)  # [K, 512]\n",
    "\n",
    "        # Average text features to get a global modulation vector\n",
    "        \"\"\"  global_text_feat = text_features.mean(dim=0, keepdim=True)  # [1, 512]\n",
    "        global_text_feat = global_text_feat.expand(\n",
    "            image_features.size(0), -1)  # [B, 512] \"\"\"\n",
    "\n",
    "        # Cosine similarity between each image and each class text feature\n",
    "        sim = image_features @ text_features.t()  # [B, K]\n",
    "\n",
    "        # Softmax over classes to get weights\n",
    "        weights = sim.softmax(dim=1)  # [B, K]\n",
    "\n",
    "        # Weighted average of text features per image\n",
    "        global_text_feat = weights @ text_features  # [B, 512]\n",
    "\n",
    "        # Modulate image features\n",
    "        image_features = self.film_modulation(image_features, global_text_feat)\n",
    "\n",
    "        # Normalize\n",
    "        image_features = image_features / \\\n",
    "            image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / \\\n",
    "            text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e79397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_film_coop(\n",
    "    net,\n",
    "    dataset_splits,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "    skip_test=False\n",
    "):\n",
    "    \"\"\"\n",
    "    @param: dataset_class\n",
    "    @param: dataset_splits tuple that contains (training, validation, test) \"\"\"\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        dataset_splits[0], dataset_splits[1], dataset_splits[2], transform=basic_image_transformations, batch_size=batch_size)\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder adn film Module\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "ImageNetA\n",
    "    # Define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # Computes evaluation results before training\n",
    "    if not skip_test:\n",
    "        print(\"Before training:\")\n",
    "        train_loss, train_accuracy = test_step(\n",
    "            net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "        print(\n",
    "            f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    print(\"Turning off gradients prompt learner and activating the FILM ones \")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"film_modulation\" in name:\n",
    "            param.requires_grad_(True)\n",
    "            print(f\"film grad {param.requires_grad}\")\n",
    "        else:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "\n",
    "    train_loss, train_accuracy = test_step(\n",
    "        net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "    print(\n",
    "        f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(\n",
    "        f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(\n",
    "        f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "425e0c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' main_film_coop(coop_film_net, splitted_datasets, batch_size=16, skip_test=True)\\ntorch.save(coop_film_net.state_dict(), \"./working_directory/film_model.pth\") '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" coop_film_net, preprocess = new_model(FilmCLIP)\n",
    "splitted_datasets = get_dataset_split(ImageNetA)\n",
    "coop_film_net.load_state_dict(torch.load(\n",
    "    \"./working_directory/film_model.pth\", weights_only=True))\n",
    "coop_film_net.eval() \"\"\"\n",
    "\n",
    "\"\"\" main_film_coop(coop_film_net, splitted_datasets, batch_size=16, skip_test=True)\n",
    "torch.save(coop_film_net.state_dict(), \"./working_directory/film_model.pth\") \"\"\"\n",
    "\n",
    "# tpt_test(coop_film_net, splitted_datasets, batch_size=64, num_aug=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed221f",
   "metadata": {},
   "source": [
    "### Average Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba4b3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_avg_vis_step_tpt(net: OurCLIP, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16):\n",
    "    \"\"\"\n",
    "    @param net takes a OurClip model type\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(range(len(dataset)), desc=\"TPT_avg_vis_testing\", position=0,\n",
    "                leave=True, total=len(dataset))\n",
    "    # Iterate over the indices of the test set\n",
    "    try:\n",
    "        for sample_idx in pbar:  # Iterate through indices\n",
    "            net.reset_ctx()\n",
    "            optimizer.load_state_dict(optimizer_state_dict)\n",
    "            # print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
    "\n",
    "            # Create augmented dataset for the current sample\n",
    "            aug_data = AugmentSingleSampleDataset(\n",
    "                dataset.dataset, sample_idx, num_augments=num_aug)  # Pass the base dataset and index\n",
    "\n",
    "            # Create a DataLoader for the augmented samples of this single image\n",
    "            aug_dataloader = torch.utils.data.DataLoader(\n",
    "                aug_data, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker,\n",
    "                generator=g)\n",
    "\n",
    "            # Process the augmented images for this sample\n",
    "            all_outputs = []\n",
    "            for images, labels in aug_dataloader:\n",
    "                try:\n",
    "                    with autocast():\n",
    "                        images = images.to(DEVICE)\n",
    "                        outputs = net(images)  # Use the provided net\n",
    "                        all_outputs.append(outputs)\n",
    "\n",
    "                except:\n",
    "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                    raise\n",
    "\n",
    "            # Get the original label for this sample\n",
    "            original_image, target = dataset[sample_idx]\n",
    "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            # Make target a tensor and move to device\n",
    "            target = torch.tensor([target]).to(DEVICE)\n",
    "\n",
    "            # Concatenate outputs from all batches for this sample\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "            # Select confident samples and compute average entropy\n",
    "            top_outputs, top_image_idx = select_confident_samples(\n",
    "                all_outputs, 0.2)\n",
    "            loss = compute_avg_entropy(top_outputs)\n",
    "            # Loss computation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Building average embedding\n",
    "            \"\"\" aug_dataloader = torch.utils.data.DataLoader(\n",
    "                dataset.dataset[top_image_idx], batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker,\n",
    "                generator=g) \"\"\"\n",
    "\n",
    "            visual_outputs = []\n",
    "            images = []\n",
    "            for idx in top_image_idx:\n",
    "                image, labels = aug_data[idx]\n",
    "                images.append(image)\n",
    "            try:\n",
    "                images = torch.stack(images).to(DEVICE)\n",
    "\n",
    "                with autocast():\n",
    "                    visual_outputs.append(net.image_encoder(\n",
    "                        images))  # Use the provided net\n",
    "\n",
    "            except:\n",
    "                torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                raise\n",
    "\n",
    "            average_visual = torch.stack(visual_outputs).mean(dim=0)\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                prompts = net.prompt_learner()\n",
    "                tokenized_prompts = net.tokenized_prompts\n",
    "                text_features = net.text_encoder(\n",
    "                    prompts, tokenized_prompts)\n",
    "\n",
    "                image_features = average_visual / \\\n",
    "                    average_visual.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / \\\n",
    "                    text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                logit_scale = net.logit_scale.exp()\n",
    "                logits = logit_scale * image_features @ text_features.t()\n",
    "                # outputs = net(original_image)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += original_image.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        del all_outputs\n",
    "        del aug_data\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "164c3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_avg_vis_test(net, datasets, run_name=\"tpt_vis\", num_aug=63, batch_size=64):\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    net.set_ctx_checkpoint()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "        # print(f\"{name}is in {param.requires_grad}\")\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
    "    # , weight_decay=wd, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)\n",
    "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    print(\"Test tpt:\")\n",
    "    test_loss, test_accuracy = test_avg_vis_step_tpt(\n",
    "        net, datasets[2], optimizer, optimizer_state_dict, log_writer, num_aug=num_aug, batch_size=batch_size)\n",
    "\n",
    "    # Closes the logger\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0f0fecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' coop_vis_net, preprocess = new_model(OurCLIP, ImageNetA)\\nsplitted_datasets = get_dataset_split(ImageNetA)\\ncoop_vis_net.load_state_dict(torch.load(\\n    \"./working_directory/model.pth\", weights_only=True))\\ncoop_vis_net.eval()\\ntpt_avg_vis_test(coop_vis_net, splitted_datasets) '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" coop_vis_net, preprocess = new_model(OurCLIP, ImageNetA)\n",
    "splitted_datasets = get_dataset_split(ImageNetA)\n",
    "coop_vis_net.load_state_dict(torch.load(\n",
    "    \"./working_directory/model.pth\", weights_only=True))\n",
    "coop_vis_net.eval()\n",
    "tpt_avg_vis_test(coop_vis_net, splitted_datasets) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
