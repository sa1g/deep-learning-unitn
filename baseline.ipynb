{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6fGDwsTqOuC"
      },
      "source": [
        "### Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39noccGxC8OO",
        "outputId": "c2178cd5-820a-4ef2-c004-6561e44373ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ftfy regex tqdm scikit-learn scikit-image\n",
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smXq0XTCqWsh"
      },
      "source": [
        "### Download and extract the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCIUTyLQC-CL",
        "outputId": "3ba47ad2-89e4-4562-c4d2-f1e1a447daf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hpE04mRnDApP",
        "outputId": "14cafcbb-d7f0-47ee-edd5-78a2b5343291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n",
            "tar: imagenet-a.tar: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "!mkdir datasets\n",
        "!tar -xvf imagenet-a.tar -C datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eauxAnMuCpOf"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p71Vb0sCpOf",
        "outputId": "b4c3b3fd-4be8-4225-bbf4-662d087f7156"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as v2\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4LMjLjjkCvIV"
      },
      "outputs": [],
      "source": [
        "class ImageNetA(Dataset):\n",
        "    def __init__(self, root_dir='datasets/imagenet-a', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load class code to name mapping from README.txt\n",
        "        self.class_code_to_name = self._load_class_mapping(os.path.join(root_dir, 'README.txt'))\n",
        "\n",
        "        # Map class codes to integer labels\n",
        "        self.class_codes = sorted([\n",
        "            d for d in os.listdir(root_dir)\n",
        "            if os.path.isdir(os.path.join(root_dir, d)) and d in self.class_code_to_name\n",
        "        ])\n",
        "        self.class_code_to_idx = {code: idx for idx, code in enumerate(self.class_codes)}\n",
        "\n",
        "        # Collect all image paths and labels\n",
        "        self.samples = []\n",
        "        for class_code in self.class_codes:\n",
        "            class_folder = os.path.join(root_dir, class_code)\n",
        "            for fname in os.listdir(class_folder):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    path = os.path.join(class_folder, fname)\n",
        "                    label = self.class_code_to_idx[class_code]\n",
        "                    self.samples.append((path, label))\n",
        "\n",
        "    def _load_class_mapping(self, readme_path):\n",
        "        mapping = {}\n",
        "        with open(readme_path, 'r') as f:\n",
        "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
        "            for line in lines:\n",
        "                parts = line.strip().split(' ', 1)\n",
        "                if len(parts) == 2:\n",
        "                    code, name = parts\n",
        "                    mapping[code] = name\n",
        "        return mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.samples[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x748ce55fb010>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Reproducibility steps by https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgSUnteCCpOh"
      },
      "source": [
        "### Import the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0cRqrP_2Dd41"
      },
      "outputs": [],
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uvGkdr9XDe3A"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WsSf3DgbDgMx"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "class PromptLearner(nn.Module): # Basic mechanics are taken from the Lab Number 3 of 2024/2025 accademic year\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        print(classnames)\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "        self.ctx_checkpoint = ctx_vectors.detach().clone()\n",
        "        \n",
        "\n",
        "    def reset_ctx(self):  # https://discuss.pytorch.org/t/reset-model-weights/19180\n",
        "        with torch.no_grad():\n",
        "            self.ctx.copy_(self.ctx_checkpoint)\n",
        "\n",
        "        self.ctx.requires_grad = True\n",
        "        \n",
        "    def set_ctx_checkpoint(self):\n",
        "        with torch.no_grad():\n",
        "            self.ctx_checkpoint.copy_(self.ctx)\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ry7I6MhNDhoc"
      },
      "outputs": [],
      "source": [
        "class OurCLIP(nn.Module):\n",
        "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
        "        # clip_model = clip_model.cpu()\n",
        "        clip_model = clip_model\n",
        "\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "    def reset_ctx(self):\n",
        "        self.prompt_learner.reset_ctx()\n",
        "    \n",
        "    def set_ctx_checkpoint(self):\n",
        "        self.prompt_learner.set_ctx_checkpoint()\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image)\n",
        "\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2nxZZkDCBi2"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b-_SNDgyi3Js"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "\n",
        "\n",
        "def show_augmented_images(dataset, num_images=16, label = \"augmented\"):\n",
        "    \"\"\"\n",
        "    Show a grid of augmented images from the dataset.\n",
        "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
        "    \"\"\"\n",
        "    assert int(\n",
        "        num_images ** 0.5) ** 2 == num_images, \"num_images must be a perfect square\"\n",
        "\n",
        "    images = []\n",
        "    for i in range(num_images):\n",
        "        img_tensor, _ = dataset[i]\n",
        "        images.append(img_tensor)\n",
        "\n",
        "    # Make a grid of images\n",
        "    grid = torchvision.utils.make_grid(\n",
        "        images, nrow=int(num_images ** 0.5), padding=2)\n",
        "    npimg = grid.numpy()\n",
        "\n",
        "    # Convert and display\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{num_images} Augmented Versions (including original) of {label}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_augmented_images_from_id_list(dataset, id_list, title = \"\"):\n",
        "    \"\"\"\n",
        "    Show a grid of augmented images from the dataset.\n",
        "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
        "    \"\"\"\n",
        "\n",
        "    images = []\n",
        "    for i in id_list:\n",
        "        img_tensor, _ = dataset[i]\n",
        "        images.append(img_tensor)\n",
        "\n",
        "    # Make a grid of images\n",
        "    grid = torchvision.utils.make_grid(\n",
        "        images, nrow=int(len(id_list) ** 0.5), padding=2)\n",
        "    npimg = grid.numpy()\n",
        "\n",
        "    # Convert and display\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{len(id_list)} Augmented Versions: {title}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_tensor_images(tensor_images):\n",
        "    \"\"\"\n",
        "    Show a grid of augmented images from the dataset.\n",
        "    num_images must be a perfect square (e.g., 4, 9, 16, 25, 36, ...)\n",
        "    \"\"\"\n",
        "\n",
        "    # Make a grid of images\n",
        "    grid = torchvision.utils.make_grid(\n",
        "        tensor_images, nrow=int(len(tensor_images) ** 0.5), padding=2)\n",
        "    npimg = grid.numpy()\n",
        "\n",
        "    # Convert and display\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{len(tensor_images)} Augmented Versions')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NEOMgbEfD9Ne"
      },
      "outputs": [],
      "source": [
        "def select_confident_samples(logits, top_p):\n",
        "    \"\"\"\n",
        "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
        "    \"\"\"\n",
        "    assert 0 <= top_p < 1, \"The value must be between 0 and 1\"\n",
        "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
        "    idx = torch.argsort(batch_entropy, descending=False)[\n",
        "        :int(batch_entropy.size()[0] * top_p)]\n",
        "    return logits[idx], idx\n",
        "\n",
        "\n",
        "def compute_avg_entropy(outputs):\n",
        "    \"\"\"\n",
        "    Compute marginal entropy of samples and return the average.\n",
        "    \"\"\"\n",
        "    # Calculate probabilities from logits\n",
        "    probs = outputs.softmax(dim=1)\n",
        "    # To avoid log(0), clamp probabilities to a minimum value\n",
        "    probs = probs.clamp(min=1e-9)\n",
        "    entropy = -(probs * probs.log()).sum(dim=1)\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7teFCGLpCpOj"
      },
      "outputs": [],
      "source": [
        "def get_dataset_split(dataset_class,train_percentage=0.5, validation_percentage=0.25):\n",
        "    # Load data\n",
        "    dataset = dataset_class(\"datasets/imagenet-a\", preprocess)\n",
        "\n",
        "    # Create train validation and test samples\n",
        "    num_samples = len(dataset)\n",
        "    training_sample = int(num_samples * train_percentage + 1)\n",
        "    validation_sample = int(num_samples * validation_percentage)\n",
        "    test_sample = num_samples - training_sample - validation_sample\n",
        "\n",
        "    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [training_sample, validation_sample, test_sample])\n",
        "\n",
        "    return (training_dataset, validation_dataset, test_dataset)\n",
        "\n",
        "\n",
        "def get_data(training_dataset, validation_dataset, test_dataset, batch_size=64, transform=None, num_workers=8):\n",
        "    \"\"\"\n",
        "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
        "    \"\"\"\n",
        "\n",
        "    if not transform:\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "\n",
        "    # Create a DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        training_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,    worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def embed_dataset_classnames(dataset_class, model, preprocess, templates=[\"a photo of a {}.\"]):\n",
        "    \"\"\"\n",
        "    Embed the classnames in the prompt template.\n",
        "    Return the classnames and the normalized textual features.\n",
        "    \"\"\"\n",
        "    # Create the list of descriptions and tokenize them\n",
        "    dataset = dataset_class(\"datasets/imagenet-a\", preprocess)\n",
        "    classnames = dataset.class_code_to_name.values()\n",
        "\n",
        "    texts_z_views = []\n",
        "    for template in templates:\n",
        "        descriptions = [template.format(c) for c in classnames]\n",
        "        text_tokens = clip.tokenize(descriptions).to(DEVICE)\n",
        "\n",
        "        # Get the normalized textual features\n",
        "        with torch.no_grad():\n",
        "            texts_z = model.encode_text(text_tokens).float()\n",
        "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "            texts_z_views.append(texts_z)\n",
        "\n",
        "    # Evaluate the mean representation\n",
        "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
        "\n",
        "    # Renormalise\n",
        "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return classnames, texts_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j9MF03rlrii5"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": model.parameters()}\n",
        "    ], lr=lr, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def get_cost_function():\n",
        "    cost_function = torch.nn.CrossEntropyLoss()\n",
        "    return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B00EN1J0rwia"
      },
      "outputs": [],
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zC0YYrhkrcDr"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Training step (for CoOp).\n",
        "    \"\"\"\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # Set the network to training mode\n",
        "    net.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gradients reset\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Fetch prediction and loss value\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "        # Compute training accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
        "        pbar.update(1)\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # Set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader))\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test set\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            # Load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # Fetch prediction and loss value\n",
        "            samples += inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            # Compute accuracy\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
        "            pbar.update(1)\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgQ5QNPD0ly"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QsVjFiranPu4"
      },
      "outputs": [],
      "source": [
        "def main_coop(\n",
        "    net,\n",
        "    dataset_splits,\n",
        "    batch_size=16,\n",
        "    learning_rate=0.002,\n",
        "    weight_decay=0.0005,\n",
        "    momentum=0.9,\n",
        "    epochs=2,\n",
        "    run_name=\"exp1\",\n",
        "    skip_test = False\n",
        "):\n",
        "    \"\"\"\n",
        "    @param: dataset_class\n",
        "    @param: dataset_splits tuple that contains (training, validation, test) \"\"\"\n",
        "\n",
        "    # Create a logger for the experiment\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    # Get dataloaders\n",
        "    train_loader, val_loader, test_loader = get_data(dataset_splits[0],dataset_splits[1],dataset_splits[2], transform=preprocess, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    print(\"Turning off gradients in both the image and the text encoder\")\n",
        "    for name, param in net.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "    print(f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Instantiate the optimizer\n",
        "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "    # Define the cost function\n",
        "    cost_function = get_cost_function()\n",
        "\n",
        "    # Computes evaluation results before training\n",
        "    if not skip_test: \n",
        "        print(\"Before training:\")\n",
        "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "        print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "        print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # For each epoch, train the network and then compute evaluation results\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
        "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "\n",
        "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
        "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
        "\n",
        "    # Compute final evaluation results\n",
        "    if not skip_test: \n",
        "        print(\"After training:\")\n",
        "    \n",
        "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "        print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "        print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # Closes the logger\n",
        "    writer.close()\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4BnmU189i3Ju"
      },
      "outputs": [],
      "source": [
        "def new_clip():\n",
        "    model, preprocess = clip.load('ViT-B/16', DEVICE)\n",
        "    classnames, _ = embed_dataset_classnames(ImageNetA, model, preprocess=preprocess)\n",
        "    n_ctx = 4\n",
        "    ctx_init = \"\"\n",
        "    class_token_position = \"end\"\n",
        "    csc = False\n",
        "    coop_net = OurCLIP(\n",
        "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
        "    ).to(DEVICE)\n",
        "    return coop_net, preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19PDj5w0r058",
        "outputId": "5969808e-75c2-4fc3-dae4-5b9eefcb1d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a generic context\n",
            "Initial context: 'X X X X'\n",
            "Number of context words (tokens): 4\n",
            "dict_values(['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn'])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' main_coop(coop_net, splitted_datasets, batch_size=16, skip_test = True)\\ntorch.save(coop_net.state_dict(), \"./working_directory/model.pth\") '"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the network and move it to the chosen device (GPU)\n",
        "coop_net, preprocess = new_clip()\n",
        "splitted_datasets = get_dataset_split(ImageNetA)\n",
        "\"\"\" main_coop(coop_net, splitted_datasets, batch_size=16, skip_test = True)\n",
        "torch.save(coop_net.state_dict(), \"./working_directory/model.pth\") \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nGxiApN6i3Ju"
      },
      "outputs": [],
      "source": [
        "def load_model(model):\n",
        "    model.load_state_dict(torch.load(\n",
        "        \"./working_directory/model.pth\", weights_only=True))\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OurCLIP(\n",
              "  (prompt_learner): PromptLearner()\n",
              "  (image_encoder): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_encoder): TextEncoder(\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_model(coop_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "x9FOEMuzi3Ju"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Example AugMix-style transform\n",
        "def get_augmix_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "# Basic original transform (non-augmented)\n",
        "original_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Wrapper Dataset that takes a base dataset + index of the sample to augment\n",
        "\n",
        "\n",
        "class AugmentSingleSampleDataset(Dataset):\n",
        "    def __init__(self, base_dataset, sample_idx, num_augments=63):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.sample_idx = sample_idx\n",
        "        self.num_augments = num_augments\n",
        "        self.augmix_transform = get_augmix_transform()\n",
        "        self.original_transform = original_transform\n",
        "\n",
        "        # Extract the image once to avoid loading it 64 times\n",
        "        image, label = self.base_dataset[self.sample_idx]\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            self.image = transforms.ToPILImage()(image)\n",
        "        else:\n",
        "            self.image = image\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_augments + 1  # 63 augments + 1 original\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx == 0:\n",
        "            image = self.original_transform(self.image)\n",
        "        else:\n",
        "            image = self.augmix_transform(self.image)\n",
        "        return image, self.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozyf05Npi3Jv",
        "outputId": "88d5350c-8b9d-4b1c-eaa7-148c987cb950"
      },
      "outputs": [],
      "source": [
        "def quick_tpt_test(net):\n",
        "    # Load your original dataset\n",
        "    imagenet_a = ImageNetA(root_dir='datasets/imagenet-a')\n",
        "\n",
        "    # Pick one sample index (say the first one)\n",
        "    sample_idx = 0\n",
        "\n",
        "    # Wrap it in the augmentation dataset\n",
        "    aug_dataset = AugmentSingleSampleDataset(imagenet_a, sample_idx)\n",
        "    show_augmented_images(aug_dataset, num_images=16)\n",
        "\n",
        "    # Now get the DataLoader\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        aug_dataset, batch_size=64, shuffle=False, worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "    with torch.no_grad():\n",
        "        images, labels = next(iter(dataloader))\n",
        "        print(len(images))\n",
        "        images = images.to(DEVICE)\n",
        "        prova = 1\n",
        "        outputs = net(images)\n",
        "        top_outputs, id_aug = select_confident_samples(outputs, .1)\n",
        "    print(id_aug)\n",
        "    show_augmented_images_from_id_list(aug_dataset, id_aug, title = labels)\n",
        "    print(len(imagenet_a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "def LINE():\n",
        "    return sys._getframe(1).f_lineno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9O1r38eni3Jv"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from torch.cuda.amp import autocast\n",
        "def test_step_tpt(net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16):\n",
        "    \"\"\"\n",
        "    @param net takes a OurClip model type\n",
        "    \"\"\"\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # Set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    pbar = tqdm(range(len(dataset)), desc=\"TPT_testing\", position=0,\n",
        "                leave=True, total=len(dataset))\n",
        "    # Iterate over the indices of the test set\n",
        "    try:\n",
        "        for sample_idx in pbar: # Iterate through indices\n",
        "            net.reset_ctx()\n",
        "            optimizer.load_state_dict(optimizer_state_dict)\n",
        "            #print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
        "\n",
        "\n",
        "            # Create augmented dataset for the current sample\n",
        "            aug_data = AugmentSingleSampleDataset(dataset.dataset, sample_idx, num_augments=num_aug) # Pass the base dataset and index\n",
        "\n",
        "            # Create a DataLoader for the augmented samples of this single image\n",
        "            aug_dataloader = torch.utils.data.DataLoader(\n",
        "                aug_data, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker,\n",
        "                generator=g)\n",
        "            \n",
        "            # Process the augmented images for this sample\n",
        "            all_outputs = []\n",
        "            for images, labels in aug_dataloader:\n",
        "                try:\n",
        "                    with autocast():\n",
        "                        #print(f\"size batch {len(images)}\")\n",
        "                        images = images.to(DEVICE)\n",
        "                        #print(torch.cuda.mem_get_info(), LINE())\n",
        "                        outputs = net(images)  # Use the provided net\n",
        "                        #print(torch.cuda.mem_get_info(), LINE())\n",
        "                        #cpu_outputs = outputs.to(\"cpu\")\n",
        "                        all_outputs.append(outputs)\n",
        "                        #print(torch.cuda.mem_get_info(), LINE())\n",
        "\n",
        "                        \"\"\" del images\n",
        "                        del outputs\n",
        "                        torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "                        print(torch.cuda.mem_get_info(), LINE()) \"\"\"\n",
        "\n",
        "                except:\n",
        "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
        "                    raise\n",
        "                    \n",
        "                    \n",
        "\n",
        "            # Get the original label for this sample\n",
        "            original_image, target = dataset[sample_idx]\n",
        "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
        "            #print(torch.cuda.mem_get_info(), LINE())\n",
        "\n",
        "            \n",
        "            target = torch.tensor([target]).to(DEVICE) # Make target a tensor and move to device\n",
        "\n",
        "\n",
        "\n",
        "            # Concatenate outputs from all batches for this sample\n",
        "            all_outputs = torch.cat(all_outputs, dim=0)\n",
        "\n",
        "            # Select confident samples and compute average entropy\n",
        "            top_outputs, _ = select_confident_samples(all_outputs, 0.2)\n",
        "            loss = compute_avg_entropy(top_outputs)\n",
        "            # Loss computation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            with autocast():\n",
        "                outputs = net(original_image)\n",
        "\n",
        "            # Fetch prediction and loss value\n",
        "            samples += original_image.shape[0]\n",
        "            # Note: the .item() is needed to extract scalars from tensors\n",
        "            cumulative_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            # Compute accuracy\n",
        "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
        "\n",
        "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
        "            pbar.update(1)\n",
        "    except:\n",
        "        raise\n",
        "    finally:\n",
        "        del all_outputs\n",
        "        del aug_data\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HxUxL726i3Jv"
      },
      "outputs": [],
      "source": [
        "def tpt_test(net,datasets, run_name=\"tpt1\", num_aug = 63, batch_size = 64):\n",
        "\n",
        "    # Create a logger for the experiment\n",
        "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "    net.set_ctx_checkpoint()\n",
        "\n",
        "    for name, param in net.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "        #print(f\"{name}is in {param.requires_grad}\")\n",
        "    print(torch.cuda.mem_get_info(), LINE())\n",
        "\n",
        "\n",
        "    # Define the optimizer\n",
        "    #optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
        "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)#, weight_decay=wd, momentum=momentum)\n",
        "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
        "    print(torch.cuda.mem_get_info(), LINE())\n",
        "\n",
        "\n",
        "    print(\"Test tpt:\")\n",
        "    test_loss, test_accuracy = test_step_tpt(net, datasets[2], optimizer, optimizer_state_dict, log_writer, num_aug = num_aug, batch_size = batch_size)\n",
        "\n",
        "    \"\"\"     log_values(log_writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(log_writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(log_writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "    print(\n",
        "        f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "    print(\n",
        "        f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\") \"\"\"\n",
        "\n",
        "    # Closes the logger\n",
        "    log_writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "collapsed": true,
        "id": "0a3MQAssFF-6",
        "outputId": "5d0957f2-067b-4b82-d5dc-bdcece246f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7096500224, 8212709376) 11\n",
            "(7096500224, 8212709376) 18\n",
            "Test tpt:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TPT_testing:   0%|          | 0/1874 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TPT_testing: 100%|██████████| 1874/1874 [21:36<00:00,  1.45it/s, test_acc=55.6]\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.memory._record_memory_history()\n",
        "tpt_test(coop_net, splitted_datasets, batch_size = 64, num_aug = 63)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
