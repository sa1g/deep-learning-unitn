{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6fGDwsTqOuC"
      },
      "source": [
        "### Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39noccGxC8OO",
        "outputId": "dfb211c1-15eb-431e-c27b-13d9feb7605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ftfy regex tqdm scikit-learn scikit-image\n",
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smXq0XTCqWsh"
      },
      "source": [
        "### Download and extract the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCIUTyLQC-CL",
        "outputId": "1df1e822-60b6-4d97-a7ad-d2bcef3f2df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hpE04mRnDApP",
        "outputId": "83e4ee72-64ea-4037-e0fa-a6ae8482072a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n",
            "tar: imagenet-a.tar: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "!mkdir datasets\n",
        "!tar -xvf imagenet-a.tar -C datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eauxAnMuCpOf"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0p71Vb0sCpOf"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import skimage\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as v2\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/16', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4LMjLjjkCvIV"
      },
      "outputs": [],
      "source": [
        "class ImageNetA(Dataset):\n",
        "    def __init__(self, root_dir='datasets/imagenet-a', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load class code to name mapping from README.txt\n",
        "        self.class_code_to_name = self._load_class_mapping(os.path.join(root_dir, 'README.txt'))\n",
        "\n",
        "        # Map class codes to integer labels\n",
        "        self.class_codes = sorted([\n",
        "            d for d in os.listdir(root_dir)\n",
        "            if os.path.isdir(os.path.join(root_dir, d)) and d in self.class_code_to_name\n",
        "        ])\n",
        "        self.class_code_to_idx = {code: idx for idx, code in enumerate(self.class_codes)}\n",
        "\n",
        "        # Collect all image paths and labels\n",
        "        self.samples = []\n",
        "        for class_code in self.class_codes:\n",
        "            class_folder = os.path.join(root_dir, class_code)\n",
        "            for fname in os.listdir(class_folder):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    path = os.path.join(class_folder, fname)\n",
        "                    label = self.class_code_to_idx[class_code]\n",
        "                    self.samples.append((path, label))\n",
        "\n",
        "    def _load_class_mapping(self, readme_path):\n",
        "        mapping = {}\n",
        "        with open(readme_path, 'r') as f:\n",
        "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
        "            for line in lines:\n",
        "                parts = line.strip().split(' ', 1)\n",
        "                if len(parts) == 2:\n",
        "                    code, name = parts\n",
        "                    mapping[code] = name\n",
        "        return mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.samples[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgSUnteCCpOh"
      },
      "source": [
        "### Import the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0cRqrP_2Dd41"
      },
      "outputs": [],
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uvGkdr9XDe3A"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WsSf3DgbDgMx"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        print(classnames)\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ry7I6MhNDhoc"
      },
      "outputs": [],
      "source": [
        "class OurCLIP(nn.Module):\n",
        "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
        "        # clip_model = clip_model.cpu()\n",
        "        clip_model = clip_model.float()\n",
        "\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image)\n",
        "\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2nxZZkDCBi2"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEOMgbEfD9Ne"
      },
      "outputs": [],
      "source": [
        "def select_confident_samples(logits, top_p):\n",
        "    \"\"\"\n",
        "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
        "    \"\"\"\n",
        "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
        "    idx = torch.argsort(batch_entropy, descending=False)[:int(batch_entropy.size()[0] * top_p)]\n",
        "    return logits[idx], idx\n",
        "\n",
        "def compute_avg_entropy(outputs):\n",
        "    \"\"\"\n",
        "    Compute marginal entropy of samples and return the average.\n",
        "    \"\"\"\n",
        "    # To avoid log(0), clamp probabilities to a minimum value\n",
        "    probs = probs.clamp(min=1e-9)\n",
        "    entropy = -(probs * probs.log()).sum(dim=1)\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7teFCGLpCpOj"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset_name, model, batch_size=64, transform=None, test_batch_size=256):\n",
        "    \"\"\"\n",
        "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
        "    \"\"\"\n",
        "\n",
        "    if not transform:\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "    # Load data\n",
        "    dataset = ImageNetA(\"datasets/imagenet-a\", preprocess)\n",
        "\n",
        "    # Create train validation and test samples\n",
        "    num_samples = len(dataset)\n",
        "    training_sample = int(num_samples * 0.5 + 1)\n",
        "    validation_sample = int(num_samples * 0.25)\n",
        "    test_sample = num_samples - training_sample - validation_sample\n",
        "\n",
        "    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [training_sample, validation_sample, test_sample])\n",
        "\n",
        "    # Create a DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False, num_workers=8)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def embed_dataset_classnames(dataset_name, model, templates=[\"a photo of a {}.\"]):\n",
        "    \"\"\"\n",
        "    Embed the classnames in the prompt template.\n",
        "    Return the classnames and the normalized textual features.\n",
        "    \"\"\"\n",
        "    # Create the list of descriptions and tokenize them\n",
        "    dataset = ImageNetA(\"datasets/imagenet-a\", preprocess)\n",
        "    classnames = dataset.class_code_to_name.values()\n",
        "\n",
        "    texts_z_views = []\n",
        "    for template in templates:\n",
        "        descriptions = [template.format(c) for c in classnames]\n",
        "        text_tokens = clip.tokenize(descriptions).cuda()\n",
        "\n",
        "        # Get the normalized textual features\n",
        "        with torch.no_grad():\n",
        "            texts_z = model.encode_text(text_tokens).float()\n",
        "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "            texts_z_views.append(texts_z)\n",
        "\n",
        "    # Evaluate the mean representation\n",
        "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
        "\n",
        "    # Renormalise\n",
        "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return classnames, texts_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9MF03rlrii5"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, lr, wd, momentum):\n",
        "    optimizer = torch.optim.SGD([\n",
        "        {\"params\": model.parameters()}\n",
        "    ], lr=lr, weight_decay=wd, momentum=momentum)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def get_cost_function():\n",
        "    cost_function = torch.nn.CrossEntropyLoss()\n",
        "    return cost_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B00EN1J0rwia"
      },
      "outputs": [],
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC0YYrhkrcDr"
      },
      "outputs": [],
      "source": [
        "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Training step (for CoOp).\n",
        "    \"\"\"\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # Set the network to training mode\n",
        "    net.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Parameters update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gradients reset\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Fetch prediction and loss value\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "        # Compute training accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
        "        pbar.update(1)\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
        "\n",
        "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
        "    samples = 0.0\n",
        "    cumulative_loss = 0.0\n",
        "    cumulative_accuracy = 0.0\n",
        "\n",
        "    # Set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "    pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader))\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test set\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            # Load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # Fetch prediction and loss value\n",
        "            samples += inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            # Compute accuracy\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
        "            pbar.update(1)\n",
        "\n",
        "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXgQ5QNPD0ly"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QsVjFiranPu4"
      },
      "outputs": [],
      "source": [
        "def main_coop(\n",
        "    dataset_name=\"cifar10\",\n",
        "    batch_size=16,\n",
        "    num_classes=10,\n",
        "    device=\"cuda:0\",\n",
        "    learning_rate=0.002,\n",
        "    weight_decay=0.0005,\n",
        "    momentum=0.9,\n",
        "    epochs=2,\n",
        "    run_name=\"exp1\",\n",
        "    n_ctx=4,\n",
        "    ctx_init=\"\",\n",
        "    class_token_position=\"end\",\n",
        "    csc=False,\n",
        "):\n",
        "    # Create a logger for the experiment\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    # Get dataloaders\n",
        "    train_loader, val_loader, test_loader = get_data(dataset_name, model, transform=preprocess, batch_size=batch_size)\n",
        "    classnames, _ = embed_dataset_classnames(dataset_name, model)\n",
        "\n",
        "    # Instantiate the network and move it to the chosen device (GPU)\n",
        "    net = OurCLIP(\n",
        "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Turning off gradients in both the image and the text encoder\")\n",
        "    for name, param in net.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "    print(f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Instantiate the optimizer\n",
        "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "    # Define the cost function\n",
        "    cost_function = get_cost_function()\n",
        "\n",
        "    # Computes evaluation results before training\n",
        "    print(\"Before training:\")\n",
        "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # For each epoch, train the network and then compute evaluation results\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
        "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "\n",
        "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
        "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
        "\n",
        "    # Compute final evaluation results\n",
        "    print(\"After training:\")\n",
        "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # Closes the logger\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19PDj5w0r058",
        "outputId": "8b8a8ee8-a439-410e-dafd-ebd30246b80d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a generic context\n",
            "Initial context: 'X X X X'\n",
            "Number of context words (tokens): 4\n",
            "dict_values(['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn'])\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Total parameters: 124,325,889\n",
            "Total trainable parameters: 2,048\n",
            "Before training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 118/118 [00:51<00:00,  2.29it/s, test_acc=36.5]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.40it/s, test_acc=36.1]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.39it/s, test_acc=35.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTraining loss 0.08987, Training accuracy 36.47\n",
            "\tValidation loss 0.08986, Validation accuracy 36.11\n",
            "\tTest loss 0.09093, Test accuracy 35.70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 118/118 [01:30<00:00,  1.30it/s, train_acc=53.5, train_loss=1.67]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.39it/s, test_acc=54.3]\n",
            "Training: 100%|██████████| 118/118 [01:31<00:00,  1.29it/s, train_acc=57, train_loss=1.56]   \n",
            "Testing: 100%|██████████| 59/59 [00:25<00:00,  2.34it/s, test_acc=55.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 118/118 [00:50<00:00,  2.34it/s, test_acc=58.7]\n",
            "Testing: 100%|██████████| 59/59 [00:25<00:00,  2.33it/s, test_acc=55.3]\n",
            "Testing: 100%|██████████| 59/59 [00:25<00:00,  2.31it/s, test_acc=55.1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTraining loss 0.04836, Training accuracy 58.65\n",
            "\tValidation loss 0.05268, Validation accuracy 55.31\n",
            "\tTest loss 0.05230, Test accuracy 55.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main_coop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_simple_coop(\n",
        "    model_class,\n",
        "    dataset_name=\"cifar10\",\n",
        "    batch_size=16,\n",
        "    num_classes=10,\n",
        "    device=\"cuda:0\",\n",
        "    learning_rate=0.002,\n",
        "    weight_decay=0.0005,\n",
        "    momentum=0.9,\n",
        "    epochs=2,\n",
        "    run_name=\"exp1\",\n",
        "    n_ctx=4,\n",
        "    ctx_init=\"\",\n",
        "    class_token_position=\"end\",\n",
        "    csc=False,\n",
        "):\n",
        "    # Create a logger for the experiment\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_data(\n",
        "        dataset_name, model, transform=preprocess, batch_size=batch_size)\n",
        "    classnames, _ = embed_dataset_classnames(dataset_name, model)\n",
        "\n",
        "    # Instantiate the network and move it to the chosen device (GPU)\n",
        "    net = model_class(\n",
        "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Turning off gradients in both the image and the text encoder\")\n",
        "    for name, param in net.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "    print(\n",
        "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Instantiate the optimizer\n",
        "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "    # Define the cost function\n",
        "    cost_function = get_cost_function()\n",
        "\n",
        "    # Computes evaluation results before training\n",
        "    print(\"Before training:\")\n",
        "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
        "\n",
        "    print(\n",
        "        f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "    print(\n",
        "        f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # For each epoch, train the network and then compute evaluation results\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_accuracy = training_step(\n",
        "            net, train_loader, optimizer, cost_function)\n",
        "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "\n",
        "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
        "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
        "\n",
        "    # Compute final evaluation results\n",
        "    print(\"After training:\")\n",
        "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
        "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
        "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
        "\n",
        "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
        "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
        "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
        "    print(\n",
        "        f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
        "    print(\n",
        "        f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
        "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
        "\n",
        "    # Closes the logger\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class simple_model_sum(nn.Module):\n",
        "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False, icvi=False):\n",
        "        super().__init__()\n",
        "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
        "        # clip_model = clip_model.cpu()\n",
        "        clip_model = clip_model.float()\n",
        "\n",
        "        self.In_class_visual_influence = icvi\n",
        "\n",
        "        self.prompt_learner = PromptLearner(\n",
        "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "        # Define two dense layers that take both text and image embeddings as input\n",
        "        # Assuming 512 dimensions for both text and image embeddings\n",
        "        self.fc1 = nn.Linear(512 + 512, 1024)\n",
        "        # Output back to 512 to match the image encoder input\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Get image features from the image encoder\n",
        "        image_features = self.image_encoder(image)\n",
        "\n",
        "        # Get tokenized prompts and compute text features from the text encoder\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        # Normalize the features\n",
        "        image_features = image_features / \\\n",
        "            image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / \\\n",
        "            text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Concatenate image and text features\n",
        "        # Concatenate along the feature dimension\n",
        "        combined_features = torch.cat((image_features, text_features), dim=-1)\n",
        "\n",
        "        # Pass the concatenated features through the dense layers\n",
        "        x = torch.relu(self.fc1(combined_features))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        # The output of the dense layers will now be used as input to the image encoder\n",
        "        # You can decide to pass the transformed features to the image encoder or use them for other purposes\n",
        "        # For now, I will assume you're using them directly for scoring.\n",
        "        transformed_image_features = x\n",
        "\n",
        "        # Logit scale adjustment\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "\n",
        "        # Compute logits\n",
        "        logits = logit_scale * transformed_image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FiLMModulation(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim):\n",
        "        super().__init__()\n",
        "        self.film = nn.Sequential(\n",
        "            nn.Linear(text_dim, image_dim * 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, image_feat, text_feat):\n",
        "        gamma_beta = self.film(text_feat)  # [B, 2 * C]\n",
        "        gamma, beta = gamma_beta.chunk(2, dim=-1)  # [B, C] each\n",
        "        return gamma * image_feat + beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FilmCLIP(nn.Module):\n",
        "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
        "        clip_model = clip_model.float()\n",
        "\n",
        "        self.prompt_learner = PromptLearner(\n",
        "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "        self.film_modulation = FiLMModulation(\n",
        "            text_dim=512, image_dim=512)  # 512 is CLIP's embedding dim\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image)  # [B, 512]\n",
        "\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(\n",
        "            prompts, tokenized_prompts)  # [K, 512]\n",
        "\n",
        "        # Average text features to get a global modulation vector\n",
        "        \"\"\"  global_text_feat = text_features.mean(dim=0, keepdim=True)  # [1, 512]\n",
        "        global_text_feat = global_text_feat.expand(\n",
        "            image_features.size(0), -1)  # [B, 512] \"\"\"\n",
        "        \n",
        "        # Cosine similarity between each image and each class text feature\n",
        "        sim = image_features @ text_features.t()  # [B, K]\n",
        "\n",
        "        # Softmax over classes to get weights\n",
        "        weights = sim.softmax(dim=1)  # [B, K]\n",
        "\n",
        "        # Weighted average of text features per image\n",
        "        global_text_feat = weights @ text_features  # [B, 512]\n",
        "\n",
        "\n",
        "        # Modulate image features\n",
        "        image_features = self.film_modulation(image_features, global_text_feat)\n",
        "\n",
        "        # Normalize\n",
        "        image_features = image_features / \\\n",
        "            image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / \\\n",
        "            text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a generic context\n",
            "Initial context: 'X X X X'\n",
            "Number of context words (tokens): 4\n",
            "dict_values(['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn'])\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Total parameters: 124,851,201\n",
            "Total trainable parameters: 2,048\n",
            "Before training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 118/118 [00:50<00:00,  2.34it/s, test_acc=1.28]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.44it/s, test_acc=1.44]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.43it/s, test_acc=1.6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTraining loss 0.20939, Training accuracy 1.28\n",
            "\tValidation loss 0.21027, Validation accuracy 1.44\n",
            "\tTest loss 0.21085, Test accuracy 1.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 118/118 [01:28<00:00,  1.33it/s, train_acc=18.1, train_loss=3.19]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.43it/s, test_acc=28.4]\n",
            "Training: 100%|██████████| 118/118 [01:28<00:00,  1.33it/s, train_acc=34.3, train_loss=3.31]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.44it/s, test_acc=38.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 118/118 [00:47<00:00,  2.47it/s, test_acc=40.2]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.45it/s, test_acc=38.1]\n",
            "Testing: 100%|██████████| 59/59 [00:24<00:00,  2.41it/s, test_acc=37.1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTraining loss 0.08154, Training accuracy 40.20\n",
            "\tValidation loss 0.08479, Validation accuracy 38.13\n",
            "\tTest loss 0.08515, Test accuracy 37.09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main_simple_coop(model_class=FilmCLIP) #TODO aggiungere freeze a FILM Layers \n",
        "#TODO Verificare l'utilitá della cosine matrix per FILM"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
