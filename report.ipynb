{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b431a3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4204f07",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we focus on **Test-Time Adaptation (TTA)**, which has recently gained traction due to its ability to enhance model performance without requiring access to training data.\n",
    "\n",
    "In this project, we focus on **TTA for image classification**, particularly using **CLIP** [[2](#ref-clip2021)] with **TPT** [[3](#ref-tpt2022)]. Our approach involves adapting the model on **single-image test instances**, with the model being reset to its pre-trained state after each instance. This resembles **TTIA**, keeping the constraint of no retention of prior test-time knowledge (between batches, so between images).\n",
    "\n",
    "<!--- visualize image using html formatting, so that i can scale it properly -->\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"img/tpt.png\" alt=\"Test-Time Prompt Tuning (TPT) for CLIP\" title=\"Test-Time Prompt Tuning (TPT) for CLIP\" width=\"600\" class=\"center\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "## A. TTIA\n",
    "\n",
    "> **Definition**: \"_Test-Time Instance Adaption, TTIA_ Given a classifier $f_\\mathcal{S}$ learned on the source domain $\\mathcal{D_s}$, and an unlabeled target instance $x_t \\in \\mathcal{D_T}$ under distribution shift, _test-time instance adaption_ aims to leverage the labeled knowledge implied in $\\mathcal{f_S}$ to infer the label of $x_t$ adaptively\" [[1](#ref-liang2025)]. In other words, TTIA aims to adapt the classifier $f_\\mathcal{S}$ to the target instance $x_t$ by leveraging the knowledge of the source domain $\\mathcal{D_S}$. [[1](#ref-liang2025)]\n",
    "\n",
    "TTIA differs from TTBA in that single-instance adaption is performed, instead of batch-wise adaption, giving an example the difference is between classifying a single frame of a video and classifying a sequence of frames. In both methods no memory of the previous test-time knowledge is retained.\n",
    "\n",
    "## B. Project Overview\n",
    "\n",
    "We aim to reproduce TPT results on ImageNetA.\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "1. Introduction\n",
    "1. Reproducing TPT + CoOp with full CoOp pretraining (on ImageNetV2)\n",
    "   - Using OpenAI CLIP (both implementation and weights)\n",
    "   - So that we can compare it with TPT + CoOp without pretraining (`a photo of a` initialization).)\n",
    "1. Reproducing TPT\n",
    "   - Using OpenAI weights and OpenCLIP implementation\n",
    "     - Compare zero-shot CLIP OpenAI (weights and implementation) with OpenCLIP (weights and implementation)\n",
    "   - Using `Kornia` instead of `AugMix` / `torchvision.transforms` (**Our contribution**)\n",
    "     - Recreate the AugMix pipeline in Kornia\n",
    "     - Kornia is faster and can directly run on the GPU\n",
    "     - Benchmarking the difference\n",
    "   - Reproduce TPT + simplified CoOp (without pretraining) (**Our contribution**)\n",
    "1. Trying to get better at TTA (**Our contribution**)\n",
    "    - A. Augment Top 1\n",
    "    - B. TPT with Top 1\n",
    "    - C. Self-Supervised Retrieval (Inspired by DinoV2) [[4](#ref-dinov2)]\n",
    "    - C. Stupid idea with adaptive layer norm\n",
    "    - D. TNT (Recreate the paper)\n",
    "    - E. TNT with Top 1\n",
    "    - F. TPS\n",
    "1. Results and Conclusion\n",
    "1. Future Work\n",
    "1. Bibliography\n",
    "\n",
    "## C. Reproducibility\n",
    "The project is designed to be reproducible. Code is also available on [GitHub]() as standard `python` files.\n",
    "\n",
    "For reproducibility seeding is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2481ae3",
   "metadata": {},
   "source": [
    "### D. Data\n",
    "\n",
    "Get datasets data, create datasets and dataloader. Seeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.0\n",
      "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting notebook==7.1.3\n",
      "  Using cached notebook-7.1.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting torchvision==0.18.0\n",
      "  Using cached torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting matplotlib==3.8.4\n",
      "  Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting openai-clip==1.0.1\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kornia\n",
      "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torch==2.3.0) (12.1.105)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0)\n",
      "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook==7.1.3)\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.22.1 (from notebook==7.1.3)\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyterlab<4.2,>=4.1.1 (from notebook==7.1.3)\n",
      "  Using cached jupyterlab-4.1.8-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook==7.1.3)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from notebook==7.1.3) (6.4)\n",
      "Requirement already satisfied: numpy in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torchvision==0.18.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from torchvision==0.18.0) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from matplotlib==3.8.4) (2.9.0.post0)\n",
      "Requirement already satisfied: ftfy in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from openai-clip==1.0.1) (6.2.0)\n",
      "Requirement already satisfied: regex in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from openai-clip==1.0.1) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from openai-clip==1.0.1) (4.66.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.4.99)\n",
      "Collecting kornia_rs>=0.1.9 (from kornia)\n",
      "  Downloading kornia_rs-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.1.3) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.1.3) (5.7.2)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading prometheus_client-0.22.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.1.3) (25.1.2)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook==7.1.3) (5.14.2)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (6.29.3)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tomli>=1.2.2 (from jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: requests>=2.31 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook==7.1.3) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.8.4) (1.16.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ftfy->openai-clip==1.0.1) (0.2.13)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.1.3) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.1.3) (3.6)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: certifi in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (8.22.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (5.9.8)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook==7.1.3)\n",
      "  Downloading rpds_py-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook==7.1.3) (4.2.0)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3) (6.0.1)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3) (2.17.2)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook==7.1.3) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook==7.1.3) (2.2.1)\n",
      "Requirement already satisfied: ptyprocess in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook==7.1.3) (0.7.0)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: decorator in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (3.0.43)\n",
      "Requirement already satisfied: stack-data in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (4.9.0)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.8.3)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/emmavico/Documents/Università/Magistrale/Appunti/Machine Learning/Ricci/Codice/.venv/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook==7.1.3) (0.2.2)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook==7.1.3)\n",
      "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
      "Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "Using cached notebook-7.1.3-py3-none-any.whl (5.0 MB)\n",
      "Using cached torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Downloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jupyterlab-4.1.8-py3-none-any.whl (11.4 MB)\n",
      "Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kornia_rs-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading prometheus_client-0.22.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rpds_py-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.0/387.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368629 sha256=127f1c6d46dea391d3f3e40208bd11788e760d9567a1cb6e961a5d75fecc7c56\n",
      "  Stored in directory: /home/emmavico/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: webencodings, fastjsonschema, websocket-client, webcolors, uri-template, types-python-dateutil, triton, tomli, tinycss2, terminado, soupsieve, sniffio, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, python-json-logger, pycparser, prometheus-client, pandocfilters, overrides, nvidia-nccl-cu12, mistune, kornia_rs, jupyterlab-pygments, jsonpointer, json5, h11, fqdn, defusedxml, bleach, babel, attrs, async-lru, referencing, openai-clip, matplotlib, jupyter-server-terminals, httpcore, cffi, beautifulsoup4, arrow, anyio, torch, jsonschema-specifications, isoduration, httpx, argon2-cffi-bindings, torchvision, kornia, jsonschema, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.1\n",
      "    Uninstalling torchvision-0.17.1:\n",
      "      Successfully uninstalled torchvision-0.17.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyio-4.9.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 cffi-1.17.1 defusedxml-0.7.1 fastjsonschema-2.21.1 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 isoduration-20.11.0 json5-0.12.0 jsonpointer-3.0.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.1.8 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 kornia-0.8.1 kornia_rs-0.1.9 matplotlib-3.8.4 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.1.3 notebook-shim-0.2.4 nvidia-nccl-cu12-2.20.5 openai-clip-1.0.1 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.22.0 pycparser-2.22 python-json-logger-3.3.0 referencing-0.36.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.25.1 send2trash-1.8.3 sniffio-1.3.1 soupsieve-2.7 terminado-0.18.1 tinycss2-1.4.0 tomli-2.2.1 torch-2.3.0 torchvision-0.18.0 triton-2.3.0 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "\n",
    "# Get datasets (ImageNet-A and ImageNetV2)\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n",
    "\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1TR1hrs9tV6rh_W-hDqRv6jH3KE-Hxsw2/view?usp=sharing\n",
    "\n",
    "!tar -xvf imagenetv2-matched-frequency-format-val.tar.gz -C datasets\n",
    "# json metadata of the datasets\n",
    "!curl https://raw.githubusercontent.com/modestyachts/ImageNetV2/refs/heads/master/data/metadata/class_info.json -o datasets/imagenetv2-matched-frequency-format-val/class_info.json\n",
    "\n",
    "!tar -xvf imagenet-a.tar -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import json\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "import torch.nn.functional as F\n",
    "# import time\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import open_clip\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding and reproducibility\n",
    "\n",
    "torch.manual_seed(456)\n",
    "torch.cuda.manual_seed(456)\n",
    "torch.randn(456).to(\"cuda\")\n",
    "np.random.seed(42)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "# https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74884d0c",
   "metadata": {},
   "source": [
    "# 2. Reproducing Coop\n",
    "\n",
    "With pre-training on ImageNetV2\n",
    "\n",
    "TODO: explain why!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bd1b2",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd451fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "vis_net, basic_image_transformations = clip.load(\"ViT-B/16\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6170c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    def __init__(\n",
    "        self, root_dir=\"datasets/imagenet-a\", transform=basic_image_transformations\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load class code to name mapping from README.txt\n",
    "        self.class_code_to_name = self._load_class_mapping(\n",
    "            os.path.join(root_dir, \"README.txt\")\n",
    "        )\n",
    "\n",
    "        # Map class codes to integer labels\n",
    "        self.class_codes : list[str]=[]\n",
    "        for d in os.listdir(root_dir):\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and d in self.class_code_to_name:\n",
    "                self.class_codes.append(d)\n",
    "\n",
    "\n",
    "        \"\"\" self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_name\n",
    "            ]\n",
    "        ) \"\"\"\n",
    "        \n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image paths and labels\n",
    "        self.samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_folder = os.path.join(root_dir, class_code)\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    self.samples.append((path, label))\n",
    "\n",
    "    def _load_class_mapping(self, readme_path)->Dict[int,str]:\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as f:\n",
    "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07fe06",
   "metadata": {},
   "source": [
    "#### Imagenet-A 🔗 ImagenetV2\n",
    "For the COOP training the ImageNetV2 dataset is linked to the available classes in ImageNetA thanks to the official json metadata info for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84020a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"datasets/imagenetv2-matched-frequency-format-val\",\n",
    "        transform=basic_image_transformations,\n",
    "        use_imagenet_a_classes=True,\n",
    "        imagenet_a=None,\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_imagenet_a = use_imagenet_a_classes\n",
    "\n",
    "        if use_imagenet_a_classes:\n",
    "            assert (\n",
    "                type(imagenet_a) == ImageNetA\n",
    "            ), \"imagenet_a_classes set to TRUE without passing imagenet_a object\"\n",
    "            imagenet_a_class_code_to_idx = imagenet_a.class_code_to_idx\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, \"class_info.json\"),\n",
    "                imagenet_a_class_code_to_idx\n",
    "            )\n",
    "            self.class_code_to_name = copy.deepcopy(\n",
    "                imagenet_a.class_code_to_name)\n",
    "\n",
    "        else:\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(\n",
    "                    root_dir, \"class_info.json\"), None\n",
    "            )\n",
    "            self.class_code_to_name = {\n",
    "                idx: self.v2id_to_info[\"label\"] for idx in self.v2id_to_info.keys()\n",
    "            }\n",
    "\n",
    "        self.samples = []\n",
    "        for v2_class_code in self.v2id_to_info.keys():\n",
    "            class_folder = os.path.join(root_dir, str(v2_class_code))\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    if use_imagenet_a_classes:\n",
    "                        self.samples.append(\n",
    "                            (path,\n",
    "                             self.v2id_to_info[v2_class_code][\"label_id\"])\n",
    "                        )\n",
    "                    else:\n",
    "                        self.samples.append((path, v2_class_code))\n",
    "\n",
    "    def _load_class_mapping(\n",
    "        self,\n",
    "        infofile_path,\n",
    "        imagenet_a_class_code_to_idx: Optional[dict[str, int]],\n",
    "    )->Dict[str,Dict[str,str]]:\n",
    "        \n",
    "        mapping = {}\n",
    "        with open(infofile_path) as f:\n",
    "            data = json.load(f)\n",
    "            for idx, item in enumerate(data):\n",
    "                if imagenet_a_class_code_to_idx is not None:\n",
    "                    if item[\"wnid\"] in imagenet_a_class_code_to_idx.keys():\n",
    "                        mapping[item[\"cid\"]] = {\n",
    "                            \"label_id\": imagenet_a_class_code_to_idx[item[\"wnid\"]],\n",
    "                            \"ia_code\": item[\"wnid\"],\n",
    "                            \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\"),\n",
    "                        }\n",
    "                else:\n",
    "                    mapping[item[\"cid\"]] = {\n",
    "                        \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\")\n",
    "                    }\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa76333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(dataset: Dataset, train_percentage=0.5, validation_percentage=0.25) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    # Load data\n",
    "\n",
    "    # Create train validation and test samples\n",
    "    num_samples = len(dataset)\n",
    "    training_sample = int(num_samples * train_percentage + 1)\n",
    "    validation_sample = int(num_samples * validation_percentage)\n",
    "    test_sample = num_samples - training_sample - validation_sample\n",
    "\n",
    "    training_dataset, validation_dataset, test_dataset = random_split(\n",
    "        dataset, [training_sample, validation_sample, test_sample]\n",
    "    )\n",
    "\n",
    "    return (training_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    training_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    transform=None,\n",
    "    num_workers=8,\n",
    ")->Tuple[DataLoader,DataLoader,DataLoader]:\n",
    "    \"\"\"\n",
    "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
    "    \"\"\"\n",
    "\n",
    "    if not transform:\n",
    "        transform = torchvision.transforms.Compose(\n",
    "            [torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def embed_dataset_classnames(dataset: ImageNetA, model, templates=[\"a photo of a {}.\"]):\n",
    "    \"\"\"\n",
    "    Embed the classnames in the prompt template.\n",
    "    Return the classnames and the normalized textual features.\n",
    "    \"\"\"\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    classnames = dataset.class_code_to_name.values()\n",
    "\n",
    "    texts_z_views = []\n",
    "    for template in templates:\n",
    "        descriptions = [template.format(c) for c in classnames]\n",
    "        text_tokens = clip.tokenize(descriptions).to(DEVICE)\n",
    "\n",
    "        # Get the normalized textual features\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "            texts_z_views.append(texts_z)\n",
    "\n",
    "    # Evaluate the mean representation\n",
    "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
    "\n",
    "    # Renormalise\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return classnames, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = ImageNetA()\n",
    "dataset_v2 = ImageNetV2(imagenet_a=dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15c624",
   "metadata": {},
   "source": [
    "### Base Model (Coop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c15192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(model_class, dataset):\n",
    "    classnames, _ = embed_dataset_classnames(dataset, vis_net)\n",
    "    n_ctx = 4\n",
    "    ctx_init = \"\"\n",
    "    class_token_position = \"end\"\n",
    "    csc = False\n",
    "    coop_net = model_class(\n",
    "        classnames=classnames,\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        csc=csc,\n",
    "    ).to(DEVICE)\n",
    "    return coop_net\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"./working_directory/model.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.text_projection\n",
    "        )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82f6e4",
   "metadata": {},
   "source": [
    "#### Prompt learner\n",
    "The prompt learner code is mostly taken from the Lab3 laboratories of the AY 24/25.\n",
    "We have done few updates to fix the weights to the ones obtained after COOP, and have the possibility to keep them and reinstate them after each iteration of TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b24dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic mechanics are taken from the Lab Number 3 of AY 24/25\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(\n",
    "                clip_model.token_embedding.weight.device\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        print(classnames)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(\n",
    "            clip_model.token_embedding.weight.device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\n",
    "            \"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "        self.ctx_checkpoint = ctx_vectors.detach().clone()\n",
    "\n",
    "    #reset context function after each TPT step  \n",
    "    def reset_ctx(self):  # https://discuss.pytorch.org/t/reset-model-weights/19180\n",
    "        with torch.no_grad():\n",
    "            self.ctx.copy_(self.ctx_checkpoint)\n",
    "\n",
    "        self.ctx.requires_grad = True\n",
    "        \n",
    "    # set context checkpoint before the TPT procedure\n",
    "    def set_ctx_checkpoint(self):\n",
    "        with torch.no_grad():\n",
    "            self.ctx_checkpoint.copy_(self.ctx)\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,  # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i: i + 1, :, :]\n",
    "                class_i = suffix[i: i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i: i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i: i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i: i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i: i + 1, :, :]\n",
    "                class_i = suffix[i: i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i: i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i: i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i,  # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model\n",
    "\n",
    "        self.prompt_learner = PromptLearner(\n",
    "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc\n",
    "        )\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def reset_ctx(self):\n",
    "        self.prompt_learner.reset_ctx()\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        self.prompt_learner.set_ctx_checkpoint()\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / \\\n",
    "            image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / \\\n",
    "            text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    net: OurCLIP,\n",
    "    data_loader: torch.utils.data.DataLoader,  # type: ignore\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cost_function,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training step (for CoOp).\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Training_Coop\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        # max() returns (maximum_value, index_of_maximum_value)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        del inputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def test_step(net: OurCLIP,\n",
    "            data_loader: DataLoader,\n",
    "            cost_function, device=DEVICE):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Testing_Coop\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(inputs)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coop(\n",
    "    net: OurCLIP,\n",
    "    dataset_splits: tuple,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"coop_training\",\n",
    "    skip_test=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    @param: dataset_class\n",
    "    @param: dataset_splits tuple that contains (training, validation, test)\"\"\"\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        dataset_splits[0],\n",
    "        dataset_splits[1],\n",
    "        dataset_splits[2],\n",
    "        transform=basic_image_transformations,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\"\n",
    "    )\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [{\"params\": net.parameters()}],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "    )\n",
    "\n",
    "    # Define the cost function\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Computes evaluation results before training\n",
    "    if not skip_test:\n",
    "        print(\"Before training:\")\n",
    "        train_loss, train_accuracy = test_step(\n",
    "            net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function\n",
    "        )\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    if not skip_test:\n",
    "        print(\"After training:\")\n",
    "\n",
    "        train_loss, train_accuracy = test_step(\n",
    "            net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_v2)\n",
    "splitted_datasets = get_dataset_split(dataset_v2)\n",
    "main_coop(coop_net, splitted_datasets, batch_size=16, skip_test=True)\n",
    "torch.save(coop_net.state_dict(), \"./working_directory/model_coop.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95816e54",
   "metadata": {},
   "source": [
    "# 3. Reproducing TPT\n",
    "\n",
    "We are always using OpenAI weights.\n",
    "\n",
    "TODO: explain why we are doing this!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4942a35",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "As in the paper: random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ed05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmix_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            #transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Basic original transform (non-augmented)\n",
    "original_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Wrapper Dataset that takes a base dataset + index of the sample to augment\n",
    "class AugmentSingleSampleDataset(Dataset):\n",
    "    def __init__(self, base_dataset, sample_idx, num_augments=63):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sample_idx = sample_idx\n",
    "        self.num_augments = num_augments\n",
    "        self.augmix_transform = get_augmix_transform()\n",
    "        self.original_transform = original_transform\n",
    "\n",
    "        # Extract the image once to avoid loading it 64 times\n",
    "        image, label = self.base_dataset[self.sample_idx]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            self.image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            self.image = image\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_augments + 1  # 63 augments + 1 original\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            image = self.original_transform(self.image)\n",
    "        else:\n",
    "            image = self.augmix_transform(self.image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a76f89",
   "metadata": {},
   "source": [
    "### TPT Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52835f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_confident_samples(logits, top_p):\n",
    "    \"\"\"\n",
    "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
    "    \"\"\"\n",
    "    assert 0 <= top_p < 1, \"The value must be between 0 and 1\"\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[\n",
    "        : int(batch_entropy.size()[0] * top_p)\n",
    "    ]\n",
    "    return logits[idx], idx\n",
    "\n",
    "\n",
    "def compute_avg_entropy(outputs):\n",
    "    \"\"\"\n",
    "    Compute marginal entropy of samples and return the average.\n",
    "    \"\"\"\n",
    "    # Calculate probabilities from logits\n",
    "    probs = outputs.softmax(dim=1)\n",
    "    # To avoid log(0), clamp probabilities to a minimum value\n",
    "    probs = probs.clamp(min=1e-9)\n",
    "    entropy = -(probs * probs.log()).sum(dim=1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_tpt(\n",
    "    net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    @param net takes a OurClip model type\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        range(len(dataset)),\n",
    "        desc=\"TPT_testing\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        total=len(dataset),\n",
    "    )\n",
    "    # Iterate over the indices of the test set\n",
    "    try:\n",
    "        for sample_idx in pbar:  # Iterate through indices\n",
    "            net.reset_ctx()\n",
    "            optimizer.load_state_dict(optimizer_state_dict)\n",
    "            # print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
    "\n",
    "            # Create augmented dataset for the current sample\n",
    "            aug_data = AugmentSingleSampleDataset(\n",
    "                dataset, sample_idx, num_augments=num_aug\n",
    "            )  # Pass the base dataset and index\n",
    "\n",
    "            # Create a DataLoader for the augmented samples of this single image\n",
    "            aug_dataloader = torch.utils.data.DataLoader(  # type: ignore\n",
    "                aug_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                worker_init_fn=seed_worker,\n",
    "                generator=g,\n",
    "            )\n",
    "\n",
    "            # Process the augmented images for this sample\n",
    "            all_outputs = []\n",
    "            for images, labels in aug_dataloader:\n",
    "                try:\n",
    "                    with autocast():\n",
    "                        # print(f\"size batch {len(images)}\")\n",
    "                        images = images.to(DEVICE)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        outputs = net(images)  # Use the provided net\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        # cpu_outputs = outputs.to(\"cpu\")\n",
    "                        all_outputs.append(outputs)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "                        \"\"\" del images\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        print(torch.cuda.mem_get_info(), LINE()) \"\"\"\n",
    "\n",
    "                except:\n",
    "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                    raise\n",
    "\n",
    "            # Get the original label for this sample\n",
    "            original_image, target = dataset[sample_idx]\n",
    "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
    "            # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "            # Make target a tensor and move to device\n",
    "            target = torch.tensor([target]).to(DEVICE)\n",
    "\n",
    "            # Concatenate outputs from all batches for this sample\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "            # Select confident samples and compute average entropy\n",
    "            top_outputs, _ = select_confident_samples(all_outputs, 0.2)\n",
    "            loss = compute_avg_entropy(top_outputs)\n",
    "            # Loss computation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(original_image)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += original_image.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        del all_outputs  # type: ignore\n",
    "        del aug_data  # type: ignore\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_test(net, dataset: Dataset, run_name=\"tpt1\", num_aug=63, batch_size=64):\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    net.set_ctx_checkpoint()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "        # print(f\"{name}is in {param.requires_grad}\")\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
    "    # , weight_decay=wd, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)\n",
    "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    print(\"Test tpt:\")\n",
    "    test_loss, test_accuracy = test_step_tpt(\n",
    "        net,\n",
    "        dataset,\n",
    "        optimizer,\n",
    "        optimizer_state_dict,\n",
    "        log_writer,\n",
    "        num_aug=num_aug,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Closes the logger\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75daade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_a)\n",
    "coop_net = load_model(coop_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt_test(coop_net, dataset_a, batch_size=64, num_aug=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23602fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clear memory. (delete everything)\n",
    "del dataset_a\n",
    "del dataset_v2\n",
    "del coop_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7683b",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "- `TPTPromptLearner`: a simple TPT prompt learner, which has a similar amount of learnable parameters as the original TPT's prompt learner, but with a simpler and more readable implementation. The idea is to simplify how the prompt learner is structured: instead of having the `class_token_position` that can be in the `end`, `middle` or `front` as in the original TPT + CoOp model, we simply split the prompt is `pre` and `post` prompts, which are then concatenated with the current class token (embedded ofc). One thing to note is that the prompt initialization, as seen in CoOp [[4](#ref-coop2021)], is done with \"a photo of a {}\", without any pre-training, as performances are close.\n",
    "- `TPTModel`: CLIP model with TPT+CoOp prompt learner.\n",
    "- `TPT`: awesome wrapper that makes possible to manage the finetuning and reset of the model after each image, it's \"invisible\" to the user and painless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: from 5_tpt.py\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: open_clip.model.CLIP,\n",
    "        arch: CLIPModels = CLIPModels.ViTB32,  # type: ignore\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer(arch)\n",
    "\n",
    "        self.__init_ctx_from_prompt(\n",
    "            tokenizer=tokenizer,\n",
    "            token_embedding=clip_model.token_embedding,\n",
    "            base_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    def __init_ctx_from_prompt(\n",
    "        self, tokenizer, token_embedding, base_prompt: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[tokenizer.sot_token_id]]).to(self.device)\n",
    "        c_token_eot = torch.tensor([[tokenizer.eot_token_id]]).to(self.device)\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = tokenizer(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = tokenizer(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = tokenizer(self.class_names).to(self.device)\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = tokenizer(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = token_embedding(c_token_sot)\n",
    "            self.embedded_eot = token_embedding(c_token_eot)\n",
    "            self.embedded_pad = token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            # [num_valid_tokens, 512]\n",
    "            clean_embed = embedded_class_names[i][mask[i]]\n",
    "            clean_embeddings.append(\n",
    "                clean_embed.unsqueeze(0)\n",
    "            )  # [1, num_valid_tokens, 512]\n",
    "\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "\n",
    "        for i, embed in enumerate(clean_embeddings):\n",
    "            self.register_buffer(f\"class_embed_{i}\", embed)\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)  # type: ignore\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)  # type: ignore\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # embeddeD_max_len: 77\n",
    "            # embedded_prefix: torch.Size([4, 512])\n",
    "            # embedded_class_names: torch.Size([1, 1, 512])\n",
    "            # embedded_suffix: torch.Size([0, 512]\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - getattr(self, f\"class_embed_{i}\").shape[1]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            ) - 2  # # -2 for SOT and EOT\n",
    "\n",
    "            # embedded sot shape: torch.Size([1, 1, 512])\n",
    "            # embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            # embedded class names shape: torch.Size([1, 1, 1, 512])\n",
    "            # embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            # embedded eot shape: torch.Size([1, 1, 512])\n",
    "            # effective padding shape: torch.Size([1, 70, 512])\n",
    "            # Prompt shape: torch.Size([1, 77, 512])\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    # self.embedded_class_names[i],\n",
    "                    getattr(self, f\"class_embed_{i}\"),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    self.embedded_pad.repeat(1, padding_size, 1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "        with torch.no_grad():\n",
    "            self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "            self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        arch: CLIPModels,\n",
    "        pretrained: str,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: open_clip.model.CLIP\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(  # type:ignore\n",
    "            model_name=arch,  # type:ignore\n",
    "            pretrained=pretrained,\n",
    "            device=device,\n",
    "            force_quick_gelu=True,\n",
    "        )\n",
    "\n",
    "        self.model = clip_model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)  # type:ignore\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.visual: open_clip.transformer.VisionTransformer = (  # type:ignore\n",
    "            clip_model.visual\n",
    "        )  # type:ignore\n",
    "        self.visual.eval()\n",
    "\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.attn_mask = clip_model.attn_mask\n",
    "        self.text_pool_type = clip_model.text_pool_type\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            arch=arch, class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "    def _pool(self, x: torch.Tensor):\n",
    "        if self.visual.attn_pool is not None:\n",
    "            if self.visual.attn_pool_contrastive is not None:\n",
    "                # This is untested, WIP pooling that should match paper\n",
    "                x = self.visual.ln_post(\n",
    "                    x\n",
    "                )  # TBD LN first or separate one after each pool?\n",
    "                tokens = self.visual.attn_pool(x)\n",
    "                if self.visual.attn_pool_type == \"parallel\":\n",
    "                    pooled = self.visual.attn_pool_contrastive(x)\n",
    "                else:\n",
    "                    assert self.visual.attn_pool_type == \"cascade\"\n",
    "                    pooled = self.visual.attn_pool_contrastive(tokens)\n",
    "            else:\n",
    "                # this is the original OpenCLIP CoCa setup, does not match paper\n",
    "                x = self.visual.attn_pool(x)\n",
    "                x = self.visual.ln_post(x)\n",
    "                pooled, tokens = self.visual._global_pool(x)\n",
    "        elif self.visual.final_ln_after_pool:\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "            pooled = self.visual.ln_post(pooled)\n",
    "        else:\n",
    "            x = self.visual.ln_post(x)\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "\n",
    "        return pooled, tokens, x\n",
    "\n",
    "    def _forward_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.visual._embeds(x)\n",
    "        x = self.visual.transformer(x)\n",
    "\n",
    "        pooled, tokens, x = self._pool(x)\n",
    "\n",
    "        if self.visual.proj is not None:\n",
    "            pooled = pooled @ self.visual.proj\n",
    "        if self.visual.output_tokens:\n",
    "            return pooled, tokens, x  # type:ignore\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def __encode_image(\n",
    "        self, image, normalize: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pooled_pre_norm = self._forward_image(image)\n",
    "        return (\n",
    "            F.normalize(pooled_pre_norm, dim=-1) if normalize else pooled_pre_norm\n",
    "        )  # type:ignore\n",
    "\n",
    "    def __encode_text(self, text=None, normalize: bool = False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.prompt_learner().to(cast_dtype)\n",
    "\n",
    "        text = self.prompt_learner.tokenized_initial_full_prompt\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = text_global_pool(x, text, self.text_pool_type)  # type:ignore\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                x = self.text_projection(x)\n",
    "            else:\n",
    "                x = x @ self.text_projection\n",
    "\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.__encode_image(image, normalize=True)\n",
    "        text_features = self.__encode_text(normalize=True)\n",
    "\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, image_features\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the prompt learner to its initial state.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.reset()\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "        model = TPTModel(\n",
    "            class_names=class_names,\n",
    "            arch=arch,\n",
    "            pretrained=pretrained,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # # # TEST - learnable layer norm\n",
    "        # self.model.visual.ln_post.requires_grad_(True)\n",
    "        # self.model.ln_final.requires_grad_(True)\n",
    "\n",
    "        # Get all trainable parameters (filter by requires_grad)\n",
    "        trainable_params = [\n",
    "            p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # Initialize optimizer with trainable parameters\n",
    "        self.optim = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim_init = deepcopy(self.optim.state_dict())\n",
    "\n",
    "        # Initialize backup lists\n",
    "        self.ln_backup = {\n",
    "            \"weights\": [],  # For gamma (scale)\n",
    "            \"biases\": [],  # For beta (shift)\n",
    "        }\n",
    "\n",
    "        # Backup all LN params in text encoder\n",
    "        for block in self.model.transformer.resblocks:  # type:ignore\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_1.weight.data.detach().clone()\n",
    "            )  # gamma for ln_1\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_1.bias.data.detach().clone()\n",
    "            )  # beta for ln_1\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_2.weight.data.detach().clone()\n",
    "            )  # gamma for ln_2\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_2.bias.data.detach().clone()\n",
    "            )  # beta for ln_2\n",
    "\n",
    "        # Backup final LN\n",
    "        self.ln_backup[\"weights\"].append(\n",
    "            self.model.ln_final.weight.data.detach().clone()\n",
    "        )\n",
    "        self.ln_backup[\"biases\"].append(\n",
    "            self.model.ln_final.bias.data.detach().clone())\n",
    "\n",
    "    def set_tta_steps(self, tta_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the number of TTA steps.\n",
    "\n",
    "        Args:\n",
    "            tta_steps (int): Number of TTA steps.\n",
    "        \"\"\"\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(\n",
    "                        logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            # input = input[-1].unsqueeze(0)\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return pred_class\n",
    "\n",
    "    def __select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def __avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def __reset(self) -> None:\n",
    "        \"\"\"Full reset of prompt learner and optimizer state\"\"\"\n",
    "        # 1. Reset prompt embeddings\n",
    "        for p in self.model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        self.model.reset()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "        #     self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            # Reset LN params in text encoder\n",
    "            for block in self.model.transformer.resblocks:  # type:ignore\n",
    "                block.ln_1.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_1.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "                block.ln_2.weight.data.copy_(\n",
    "                    self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_2.bias.data.copy_(\n",
    "                    self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "\n",
    "            # # Reset final LN\n",
    "            self.model.ln_final.weight.data.copy_(\n",
    "                self.ln_backup[\"weights\"][-1].clone())\n",
    "            self.model.ln_final.bias.data.copy_(\n",
    "                self.ln_backup[\"biases\"][-1].clone())\n",
    "\n",
    "        # # 2. Reset optimizer state\n",
    "        self.optim.load_state_dict(deepcopy(self.optim_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e0463",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f25939",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_random_crop,\n",
    "    n_views=63,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "dataloader, dataset = ImagenetA(augmenter, num_workers=5)\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE,\n",
    "      reduce=None, comment=\"tpt\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffa1bf",
   "metadata": {},
   "source": [
    "# 4. Trying to get a better at TTA (our contribution)\n",
    "\n",
    "- augmix (as a note criticizing TPT's paper.)\n",
    "- augment top 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d760c2",
   "metadata": {},
   "source": [
    "TPT with CoOp is quite slow due to the finetuning of the prompt. The idea is to try to get better or similar performances getting inspiration form TPT and other TTA methods, but, possibly, without any finetuning, or if needed, with a possibly faster finetuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79099c",
   "metadata": {},
   "source": [
    "$\\delta = \\Alpha + 3* \\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981b2a7",
   "metadata": {},
   "source": [
    "## A. Augment Top 1 🚀\n",
    "\n",
    "The idea is to remove the prompt learner (CoOp style) from the TPT model and use the most confident samples logits (top 1%) and the original image ones, average them and use the average logits as the final prediction.\n",
    "\n",
    "We want to keep the most confident samples logits, as they are the ones that are more likely to be correct, and average them with the original image logits, to avoid getting a too biased prediction and \"losing context\".\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "pred_class = int(marginal_prob.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "We expect this to be way faster than TPT (as no finetuning is done) and to have slightly better performances, as we are using the most confident samples logits, which are more likely to be correct, and keeping the original image logits to avoid getting a too biased prediction.\n",
    "\n",
    "- Why the \"biased prediction\"? Because the augmentations are random crop, so the model might be biased towards the augmented images, which might not be representative of the original image. By averaging the logits, we can mitigate this bias and get a more reliable prediction.\n",
    "- Is this the best method to do this? No, but it's a good starting point, it's simple and it works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395fd1e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad703ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        # self.logit_scale = model.log\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            initial_image_features = image_features[-1:]\n",
    "            filtered_image_features = image_features[:-1:]\n",
    "\n",
    "            # filter logits\n",
    "            initial_logits = (\n",
    "                self.logit_scale * initial_image_features @ self.text_features.t()\n",
    "            )\n",
    "            filtered_logits = (\n",
    "                self.logit_scale * filtered_image_features @ self.text_features.t()\n",
    "            )\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                # filtered_logits, top=1 / filtered_logits.shape[0]\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            # selected_logits = selected_logits.mean(0, keepdim=True)\n",
    "\n",
    "            # final_logits = selected_logits\n",
    "            final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = int(marginal_prob.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc323ba",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"top1\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c22372",
   "metadata": {},
   "source": [
    "## B. TPT with Top 1\n",
    "\n",
    "The idea is pretty simple: use the top 1 + original image logits as specified above, but on TPT w/ CoOp.\n",
    "\n",
    "The implementation is straightforward: override the `forward` method of the `TPT` class (the one which manages the finetuning of the `TPTModel`), so that it uses the top 1 + original image logits instead of the prompt learner. Note that the finetuning of the model is kept as is, only the final prediction is changed.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.__select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342c6df",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPTTop1(TPT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            pretrained=pretrained,\n",
    "            arch=arch,\n",
    "            class_names=class_names,\n",
    "            tta_steps=tta_steps,\n",
    "            lr=lr,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.__select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cc131",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tpt-top1\", visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae26cb0",
   "metadata": {},
   "source": [
    "## C. Self-Supervised Retrieval: RENAME THIS\n",
    "\n",
    "Here we get inspiration from DinoV2's [[5](#ref-dinov2)] self-supervised retrieval method. Using 63+1 augmentations, as in TPT, 6 clusters (kmeans) are populated, then cluster confidences are computed (mean cluster confidence, metric: cosine similarity). The most confident cluster is selected and it's logits are averages togheter with the original image logits.\n",
    "\n",
    "We don't expect much from this methods as CLIP is hasn't been trained, compared to DinoV2, for extracting saliency maps, so the clusters are not expected to be very meaningful. Still, this is interesting as it can be used to visualize the clusters to try to interpret, a little bit, what the model is doing. Of course this is unreliable.\n",
    "\n",
    "TODO: add plotting examples: cherry pick one that works well and one that doesn't work well at all, to show the difference in the clusters and how they are not much meaningful.\n",
    "\n",
    "TODO: it could be interesting to try to use the clusters to get a better prediction when the model is failing to classify an image, e.g. when the confidence is low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423ee3e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        # Precompute text features\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # x: (B, 3, 224, 224)\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            # Move to CPU and convert to numpy for sklearn\n",
    "            features_np = image_features.cpu().numpy()\n",
    "\n",
    "            # Standardize features\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(features_np)\n",
    "\n",
    "            # Cluster features\n",
    "            from sklearn.cluster import KMeans\n",
    "\n",
    "            kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "            ###################################################\n",
    "\n",
    "            # # get the cluster with higher confidence\n",
    "            # cluster_confidences = []\n",
    "            # for cluster_idx in range(6):\n",
    "            #     cluster_features = image_features[cluster_labels == cluster_idx]\n",
    "            #     logits = self.logit_scale * cluster_features @ self.text_features.t()\n",
    "            #     cluster_confidences.append(logits.mean().item())\n",
    "\n",
    "            # # Get the cluster with the highest confidence\n",
    "            # best_cluster_idx = cluster_confidences.index(max(cluster_confidences))\n",
    "\n",
    "            # image_features_r = image_features[cluster_labels == best_cluster_idx]\n",
    "\n",
    "            # image_features_r = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            # logits = self.logit_scale * image_features_r @ self.text_features.t()\n",
    "\n",
    "            # marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "\n",
    "            # pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            ###################################################\n",
    "\n",
    "            # Cluster closer to the original image\n",
    "            cluster_confidences = []\n",
    "            for cluster_idx in range(6):\n",
    "                cosine_sim = (\n",
    "                    image_features[cluster_labels == cluster_idx]\n",
    "                    @ image_features[-1:].t()\n",
    "                )\n",
    "                cluster_confidences.append(cosine_sim.mean().item())\n",
    "\n",
    "            # Get the cluster with the highest confidence\n",
    "            best_cluster_idx = cluster_confidences.index(max(cluster_confidences))\n",
    "\n",
    "            image_features_r = image_features[cluster_labels == best_cluster_idx]\n",
    "\n",
    "            image_features_r = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            logits = self.logit_scale * image_features_r @ self.text_features.t()\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            # # Accuracy: 55.72%\n",
    "            # # Latency: 234.35 ms\n",
    "\n",
    "            ####################################################\n",
    "            # # # ll = []\n",
    "\n",
    "            # # # for cluster_idx in range(6):\n",
    "            # # #     cluster_indices = (cluster_labels == cluster_idx)\n",
    "            # # #     if len(cluster_indices) == 0:\n",
    "            # # #         continue\n",
    "\n",
    "            # # #     cluster_features = image_features[cluster_indices]\n",
    "            # # #     logits = self.logit_scale * cluster_features @ self.text_features.t()\n",
    "            # # #     logits = logits.mean(dim=0)\n",
    "\n",
    "            # # #     ll.append(logits)\n",
    "            # # #     print(logits.shape)\n",
    "            # # # # ll\n",
    "            # # # # print(ll.shape)\n",
    "            # # # ll = torch.stack(ll, dim=0)\n",
    "            # # # ll = ll.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # # # marginal_prob = F.softmax(ll, dim=1).mean(0)\n",
    "            # # # pred_class = marginal_prob.argmax().item()\n",
    "            ####################################################\n",
    "\n",
    "            # exit()\n",
    "\n",
    "            # image_features = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            # logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            # marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            # pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            # # Visualize each cluster's images\n",
    "            # import matplotlib.pyplot as plt\n",
    "            # from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "            # for cluster_idx in range(6):\n",
    "            #     # Get indices of images in this cluster\n",
    "            #     cluster_indices = (cluster_labels == cluster_idx).nonzero()[0]\n",
    "\n",
    "            #     if len(cluster_indices) == 0:\n",
    "            #         continue\n",
    "\n",
    "            #     print(f\"Cluster {cluster_idx} has {len(cluster_indices)} images\")\n",
    "\n",
    "            #     # Setup plot\n",
    "            #     cols = min(8, len(cluster_indices))\n",
    "            #     rows = (len(cluster_indices) + cols - 1) // cols\n",
    "            #     plt.figure(figsize=(cols * 2, rows * 2))\n",
    "            #     plt.suptitle(f\"Cluster {cluster_idx} - {len(cluster_indices)} images\")\n",
    "\n",
    "            #     for plot_idx, img_idx in enumerate(cluster_indices, start=1):\n",
    "            #         if plot_idx > cols * rows:\n",
    "            #             break\n",
    "\n",
    "            #         img = x[img_idx].permute(1, 2, 0).cpu().numpy()\n",
    "            #         img = to_pil_image(img)\n",
    "            #         plt.subplot(rows, cols, plot_idx)\n",
    "            #         plt.imshow(img)\n",
    "            #         plt.axis('off')\n",
    "\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.show()\n",
    "\n",
    "            ####################################################################\n",
    "\n",
    "            # # # Visualize cluster_labels\n",
    "            # import matplotlib.pyplot as plt\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "            # plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "            # plt.title('KMeans Clustering of Image Features')\n",
    "            # plt.xlabel('Feature 1')\n",
    "            # plt.ylabel('Feature 2')\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            # from sklearn.manifold import TSNE\n",
    "\n",
    "            # # Apply t-SNE\n",
    "            # tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "            # X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "            # import matplotlib.pyplot as plt\n",
    "\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "\n",
    "            # # If you did clustering, color by cluster\n",
    "            # scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "\n",
    "            # # If you have true labels, you could color by those instead\n",
    "            # # scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
    "\n",
    "            # plt.colorbar(scatter)\n",
    "            # plt.title('t-SNE Visualization with Clusters')\n",
    "            # plt.xlabel('t-SNE dimension 1')\n",
    "            # plt.ylabel('t-SNE dimension 2')\n",
    "            # plt.show()\n",
    "\n",
    "            # cosine_sim = torch.mm(image_features, image_features.t())\n",
    "            # cosine_sim = image_features @ image_features.t()\n",
    "\n",
    "            # # kmeans clusters\n",
    "            # kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "            # kmeans.fit(cosine_sim.cpu().numpy())\n",
    "            # print(kmeans.labels_)\n",
    "\n",
    "            # show clusters\n",
    "\n",
    "            # #  2. Get similarities of the last image ([-1]) with all others\n",
    "            # last_img_similarities = cosine_sim[-1, :]  # (B,)\n",
    "\n",
    "            # # 3. Sort indices (descending order, excluding the last image itself)\n",
    "            # sorted_indices = torch.argsort(last_img_similarities, descending=True).cpu().numpy()\n",
    "            # # sorted_indices = sorted_indices[sorted_indices != len(x)-1]  # Remove self-comparison\n",
    "\n",
    "            # # 4. Visualize the last image + top-k most similar images\n",
    "            # k = 63  # Number of similar images to display\n",
    "            # cols = 8\n",
    "            # rows = (image_features.shape[0] + cols - 1) // cols\n",
    "            # plt.figure(figsize=(cols * 2, rows * 2))\n",
    "            # for i, idx in enumerate(sorted_indices[:k], start=2):\n",
    "            #     img = x[idx].permute(1, 2, 0).cpu().numpy()\n",
    "            #     img = TF.to_pil_image(img)\n",
    "\n",
    "            #     plt.subplot(rows, cols, i)\n",
    "            #     plt.imshow(img)\n",
    "            #     plt.title(f\"Sim: {last_img_similarities[idx]:.3f}\")\n",
    "            #     plt.axis('off')\n",
    "\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "            # Perform KMeans clustering\n",
    "            # kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "            # kmeans.fit(cosine_sim.cpu().numpy())\n",
    "            # labels = kmeans.labels_\n",
    "            # print(labels)\n",
    "\n",
    "            # exit()\n",
    "\n",
    "        return int(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3b3b4",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=200, comment=\"\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe0528",
   "metadata": {},
   "source": [
    "## C. (stupid) Adaptive Layer Norm 😥\n",
    "\n",
    "dire che ho provato anche a fare layernorm learnable e effettivamente funziona meglio, ma che rottura di coglioni, lo volevo senza backprop.\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efd950",
   "metadata": {},
   "source": [
    "## D. TNT\n",
    "\n",
    "TODO: add amount of learnable parameters wrt TPT\n",
    "\n",
    "Implementation of TNT [[6](#ref-tnt2023)]. It's pretty straightforward, it's CLIP with learnable random noise on the augmented images (noise shape: `(3, height, width)`).\n",
    "\n",
    "We expect this to be slightly faster than TPT, and have slightly better performances, as in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce6417",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c698216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.tnt_steps = tnt_steps\n",
    "        self.top_k = top_k\n",
    "        self.eps = epsilon\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.temperature = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.noise = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise = None\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)[-1:]\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            probs = F.softmax(logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d77520",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNT(\n",
    "    clip_model,  # type:ignore\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tnt\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f184e",
   "metadata": {},
   "source": [
    "## E. TNT + Top 1\n",
    "\n",
    "It's the same as TNT, but with the top 1 + original image logits as final prediction.\n",
    "\n",
    "We expect this to be slightly more accurate than TNT.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad412d5e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNTTop1(TNT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            class_labels=class_labels,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            tnt_steps=tnt_steps,\n",
    "            top_k=top_k,\n",
    "            epsilon=epsilon,\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3135d55",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd673ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNT(\n",
    "    clip_model,\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tnt-top1\", visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f2ffa",
   "metadata": {},
   "source": [
    "## F. TPS\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52100809",
   "metadata": {},
   "source": [
    "## G. FILM\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726c201",
   "metadata": {},
   "source": [
    "# 5. Thoughts and Conclusion\n",
    "\n",
    "Pro e contro di ogni uno\n",
    "\n",
    "TODO: plot accuracy/latency.\n",
    "\n",
    "quindi che usare backprop non conviene (citare frustaingly easy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2731d6f",
   "metadata": {},
   "source": [
    "# 6. Future Work\n",
    "\n",
    "- ai based augmentation (trying to optimize e.g. random crops)\n",
    "- do something stupid like merge TPT + TNT + frustatingly easy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29860d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19369d9",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
