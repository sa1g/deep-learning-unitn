{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b431a3b",
   "metadata": {},
   "source": [
    "_For clarity different models have been developed to handle different, and, or, similar algorithms, with the idea of having more readable code._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4204f07",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we focus on **Test-Time Adaptation (TTA)**, which has recently gained traction due to its ability to enhance model performance without requiring access to training data.\n",
    "\n",
    "In this project, we focus on **TTA for image classification**, particularly using **CLIP** [[2](#ref-clip2021)] with **TPT** [[3](#ref-tpt2022)]. Our approach involves adapting the model on **single-image test instances**, with the model being reset to its pre-trained state after each instance. This resembles **TTIA**, keeping the constraint of no retention of prior test-time knowledge (between batches, so between images).\n",
    "\n",
    "![](img/tpt.png \"Test-Time Prompt Tuning (TPT) for CLIP\")\n",
    "\n",
    "## A. TTIA\n",
    "\n",
    "> **Definition**: \"_Test-Time Instance Adaption, TTIA_ Given a classifier $f_\\mathcal{S}$ learned on the source domain $\\mathcal{D_s}$, and an unlabeled target instance $x_t \\in \\mathcal{D_T}$ under distribution shift, _test-time instance adaption_ aims to leverage the labeled knowledge implied in $\\mathcal{f_S}$ to infer the label of $x_t$ adaptively\" [[1](#ref-liang2025)]. In other words, TTIA aims to adapt the classifier $f_\\mathcal{S}$ to the target instance $x_t$ by leveraging the knowledge of the source domain $\\mathcal{D_S}$. [[1](#ref-liang2025)]\n",
    "\n",
    "TTIA differs from TTBA in that single-instance adaption is performed, instead of batch-wise adaption, giving an example the difference is between classifying a single frame of a video and classifying a sequence of frames. In both methods no memory of the previous test-time knowledge is retained.\n",
    "\n",
    "## B. Reproducibility\n",
    "\n",
    "- github link\n",
    "- seeding is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2481ae3",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Get datasets data, create datasets and dataloaderd. Seeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "# TODO: add packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "\n",
    "# Get datasets (ImageNet-A and \n",
    "# TODO: ImageNetV2)\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n",
    "\n",
    "!curl https://raw.githubusercontent.com/modestyachts/ImageNetV2/refs/heads/master/data/metadata/class_info.json -o datasets/imagenetv2-matched-frequency-format-val/class_info.json\n",
    "\n",
    "!tar -xvf imagenet-a.tar -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import json\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import clip.model\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import open_clip\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding and reproducibility\n",
    "\n",
    "torch.manual_seed(456)\n",
    "torch.cuda.manual_seed(456)\n",
    "torch.randn(456).to(\"cuda\")\n",
    "np.random.seed(42)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "# https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "vis_net, basic_image_transformations = clip.load(\"ViT-B/16\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    def __init__(\n",
    "        self, root_dir=\"datasets/imagenet-a\", transform=basic_image_transformations\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load class code to name mapping from README.txt\n",
    "        self.class_code_to_name = self._load_class_mapping(\n",
    "            os.path.join(root_dir, \"README.txt\")\n",
    "        )\n",
    "\n",
    "        # Map class codes to integer labels\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_name\n",
    "            ]\n",
    "        )\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image paths and labels\n",
    "        self.samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_folder = os.path.join(root_dir, class_code)\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    self.samples.append((path, label))\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as f:\n",
    "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"datasets/imagenetv2-matched-frequency-format-val\",\n",
    "        transform=basic_image_transformations,\n",
    "        use_imagenet_a_classes=True,\n",
    "        imagenet_a=None,\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_imagenet_a = use_imagenet_a_classes\n",
    "\n",
    "        if use_imagenet_a_classes:\n",
    "            assert (\n",
    "                type(imagenet_a) == ImageNetA\n",
    "            ), \"imagenet_a_classes set to TRUE without passing imagenet_a object\"\n",
    "            imagenet_a_class_code_to_idx = imagenet_a.class_code_to_idx\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, \"class_info.json\"),\n",
    "                use_imagenet_a_classes,\n",
    "                imagenet_a_class_code_to_idx,\n",
    "            )\n",
    "            self.class_code_to_name = copy.deepcopy(imagenet_a.class_code_to_name)\n",
    "\n",
    "        else:\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, \"class_info.json\"), use_imagenet_a_classes, None\n",
    "            )\n",
    "            self.class_code_to_name = {\n",
    "                idx: self.v2id_to_info[\"label\"] for idx in self.v2id_to_info.keys()\n",
    "            }\n",
    "\n",
    "        self.samples = []\n",
    "        for v2_class_code in self.v2id_to_info.keys():\n",
    "            class_folder = os.path.join(root_dir, str(v2_class_code))\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    if use_imagenet_a_classes:\n",
    "                        self.samples.append(\n",
    "                            (path, self.v2id_to_info[v2_class_code][\"label_id\"])\n",
    "                        )\n",
    "                    else:\n",
    "                        self.samples.append((path, v2_class_code))\n",
    "\n",
    "    def _load_class_mapping(\n",
    "        self,\n",
    "        infofile_path,\n",
    "        use_imagenet_a_classes,\n",
    "        imagenet_a_class_code_to_idx: dict[str, int],\n",
    "    ):\n",
    "        mapping = {}\n",
    "        with open(infofile_path) as f:\n",
    "            data = json.load(f)\n",
    "            for idx, item in enumerate(data):\n",
    "                if use_imagenet_a_classes:\n",
    "                    if item[\"wnid\"] in imagenet_a_class_code_to_idx.keys():\n",
    "                        mapping[item[\"cid\"]] = {\n",
    "                            \"label_id\": imagenet_a_class_code_to_idx[item[\"wnid\"]],\n",
    "                            \"ia_code\": item[\"wnid\"],\n",
    "                            \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\"),\n",
    "                        }\n",
    "                else:\n",
    "                    mapping[item[\"cid\"]] = {\n",
    "                        \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\")\n",
    "                    }\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e503ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(dataset, train_percentage=0.5, validation_percentage=0.25):\n",
    "    # Load data\n",
    "\n",
    "    # Create train validation and test samples\n",
    "    num_samples = len(dataset)\n",
    "    training_sample = int(num_samples * train_percentage + 1)\n",
    "    validation_sample = int(num_samples * validation_percentage)\n",
    "    test_sample = num_samples - training_sample - validation_sample\n",
    "\n",
    "    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [training_sample, validation_sample, test_sample]\n",
    "    )\n",
    "\n",
    "    return (training_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    training_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    transform=None,\n",
    "    num_workers=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
    "    \"\"\"\n",
    "\n",
    "    if not transform:\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def embed_dataset_classnames(dataset: ImageNetA, model, templates=[\"a photo of a {}.\"]):\n",
    "    \"\"\"\n",
    "    Embed the classnames in the prompt template.\n",
    "    Return the classnames and the normalized textual features.\n",
    "    \"\"\"\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    classnames = dataset.class_code_to_name.values()\n",
    "\n",
    "    texts_z_views = []\n",
    "    for template in templates:\n",
    "        descriptions = [template.format(c) for c in classnames]\n",
    "        text_tokens = clip.tokenize(descriptions).to(DEVICE)\n",
    "\n",
    "        # Get the normalized textual features\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "            texts_z_views.append(texts_z)\n",
    "\n",
    "    # Evaluate the mean representation\n",
    "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
    "\n",
    "    # Renormalise\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return classnames, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a082f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = ImageNetA()\n",
    "dataset_v2 = ImageNetV2(imagenet_a=dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74884d0c",
   "metadata": {},
   "source": [
    "# 2. Reproducing Coop\n",
    "\n",
    "With pre-training on ImageNetV2\n",
    "\n",
    "TODO: explain why!\n",
    "\n",
    "### Base Model (Coop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c15192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(model_class, dataset):\n",
    "    classnames, _ = embed_dataset_classnames(dataset, vis_net)\n",
    "    n_ctx = 4\n",
    "    ctx_init = \"\"\n",
    "    class_token_position = \"end\"\n",
    "    csc = False\n",
    "    coop_net = model_class(\n",
    "        classnames=classnames,\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        csc=csc,\n",
    "    ).to(DEVICE)\n",
    "    return coop_net\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"./working_directory/model.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.text_projection\n",
    "        )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b24dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic mechanics are taken from the Lab Number 3 of AY 24/25\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(\n",
    "                clip_model.token_embedding.weight.device\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        print(classnames)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        # print(\"+++\")\n",
    "        # print(\"Prompts:\")\n",
    "        # for p in prompts:\n",
    "        #     print(p)\n",
    "        # print(\"+++\")\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(\n",
    "            clip_model.token_embedding.weight.device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "        self.ctx_checkpoint = ctx_vectors.detach().clone()\n",
    "\n",
    "    def reset_ctx(self):  # https://discuss.pytorch.org/t/reset-model-weights/19180\n",
    "        with torch.no_grad():\n",
    "            self.ctx.copy_(self.ctx_checkpoint)\n",
    "\n",
    "        self.ctx.requires_grad = True\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        with torch.no_grad():\n",
    "            self.ctx_checkpoint.copy_(self.ctx)\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,  # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i,  # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model\n",
    "\n",
    "        self.prompt_learner = PromptLearner(\n",
    "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc\n",
    "        )\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def reset_ctx(self):\n",
    "        self.prompt_learner.reset_ctx()\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        self.prompt_learner.set_ctx_checkpoint()\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    net: OurCLIP,\n",
    "    data_loader: torch.utils.data.DataLoader,  # type: ignore\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cost_function,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training step (for CoOp).\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        \"\"\" print(f\"input shape {inputs.shape}\")\n",
    "        print(f\"input type {inputs.dtype}\")\n",
    "        print(f\"labels{targets}\") \"\"\"\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        # max() returns (maximum_value, index_of_maximum_value)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        del inputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(inputs)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coop(\n",
    "    net,\n",
    "    dataset_splits,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "    skip_test=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    @param: dataset_class\n",
    "    @param: dataset_splits tuple that contains (training, validation, test)\"\"\"\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        dataset_splits[0],\n",
    "        dataset_splits[1],\n",
    "        dataset_splits[2],\n",
    "        transform=basic_image_transformations,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\"\n",
    "    )\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [{\"params\": net.parameters()}],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "    )\n",
    "\n",
    "    # Define the cost function\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Computes evaluation results before training\n",
    "    if not skip_test:\n",
    "        print(\"Before training:\")\n",
    "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function\n",
    "        )\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    if not skip_test:\n",
    "        print(\"After training:\")\n",
    "\n",
    "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_v2)\n",
    "splitted_datasets = get_dataset_split(dataset_v2)\n",
    "main_coop(coop_net, splitted_datasets, batch_size=16, skip_test=True)\n",
    "torch.save(coop_net.state_dict(), \"./working_directory/model_coop.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95816e54",
   "metadata": {},
   "source": [
    "# 3. Reproducing TPT\n",
    "\n",
    "We are always using OpenAI weights.\n",
    "\n",
    "TODO: explain why we are doing this!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4942a35",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "As in the paper: random crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ed05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmix_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Basic original transform (non-augmented)\n",
    "original_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Wrapper Dataset that takes a base dataset + index of the sample to augment\n",
    "class AugmentSingleSampleDataset(Dataset):\n",
    "    def __init__(self, base_dataset, sample_idx, num_augments=63):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sample_idx = sample_idx\n",
    "        self.num_augments = num_augments\n",
    "        self.augmix_transform = get_augmix_transform()\n",
    "        self.original_transform = original_transform\n",
    "\n",
    "        # Extract the image once to avoid loading it 64 times\n",
    "        image, label = self.base_dataset[self.sample_idx]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            self.image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            self.image = image\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_augments + 1  # 63 augments + 1 original\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            image = self.original_transform(self.image)\n",
    "        else:\n",
    "            image = self.augmix_transform(self.image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a76f89",
   "metadata": {},
   "source": [
    "### TPT Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52835f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_confident_samples(logits, top_p):\n",
    "    \"\"\"\n",
    "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
    "    \"\"\"\n",
    "    assert 0 <= top_p < 1, \"The value must be between 0 and 1\"\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[\n",
    "        : int(batch_entropy.size()[0] * top_p)\n",
    "    ]\n",
    "    return logits[idx], idx\n",
    "\n",
    "\n",
    "def compute_avg_entropy(outputs):\n",
    "    \"\"\"\n",
    "    Compute marginal entropy of samples and return the average.\n",
    "    \"\"\"\n",
    "    # Calculate probabilities from logits\n",
    "    probs = outputs.softmax(dim=1)\n",
    "    # To avoid log(0), clamp probabilities to a minimum value\n",
    "    probs = probs.clamp(min=1e-9)\n",
    "    entropy = -(probs * probs.log()).sum(dim=1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_tpt(\n",
    "    net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    @param net takes a OurClip model type\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        range(len(dataset)),\n",
    "        desc=\"TPT_testing\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        total=len(dataset),\n",
    "    )\n",
    "    # Iterate over the indices of the test set\n",
    "    try:\n",
    "        for sample_idx in pbar:  # Iterate through indices\n",
    "            net.reset_ctx()\n",
    "            optimizer.load_state_dict(optimizer_state_dict)\n",
    "            # print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
    "\n",
    "            # Create augmented dataset for the current sample\n",
    "            aug_data = AugmentSingleSampleDataset(\n",
    "                dataset, sample_idx, num_augments=num_aug\n",
    "            )  # Pass the base dataset and index\n",
    "\n",
    "            # Create a DataLoader for the augmented samples of this single image\n",
    "            aug_dataloader = torch.utils.data.DataLoader(  # type: ignore\n",
    "                aug_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                worker_init_fn=seed_worker,\n",
    "                generator=g,\n",
    "            )\n",
    "\n",
    "            # Process the augmented images for this sample\n",
    "            all_outputs = []\n",
    "            for images, labels in aug_dataloader:\n",
    "                try:\n",
    "                    with autocast():\n",
    "                        # print(f\"size batch {len(images)}\")\n",
    "                        images = images.to(DEVICE)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        outputs = net(images)  # Use the provided net\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        # cpu_outputs = outputs.to(\"cpu\")\n",
    "                        all_outputs.append(outputs)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "                        \"\"\" del images\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        print(torch.cuda.mem_get_info(), LINE()) \"\"\"\n",
    "\n",
    "                except:\n",
    "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                    raise\n",
    "\n",
    "            # Get the original label for this sample\n",
    "            original_image, target = dataset[sample_idx]\n",
    "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
    "            # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "            # Make target a tensor and move to device\n",
    "            target = torch.tensor([target]).to(DEVICE)\n",
    "\n",
    "            # Concatenate outputs from all batches for this sample\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "            # Select confident samples and compute average entropy\n",
    "            top_outputs, _ = select_confident_samples(all_outputs, 0.2)\n",
    "            loss = compute_avg_entropy(top_outputs)\n",
    "            # Loss computation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(original_image)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += original_image.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        del all_outputs  # type: ignore\n",
    "        del aug_data  # type: ignore\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_test(net, dataset: Dataset, run_name=\"tpt1\", num_aug=63, batch_size=64):\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    net.set_ctx_checkpoint()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "        # print(f\"{name}is in {param.requires_grad}\")\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
    "    # , weight_decay=wd, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)\n",
    "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    print(\"Test tpt:\")\n",
    "    test_loss, test_accuracy = test_step_tpt(\n",
    "        net,\n",
    "        dataset,\n",
    "        optimizer,\n",
    "        optimizer_state_dict,\n",
    "        log_writer,\n",
    "        num_aug=num_aug,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Closes the logger\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75daade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_a)\n",
    "coop_net = load_model(coop_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt_test(coop_net, dataset_a, batch_size=64, num_aug=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23602fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clear memory. (delete everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bfde4",
   "metadata": {},
   "source": [
    "## Simpler TPT with OpenCLIP\n",
    "\n",
    "The idea is to verify if TPT + CoOp with OpenCLIP implementation of CLIP and OpenAI weights and Kornia as image augmentator is equivalent to the original TPT + CoOp with OpenAI clip and weights and torchvision as image augmentator.\n",
    "\n",
    "Kornia was used to try reduce latency in the image augmentation step, as it is a GPU-accelerated library.\n",
    "\n",
    "TPT + \"one-shot\" Coop inspiration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc64a5d",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader\n",
    "\n",
    "Another dataset was made as images shall be loaded directly as tensors (so with torchvision instead of PIL) and the image augmentation is done with Kornia.\n",
    "\n",
    "TL:DR: same implementation as above, but with Kornia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the ImageNet-A dataset.\n",
    "\n",
    "    ----\n",
    "\n",
    "    The dataset is organized into subdirectories, each named with a class code (e.g., \"n01614925\").\n",
    "    Each subdirectory contains images belonging to that class. The dataset also includes a README.txt file that maps class codes to human-readable names.\n",
    "\n",
    "    The dataset is expected to be structured as follows:\n",
    "    ```\n",
    "    datasets/imagenet-a/\n",
    "        n01440764/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        n01614925/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        ...\n",
    "        README.txt\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir=\"datasets/imagenet-a\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__download_if_needed()\n",
    "\n",
    "        # Load mapping from class codes (e.g., \"n01614925\") to human-readable names\n",
    "        readme_path = os.path.join(root_dir, \"README.txt\")\n",
    "        self.class_code_to_label = self._load_class_mapping(readme_path)\n",
    "\n",
    "        # Filter valid class directories that match the mapping\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_label\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Map class codes to indices\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image file paths and corresponding labels\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "        # Inverse mapping from label index to class name\n",
    "        self.idx_to_label = {\n",
    "            idx: self.class_code_to_label[code]\n",
    "            for code, idx in self.class_code_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def __download_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if the dataset is already downloaded. If not, download it.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found at {self.root_dir}. Please download it first.\"\n",
    "            )\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        \"\"\"\n",
    "        Load class code to human-readable name mapping from README.txt.\n",
    "        Skips header lines and parses lines in format: 'n01440764 tench'.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as file:\n",
    "            lines = file.readlines()[12:]  # Skip first 12 header lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        \"\"\"\n",
    "        Walk through each class directory to gather image paths and corresponding labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_dir = os.path.join(self.root_dir, class_code)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    samples.append((image_path, label))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load image and return dictionary containing image, label index, and class name.\n",
    "\n",
    "        Returns:\n",
    "            image (tensor)\n",
    "            label (tensor)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path).float() / 255.0\n",
    "\n",
    "        if image.shape[0] == 1:  # Grayscale  RGB\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        elif image.shape[0] == 4:  # RGBA  RGB\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        elif image.shape[0] != 3:\n",
    "            raise ValueError(f\"Unsupported number of channels: {image.shape[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"\n",
    "        Get human-readable class name for a given index.\n",
    "        \"\"\"\n",
    "        return self.idx_to_label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92b5f3",
   "metadata": {},
   "source": [
    "### Faster \"AugMix\"\n",
    "\n",
    "TODO: add motivation and benchmark\n",
    "\n",
    "TPT and other TTA techniques, as already seen, use image augmentation to improve the performance of the model. Usually torchvision's AugMix [[18](#ref-augmix)] is used, but it's quite slow. For this reason it has been rewritten using kornia [[19](#ref-kornia)] which is more efficient and can be more easily customized and extended. It follows it's implementation and a small performance test to compare it with the original one.\n",
    "\n",
    "The usage of Kornia for AugMix is a way to speed up the process, as it's differentiable its gradients are suppressed. Performance wise performing random augmentation on images is not \"well parallelizable\" on a GPU, still some performance improvements can be achieved and the data can be kept on the GPU, avoiding _useless_ data transfers. Better performances when performing e.g. random cropping, resizing are expected.\n",
    "\n",
    "1. Kornia performances are generally better than torchvision's, as can be seen in [Kornia's Benchmark](https://kornia.readthedocs.io/en/stable/augmentation.html#benchmark).\n",
    "1. It has been noted that there are training [performance (model wise, accuracy) issues](https://discuss.pytorch.org/t/significant-difference-in-performance-between-torchvision-and-kornia-image-augmentations/97596) when comparing Kornia and torchvision, this is not covered in this project.\n",
    "1. Kornia's AugMix is not that well optimized as new memory allocation is performed on the device, instead of reusing a pre-allocated tensor. This can be done for future work, but it is not a priority for this project.\n",
    "1. Kornia operatoins are differentiable, so the gradients are propagated through the augmentations, which is not needed for TTA. Of course the _transformation_ class (`ImageTransform`) has a `torch.no_grad()` to avoid having unwanted gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made to compare AugMix efficiency with Kornia.\n",
    "# Note that TPT says that uses AugMix, but in reality it uses Random Crop. Even tho AugMix was used\n",
    "# and discarded later.\n",
    "class AugMixKornia(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        severity: int = 3,\n",
    "        width: int = 3,\n",
    "        depth: int = -1,\n",
    "        alpha: float = 1.0,\n",
    "        mixture_width: int = 3,\n",
    "        chain_depth: int = 3,\n",
    "        all_ops: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AugMix implementation using Kornia with closer fidelity to the original paper.\n",
    "\n",
    "        Args:\n",
    "            severity: Severity level of augmentations (1-10)\n",
    "            width: Width of augmentation chain (not used directly, kept for compatibility)\n",
    "            depth: Depth of augmentation chain (-1 for random between 1-3)\n",
    "            alpha: Dirichlet distribution parameter for mixing weights\n",
    "            mixture_width: Number of augmentation chains to mix\n",
    "            chain_depth: Number of operations in each chain\n",
    "            all_ops: Whether to use all augmentation operations\n",
    "            device: Device to run on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.severity = severity\n",
    "        self.alpha = alpha\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth if depth <= 0 else depth\n",
    "        self.all_ops = all_ops\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define augmentation operations\n",
    "        self.augmentations = self._get_augmentations()\n",
    "\n",
    "    def _get_augmentations(self) -> List[nn.Module]:\n",
    "        \"\"\"Create a list of augmentation operations that will be randomly applied\"\"\"\n",
    "        severity_factor = self.severity / 10.0\n",
    "\n",
    "        if self.all_ops:\n",
    "            # Full set of augmentations similar to original AugMix\n",
    "            return [\n",
    "                # AutoContrast\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                # Equalize\n",
    "                Ke.equalize,\n",
    "                # Posterize\n",
    "                K.RandomPosterize(bits=max(1, 8 - self.severity), p=1.0),\n",
    "                # Rotate\n",
    "                K.RandomRotation(\n",
    "                    degrees=(-30 * severity_factor, 30 * severity_factor), p=1.0\n",
    "                ),\n",
    "                # Solarize\n",
    "                K.RandomSolarize(\n",
    "                    thresholds=0.5, additions=(0.0, 0.1 * self.severity), p=1.0\n",
    "                ),\n",
    "                # Shear\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    shear=(-15 * severity_factor, 15 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # Translate\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.1 * severity_factor, 0.1 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # ColorJitter\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity,\n",
    "                    contrast=0.1 * self.severity,\n",
    "                    saturation=0.1 * self.severity,\n",
    "                    hue=0.1,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Simplified version\n",
    "            return [\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                Ke.equalize,\n",
    "                K.RandomAffine(\n",
    "                    degrees=(-15 * severity_factor, 15 * severity_factor), p=1.0\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _apply_augmentation_chain(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a random sequence of augmentations to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented image tensor (C, H, W)\n",
    "        \"\"\"\n",
    "        # Randomly select augmentations for this chain\n",
    "        op_indices = np.random.choice(\n",
    "            len(self.augmentations), size=self.chain_depth, replace=True\n",
    "        )\n",
    "\n",
    "        augmented = image  # Don't clone immediately\n",
    "        for op_idx in op_indices:\n",
    "            augmented = self.augmentations[op_idx](augmented)\n",
    "\n",
    "        return augmented.squeeze(0)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply AugMix to a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: Input batch of images (B, C, H, W) or (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented batch (same shape as input)\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            # Input validation\n",
    "            if not isinstance(images, torch.Tensor):\n",
    "                images = K.image_to_tensor(images)\n",
    "\n",
    "            if images.dim() == 3:\n",
    "                images = images.unsqueeze(0)\n",
    "\n",
    "            # Move to device if needed\n",
    "            if images.device != self.device:\n",
    "                images = images.to(self.device)\n",
    "\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            # Sample mixing weights from Dirichlet distribution\n",
    "            weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet(\n",
    "                        [self.alpha] * self.mixture_width, size=batch_size\n",
    "                    )\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, mixture_width)\n",
    "\n",
    "            # Sample weights for mixing with original\n",
    "            mix_weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet([self.alpha, self.alpha], size=batch_size)\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, 2)\n",
    "\n",
    "            # Generate augmented versions for each mixture component\n",
    "            # Pre-allocate memory for augmented versions\n",
    "            augmented = torch.empty(\n",
    "                (self.mixture_width, batch_size, *images.shape[1:]), device=self.device\n",
    "            )\n",
    "\n",
    "            for i in range(self.mixture_width):\n",
    "                augmented[i] = self._apply_augmentation_chain(images)\n",
    "\n",
    "            # Weighted sum of augmented versions\n",
    "            mixed = torch.einsum(\"mbchw,bm->bchw\", augmented, weights).to(self.device)\n",
    "\n",
    "            # Final mix with original image\n",
    "            result = (\n",
    "                mix_weights[:, 0:1, None, None] * images\n",
    "                + mix_weights[:, 1:2, None, None] * mixed\n",
    "            )\n",
    "\n",
    "            result = result.squeeze(0) if result.shape[0] == 1 else result\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Simple, yet effective, rancom crop using Kornia.\n",
    "def kornia_random_crop(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies random crop to a batch of images using Kornia's RandomResizedCrop.\n",
    "    Preserves the original image size while randomly cropping a portion.\n",
    "    \"\"\"\n",
    "    b, c, h, w = images.shape\n",
    "\n",
    "    # Create random crop transform that:\n",
    "    # 1. Crops between 50% and 100% of original area\n",
    "    # 2. Maintains original aspect ratio\n",
    "    # 3. Resizes back to original dimensions\n",
    "    transform = K.RandomResizedCrop(\n",
    "        size=(h, w),\n",
    "        # scale=(0.5, 1.0),  # Crop between 50% and 100% of original area\n",
    "        # ratio=(1.0, 1.0),  # Maintain original aspect ratio\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "        same_on_batch=False,  # Different crop for each image in batch\n",
    "    )\n",
    "\n",
    "    return transform(images)\n",
    "\n",
    "\n",
    "# Preprocessing pipeline using Kornia for CLIP models.\n",
    "# mean and std are extracted from the `preprocess` of ViT-B/16 model (OpenAI weights).\n",
    "# Note that mean and std are the same also for ViT-B/32.\n",
    "kornia_preprocess = nn.Sequential(\n",
    "    K.SmallestMaxSize(\n",
    "        224,\n",
    "        resample=kornia.constants.Resample.BICUBIC,  # type:ignore\n",
    "    ),\n",
    "    K.CenterCrop(\n",
    "        size=(224, 224),\n",
    "        resample=kornia.constants.Resample.BICUBIC,  # type:ignore\n",
    "    ),\n",
    "    kornia.enhance.Normalize(\n",
    "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]),\n",
    "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper for image transformations\n",
    "# The custom transformation can be either AugMixKornia or kornia_random_crop, or any\n",
    "# function (or class with __call__ method) with the same signature.\n",
    "class ImageTransform(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_transform, custom_transform=None, n_views=63, device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_transform = model_transform\n",
    "        self.custom_transform = custom_transform\n",
    "        self.n_views = n_views\n",
    "        self.device = device\n",
    "\n",
    "        self.eval()\n",
    "        # self.model_transform.eval()\n",
    "        # self.custom_transform.eval() if custom_transform is not None else None\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model transform and custom transform to the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            image = image.to(self.device)\n",
    "\n",
    "            if self.custom_transform is not None:\n",
    "                with torch.no_grad():\n",
    "                    views = torch.empty(\n",
    "                        (self.n_views + 1, *image.shape), device=self.device\n",
    "                    )\n",
    "                    views[:-1] = self.custom_transform(\n",
    "                        image.repeat(self.n_views, 1, 1, 1)\n",
    "                    )\n",
    "                    views[-1] = image\n",
    "                    return self.model_transform(views)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    return self.model_transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ec9a",
   "metadata": {},
   "source": [
    "#### AugMix and AugMixKornia Comparison\n",
    "\n",
    "Simple benchmark to compare the performance of the two implementations of AugMix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32244b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "image = torchvision.io.read_image(\n",
    "    \"datasets/imagenet-a/n01641577/0.038738_agama _ newt_0.7465035.jpg\"\n",
    ")\n",
    "\n",
    "n_times = 100\n",
    "n_augmentations = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original AugMix (torchvision_\n",
    "# The `preprocess` comes from `clip.load(\"ViT-B/32\")`\n",
    "# The idea is to augment the same image `n` times (as if they were different batches) and get the average latency.\n",
    "# The code follows a straightforward approach to augment the image as done for TPT: [B, C, H, W], where B is\n",
    "# composed by `n` augmentation images and 1 original image.\n",
    "# Output for both methods is a tensor of shape [B, C, H, W] where B = n + 1.\n",
    "\n",
    "preprocess = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(\n",
    "            size=224,\n",
    "            interpolation=torchvision.transforms.InterpolationMode.BICUBIC,\n",
    "            max_size=None,\n",
    "            antialias=True,\n",
    "        ),\n",
    "        torchvision.transforms.CenterCrop(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "            std=(0.26862954, 0.26130258, 0.27577711),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmix = torchvision.transforms.AugMix()\n",
    "\n",
    "transform = torchvision.transforms.Compose([augmix, preprocess])\n",
    "\n",
    "# Compose needs PIL images, so we need to convert it\n",
    "# Image casting is not benched as it could easily be done in the dataloader\n",
    "# by reading the image with PIL instead of torch.\n",
    "pil_image = torchvision.transforms.functional.to_pil_image(image)  # type: ignore\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(n_times)):\n",
    "    transformed = [transform(pil_image) for _ in range(n_augmentations)]\n",
    "    transformed = torch.stack(transformed)\n",
    "end = time.time()\n",
    "\n",
    "torchvision_latency = (end - start) / n_times * 1000\n",
    "print(f\"torchvision latency: {torchvision_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e561e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing could be done with cuda events, but for simplicity we use time.time()\n",
    "\n",
    "kornia_augmix = AugMixKornia()\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(n_times)):\n",
    "    images = image.float().div(255).repeat(63, 1, 1, 1)\n",
    "    views = kornia_augmix(images)\n",
    "    views = kornia_preprocess(views)\n",
    "end = time.time()\n",
    "\n",
    "kornia_latency = (end - start) / n_times * 1000\n",
    "print(f\"Latency per image: {kornia_latency:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a528b1",
   "metadata": {},
   "source": [
    "##### **Results**\n",
    "\n",
    "**Table 1**: Performance comparison between torchvision and kornia implementations of AugMix. Test performed on a single NVIDIA RTX 4060 (140W, performance mode). This was tested also on CPU and the results were similar, thus showing that the performance difference is not due to the GPU, but rather to the implementation of the augmentations (and performing full AugMix on the GPU is, of course, not well parallelizable).\n",
    "\n",
    "It's worth noting that the performance of Kornia is better than torchvision's, as expected. The speedup is not huge, mostly because AugMix performs random operations on the images, which is not well parallelizable on a GPU (warp divergence) Still, the data is kept on the GPU, avoiding multiple data copies and transfers.\n",
    "\n",
    "E.g. augmenting with AugMix has the generation of the augmented images on the CPU, then data is copied to the GPU, so the copy is e.g. 64 images at once. While with Kornia only the original image is copied on the GPU, then augmentations are done on device. This inherently speeds up the process.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| **Implementation** | **Time (ms)** | **Speedup** |\n",
    "| :----------------: | :-----------: | :---------: |\n",
    "|    torchvision     |    742.37     |     1.0     |\n",
    "|       kornia       |    552.19     |    1.35     |\n",
    "|      $\\Delta$      |    190.18     |    0.35     |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d90928",
   "metadata": {},
   "source": [
    "#### Preprocess\n",
    "\n",
    "The preprocessing done with Kornia needs to be the same as the one done with torchvision (`torchvision.transforms.Compose` saw above). This is verified by comparing the output of the two implementations.\n",
    "\n",
    "Warning: Differences are not noticable by eye, but they are there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c85722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = torchvision.transforms.functional.to_pil_image(image)  # type: ignore\n",
    "\n",
    "preprocessed_torchvision = preprocess(pil_image)\n",
    "\n",
    "preprocessed_kornia = kornia_preprocess(image.float().div(255)).squeeze(0)\n",
    "\n",
    "# Visuaize both on the same plot and row\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n",
    "\n",
    "axes[0].imshow(preprocessed_torchvision.permute(1, 2, 0).cpu().numpy())\n",
    "axes[0].set_title(\"AugMix (torchvision)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(preprocessed_kornia.permute(1, 2, 0).cpu().numpy())\n",
    "axes[1].set_title(\"AugMix (kornia)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9910555",
   "metadata": {},
   "source": [
    "### Image-A Builder\n",
    "\n",
    "Build the dataset and dataloader with image-agumentation at data-loading time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImagenetA(\n",
    "    augmenter: ImageTransform,\n",
    "    root_dir=\"datasets/imagenet-a\",\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the ImageNet-A dataset. Defaults to 1 element per batch.\n",
    "    Non modifiable. No shuffling.\n",
    "    Args:\n",
    "        augmenter (callable):\n",
    "        root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (DataLoader): DataLoader for the ImageNet-A dataset.\n",
    "        dataset (ImageNetADataset): The underlying dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle the batch of images and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images = batch[0][0]\n",
    "\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        labels = batch[0][1]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    dataset = ImageNetADataset(root_dir=root_dir, transform=augmenter)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bdaf1",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "This is as simple benchmark function to easily and reliably test the performance of different TTA implementations. The idea is that images are fed in the same way to the model. The `forward` method of the model internally manages every detail of the TTA (image augmentation excluded).\n",
    "\n",
    "Timing is done with cuda events, only on the `forward` method as we are mostly interested in the inference time of the model, not the data loading/augmentation time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str,\n",
    "    comment: str,\n",
    "    reduce: Optional[int | None] = None,\n",
    "    visualize: Optional[bool] = False,\n",
    "):\n",
    "    \"\"\"Benchmark the model on the dataset.\n",
    "\n",
    "    The model must return logits.\n",
    "    \"\"\"\n",
    "\n",
    "    board = SummaryWriter(comment=comment)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    times = []\n",
    "\n",
    "    total_tqdm = reduce if reduce is not None else len(dataloader)\n",
    "    # \n",
    "    # ascii=\" \"\n",
    "    # ascii=' >='\n",
    "    start_event, end_event = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n",
    "        enable_timing=True\n",
    "    )\n",
    "    for image, label in tqdm(dataloader, total=total_tqdm, ascii=\" \"):\n",
    "        image = image.to(device)\n",
    "\n",
    "        start_event.record()  # type: ignore\n",
    "        pred_class = model(image)\n",
    "        end_event.record()  # type: ignore\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        times.append(start_event.elapsed_time(end_event))\n",
    "\n",
    "        # del image\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total += 1\n",
    "        correct += int((pred_class == label))\n",
    "\n",
    "        if reduce:\n",
    "            if total > reduce:\n",
    "                break\n",
    "\n",
    "        # break\n",
    "        board.add_scalar(\"accuracy\", correct / total, total)\n",
    "        board.add_scalar(\"dbg/label/predict_class\", pred_class, total)\n",
    "        board.add_scalar(\"dbg/label/label\", label, total)\n",
    "\n",
    "        running_accuracy = correct / total\n",
    "\n",
    "        if visualize:\n",
    "            print(f\"[{label} || {pred_class}] | Acc: [{running_accuracy*100:.2f}%]\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    latency = (np.array(times).sum() / total).item()  # ms\n",
    "\n",
    "    board.add_scalar(\"metrics/latency (ms)\", latency)\n",
    "    board.add_scalar(\"metrics/accuracy\", accuracy)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Latency: {latency:.2f} ms\")\n",
    "\n",
    "    return accuracy, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7683b",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "- `TPTPromptLearner`: a simple TPT prompt learner, which has a similar amount of learnable parameters as the original TPT's prompt learner, but with a simpler and more readable implementation. The idea is to simplify how the prompt learner is structured: instead of having the `class_token_position` that can be in the `end`, `middle` or `front` as in the original TPT + CoOp model, we simply split the prompt is `pre` and `post` prompts, which are then concatenated with the current class token (embedded ofc). One thing to note is that the prompt initialization, as seen in CoOp [[4](#ref-coop2021)], is done with \"a photo of a {}\", without any pre-training, as performances are close.\n",
    "- `TPTModel`: CLIP model with TPT+CoOp prompt learner.\n",
    "- `TPT`: awesome wrapper that makes possible to manage the finetuning and reset of the model after each image, it's \"invisible\" to the user and painless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: from 5_tpt.py\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: open_clip.model.CLIP,\n",
    "        arch: CLIPModels = CLIPModels.ViTB32,  # type: ignore\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer(arch)\n",
    "\n",
    "        self.__init_ctx_from_prompt(\n",
    "            tokenizer=tokenizer,\n",
    "            token_embedding=clip_model.token_embedding,\n",
    "            base_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    def __init_ctx_from_prompt(\n",
    "        self, tokenizer, token_embedding, base_prompt: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[tokenizer.sot_token_id]]).to(self.device)\n",
    "        c_token_eot = torch.tensor([[tokenizer.eot_token_id]]).to(self.device)\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = tokenizer(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = tokenizer(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = tokenizer(self.class_names).to(self.device)\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = tokenizer(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = token_embedding(c_token_sot)\n",
    "            self.embedded_eot = token_embedding(c_token_eot)\n",
    "            self.embedded_pad = token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            clean_embed = embedded_class_names[i][mask[i]]  # [num_valid_tokens, 512]\n",
    "            clean_embeddings.append(\n",
    "                clean_embed.unsqueeze(0)\n",
    "            )  # [1, num_valid_tokens, 512]\n",
    "\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "\n",
    "        for i, embed in enumerate(clean_embeddings):\n",
    "            self.register_buffer(f\"class_embed_{i}\", embed)\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)  # type: ignore\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)  # type: ignore\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # embeddeD_max_len: 77\n",
    "            # embedded_prefix: torch.Size([4, 512])\n",
    "            # embedded_class_names: torch.Size([1, 1, 512])\n",
    "            # embedded_suffix: torch.Size([0, 512]\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - getattr(self, f\"class_embed_{i}\").shape[1]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            ) - 2  # # -2 for SOT and EOT\n",
    "\n",
    "            ## embedded sot shape: torch.Size([1, 1, 512])\n",
    "            ## embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            ## embedded class names shape: torch.Size([1, 1, 1, 512])\n",
    "            ## embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            ## embedded eot shape: torch.Size([1, 1, 512])\n",
    "            ## effective padding shape: torch.Size([1, 70, 512])\n",
    "            ## Prompt shape: torch.Size([1, 77, 512])\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    # self.embedded_class_names[i],\n",
    "                    getattr(self, f\"class_embed_{i}\"),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    self.embedded_pad.repeat(1, padding_size, 1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "        with torch.no_grad():\n",
    "            self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "            self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        arch: CLIPModels,\n",
    "        pretrained: str,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: open_clip.model.CLIP\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(  # type:ignore\n",
    "            model_name=arch,  # type:ignore\n",
    "            pretrained=pretrained,\n",
    "            device=device,\n",
    "            force_quick_gelu=True,\n",
    "        )\n",
    "\n",
    "        self.model = clip_model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)  # type:ignore\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.visual: open_clip.transformer.VisionTransformer = (  # type:ignore\n",
    "            clip_model.visual\n",
    "        )  # type:ignore\n",
    "        self.visual.eval()\n",
    "\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.attn_mask = clip_model.attn_mask\n",
    "        self.text_pool_type = clip_model.text_pool_type\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            arch=arch, class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "    def _pool(self, x: torch.Tensor):\n",
    "        if self.visual.attn_pool is not None:\n",
    "            if self.visual.attn_pool_contrastive is not None:\n",
    "                # This is untested, WIP pooling that should match paper\n",
    "                x = self.visual.ln_post(\n",
    "                    x\n",
    "                )  # TBD LN first or separate one after each pool?\n",
    "                tokens = self.visual.attn_pool(x)\n",
    "                if self.visual.attn_pool_type == \"parallel\":\n",
    "                    pooled = self.visual.attn_pool_contrastive(x)\n",
    "                else:\n",
    "                    assert self.visual.attn_pool_type == \"cascade\"\n",
    "                    pooled = self.visual.attn_pool_contrastive(tokens)\n",
    "            else:\n",
    "                # this is the original OpenCLIP CoCa setup, does not match paper\n",
    "                x = self.visual.attn_pool(x)\n",
    "                x = self.visual.ln_post(x)\n",
    "                pooled, tokens = self.visual._global_pool(x)\n",
    "        elif self.visual.final_ln_after_pool:\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "            pooled = self.visual.ln_post(pooled)\n",
    "        else:\n",
    "            x = self.visual.ln_post(x)\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "\n",
    "        return pooled, tokens, x\n",
    "\n",
    "    def _forward_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.visual._embeds(x)\n",
    "        x = self.visual.transformer(x)\n",
    "\n",
    "        pooled, tokens, x = self._pool(x)\n",
    "\n",
    "        if self.visual.proj is not None:\n",
    "            pooled = pooled @ self.visual.proj\n",
    "        if self.visual.output_tokens:\n",
    "            return pooled, tokens, x  # type:ignore\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def __encode_image(\n",
    "        self, image, normalize: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pooled_pre_norm = self._forward_image(image)\n",
    "        return (\n",
    "            F.normalize(pooled_pre_norm, dim=-1) if normalize else pooled_pre_norm\n",
    "        )  # type:ignore\n",
    "\n",
    "    def __encode_text(self, text=None, normalize: bool = False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.prompt_learner().to(cast_dtype)\n",
    "\n",
    "        text = self.prompt_learner.tokenized_initial_full_prompt\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = text_global_pool(x, text, self.text_pool_type)  # type:ignore\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                x = self.text_projection(x)\n",
    "            else:\n",
    "                x = x @ self.text_projection\n",
    "\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.__encode_image(image, normalize=True)\n",
    "        text_features = self.__encode_text(normalize=True)\n",
    "\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, image_features\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the prompt learner to its initial state.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.reset()\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "        model = TPTModel(\n",
    "            class_names=class_names,\n",
    "            arch=arch,\n",
    "            pretrained=pretrained,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # # # TEST - learnable layer norm\n",
    "        # self.model.visual.ln_post.requires_grad_(True)\n",
    "        # self.model.ln_final.requires_grad_(True)\n",
    "\n",
    "        # Get all trainable parameters (filter by requires_grad)\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # Initialize optimizer with trainable parameters\n",
    "        self.optim = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim_init = deepcopy(self.optim.state_dict())\n",
    "\n",
    "        # Initialize backup lists\n",
    "        self.ln_backup = {\n",
    "            \"weights\": [],  # For gamma (scale)\n",
    "            \"biases\": [],  # For beta (shift)\n",
    "        }\n",
    "\n",
    "        # Backup all LN params in text encoder\n",
    "        for block in self.model.transformer.resblocks:  # type:ignore\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_1.weight.data.detach().clone()\n",
    "            )  # gamma for ln_1\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_1.bias.data.detach().clone()\n",
    "            )  # beta for ln_1\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_2.weight.data.detach().clone()\n",
    "            )  # gamma for ln_2\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_2.bias.data.detach().clone()\n",
    "            )  # beta for ln_2\n",
    "\n",
    "        # Backup final LN\n",
    "        self.ln_backup[\"weights\"].append(\n",
    "            self.model.ln_final.weight.data.detach().clone()\n",
    "        )\n",
    "        self.ln_backup[\"biases\"].append(self.model.ln_final.bias.data.detach().clone())\n",
    "\n",
    "    def set_tta_steps(self, tta_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the number of TTA steps.\n",
    "\n",
    "        Args:\n",
    "            tta_steps (int): Number of TTA steps.\n",
    "        \"\"\"\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            # input = input[-1].unsqueeze(0)\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return pred_class\n",
    "\n",
    "    def __select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def __avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def __reset(self) -> None:\n",
    "        \"\"\"Full reset of prompt learner and optimizer state\"\"\"\n",
    "        # 1. Reset prompt embeddings\n",
    "        for p in self.model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        self.model.reset()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "        #     self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            # Reset LN params in text encoder\n",
    "            for block in self.model.transformer.resblocks:  # type:ignore\n",
    "                block.ln_1.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_1.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "                block.ln_2.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_2.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "\n",
    "            # # Reset final LN\n",
    "            self.model.ln_final.weight.data.copy_(self.ln_backup[\"weights\"][-1].clone())\n",
    "            self.model.ln_final.bias.data.copy_(self.ln_backup[\"biases\"][-1].clone())\n",
    "\n",
    "        # # 2. Reset optimizer state\n",
    "        self.optim.load_state_dict(deepcopy(self.optim_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e0463",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f25939",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_random_crop,\n",
    "    n_views=63,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "dataloader, dataset = ImagenetA(augmenter, num_workers=5)\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tpt\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff1bba",
   "metadata": {},
   "source": [
    "## Baseline - Zero-Shot CLIP\n",
    "\n",
    "Results are compare with both Zero-Shot CLIP with OpenAI weights, impelmentation by OpenAI and OpenCLIP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d4fee",
   "metadata": {},
   "source": [
    "### OpenAI CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6daa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: clip.model.CLIP,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [clip.tokenize(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Returns the predicted class.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(x)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        return int(pred_class)\n",
    "\n",
    "\n",
    "# Load the CLIP model\n",
    "clip_model, _ = clip.load(\"ViT-B/16\", device=DEVICE, jit=True)\n",
    "clip_model.eval()\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=None,\n",
    "    comment=\"baseline clip openai\",\n",
    "    visualize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2bfb7",
   "metadata": {},
   "source": [
    "### OpenCLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd66738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        # Precompute text features\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "        return int(pred_class)\n",
    "\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    "    jit=False,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    DEVICE,\n",
    "    reduce=None,\n",
    "    comment=\"baseline clip openclip\",\n",
    "    visualize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb02ea",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "TODO: compare TPT (clip) with TPT (openclip). Show that the difference is minimal.\n",
    "\n",
    "TODO: compare clip and openclip on zero-shot classification.\n",
    "\n",
    "OBV everything in the same table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffa1bf",
   "metadata": {},
   "source": [
    "# 4. Trying to get a better at TTA (our contribution)\n",
    "\n",
    "- augmix (as a note criticizing TPT's paper.)\n",
    "- augment top 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d760c2",
   "metadata": {},
   "source": [
    "TPT with CoOp is quite slow due to the finetuning of the prompt. The idea is to try to get better or similar performances getting inspiration form TPT and other TTA methods, but, possibly, without any finetuning, or if needed, with a possibly faster finetuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79099c",
   "metadata": {},
   "source": [
    "$\\delta = \\Alpha + 3* \\gamma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981b2a7",
   "metadata": {},
   "source": [
    "## A. Augment Top 1 \n",
    "\n",
    "The idea is to remove the prompt learner (CoOp style) from the TPT model and use the most confident samples logits (top 1%) and the original image ones, average them and use the average logits as the final prediction.\n",
    "\n",
    "We want to keep the most confident samples logits, as they are the ones that are more likely to be correct, and average them with the original image logits, to avoid getting a too biased prediction and \"losing context\".\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "pred_class = int(marginal_prob.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "We expect this to be way faster than TPT (as no finetuning is done) and to have slightly better performances, as we are using the most confident samples logits, which are more likely to be correct, and keeping the original image logits to avoid getting a too biased prediction.\n",
    "\n",
    "- Why the \"biased prediction\"? Because the augmentations are random crop, so the model might be biased towards the augmented images, which might not be representative of the original image. By averaging the logits, we can mitigate this bias and get a more reliable prediction.\n",
    "- Is this the best method to do this? No, but it's a good starting point, it's simple and it works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395fd1e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad703ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        # self.logit_scale = model.log\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            initial_image_features = image_features[-1:]\n",
    "            filtered_image_features = image_features[:-1:]\n",
    "\n",
    "            # filter logits\n",
    "            initial_logits = (\n",
    "                self.logit_scale * initial_image_features @ self.text_features.t()\n",
    "            )\n",
    "            filtered_logits = (\n",
    "                self.logit_scale * filtered_image_features @ self.text_features.t()\n",
    "            )\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                # filtered_logits, top=1 / filtered_logits.shape[0]\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            # selected_logits = selected_logits.mean(0, keepdim=True)\n",
    "\n",
    "            # final_logits = selected_logits\n",
    "            final_logits = torch.cat((selected_logits, initial_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = int(marginal_prob.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc323ba",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"top1\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c22372",
   "metadata": {},
   "source": [
    "## B. TPT with Top 1\n",
    "\n",
    "The idea is pretty simple: use the top 1 + original image logits as specified above, but on TPT w/ CoOp.\n",
    "\n",
    "The implementation is straightforward: override the `forward` method of the `TPT` class (the one which manages the finetuning of the `TPTModel`), so that it uses the top 1 + original image logits instead of the prompt learner. Note that the finetuning of the model is kept as is, only the final prediction is changed.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.__select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342c6df",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPTTop1(TPT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            pretrained=pretrained,\n",
    "            arch=arch,\n",
    "            class_names=class_names,\n",
    "            tta_steps=tta_steps,\n",
    "            lr=lr,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            original_logits = logits[-1:]\n",
    "            filtered_logits = logits[:-1:]\n",
    "\n",
    "            # Get top k logits\n",
    "            selected_logits, _ = self.__select_confident_samples(\n",
    "                filtered_logits,\n",
    "                top=0.1,\n",
    "            )\n",
    "\n",
    "            final_logits = torch.cat((selected_logits, original_logits), dim=0)\n",
    "\n",
    "            marginal_prob = F.softmax(final_logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cc131",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",  # type:ignore\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),  # type:ignore\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tpt-top1\", visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae26cb0",
   "metadata": {},
   "source": [
    "## C. Self-Supervised Retrieval: RENAME THIS\n",
    "\n",
    "Here we get inspiration from DinoV2's [[5](#ref-dinov2)] self-supervised retrieval method. Using 63+1 augmentations, as in TPT, 6 clusters (kmeans) are populated, then cluster confidences are computed (mean cluster confidence, metric: cosine similarity). The most confident cluster is selected and it's logits are averages togheter with the original image logits.\n",
    "\n",
    "We don't expect much from this methods as CLIP is hasn't been trained, compared to DinoV2, for extracting saliency maps, so the clusters are not expected to be very meaningful. Still, this is interesting as it can be used to visualize the clusters to try to interpret, a little bit, what the model is doing. Of course this is unreliable.\n",
    "\n",
    "TODO: add plotting examples: cherry pick one that works well and one that doesn't work well at all, to show the difference in the clusters and how they are not much meaningful.\n",
    "\n",
    "TODO: it could be interesting to try to use the clusters to get a better prediction when the model is failing to classify an image, e.g. when the confidence is low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423ee3e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model = model\n",
    "        self.logit_scale = model.logit_scale.exp()\n",
    "\n",
    "        # Precompute text features\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # x: (B, 3, 224, 224)\n",
    "            image_features = self.model.encode_image(x, normalize=True)\n",
    "\n",
    "            # Move to CPU and convert to numpy for sklearn\n",
    "            features_np = image_features.cpu().numpy()\n",
    "\n",
    "            # Standardize features\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(features_np)\n",
    "\n",
    "            # Cluster features\n",
    "            from sklearn.cluster import KMeans\n",
    "\n",
    "            kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "            ###################################################\n",
    "\n",
    "            # # get the cluster with higher confidence\n",
    "            # cluster_confidences = []\n",
    "            # for cluster_idx in range(6):\n",
    "            #     cluster_features = image_features[cluster_labels == cluster_idx]\n",
    "            #     logits = self.logit_scale * cluster_features @ self.text_features.t()\n",
    "            #     cluster_confidences.append(logits.mean().item())\n",
    "\n",
    "            # # Get the cluster with the highest confidence\n",
    "            # best_cluster_idx = cluster_confidences.index(max(cluster_confidences))\n",
    "\n",
    "            # image_features_r = image_features[cluster_labels == best_cluster_idx]\n",
    "\n",
    "            # image_features_r = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            # logits = self.logit_scale * image_features_r @ self.text_features.t()\n",
    "\n",
    "            # marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "\n",
    "            # pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            ###################################################\n",
    "\n",
    "            # Cluster closer to the original image\n",
    "            cluster_confidences = []\n",
    "            for cluster_idx in range(6):\n",
    "                cosine_sim = (\n",
    "                    image_features[cluster_labels == cluster_idx]\n",
    "                    @ image_features[-1:].t()\n",
    "                )\n",
    "                cluster_confidences.append(cosine_sim.mean().item())\n",
    "\n",
    "            # Get the cluster with the highest confidence\n",
    "            best_cluster_idx = cluster_confidences.index(max(cluster_confidences))\n",
    "\n",
    "            image_features_r = image_features[cluster_labels == best_cluster_idx]\n",
    "\n",
    "            image_features_r = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            logits = self.logit_scale * image_features_r @ self.text_features.t()\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            # # Accuracy: 55.72%\n",
    "            # # Latency: 234.35 ms\n",
    "\n",
    "            ####################################################\n",
    "            # # # ll = []\n",
    "\n",
    "            # # # for cluster_idx in range(6):\n",
    "            # # #     cluster_indices = (cluster_labels == cluster_idx)\n",
    "            # # #     if len(cluster_indices) == 0:\n",
    "            # # #         continue\n",
    "\n",
    "            # # #     cluster_features = image_features[cluster_indices]\n",
    "            # # #     logits = self.logit_scale * cluster_features @ self.text_features.t()\n",
    "            # # #     logits = logits.mean(dim=0)\n",
    "\n",
    "            # # #     ll.append(logits)\n",
    "            # # #     print(logits.shape)\n",
    "            # # # # ll\n",
    "            # # # # print(ll.shape)\n",
    "            # # # ll = torch.stack(ll, dim=0)\n",
    "            # # # ll = ll.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # # # marginal_prob = F.softmax(ll, dim=1).mean(0)\n",
    "            # # # pred_class = marginal_prob.argmax().item()\n",
    "            ####################################################\n",
    "\n",
    "            # exit()\n",
    "\n",
    "            # image_features = torch.cat((image_features_r, image_features[-1:]), dim=0)\n",
    "\n",
    "            # logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            # marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            # pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "            # # Visualize each cluster's images\n",
    "            # import matplotlib.pyplot as plt\n",
    "            # from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "            # for cluster_idx in range(6):\n",
    "            #     # Get indices of images in this cluster\n",
    "            #     cluster_indices = (cluster_labels == cluster_idx).nonzero()[0]\n",
    "\n",
    "            #     if len(cluster_indices) == 0:\n",
    "            #         continue\n",
    "\n",
    "            #     print(f\"Cluster {cluster_idx} has {len(cluster_indices)} images\")\n",
    "\n",
    "            #     # Setup plot\n",
    "            #     cols = min(8, len(cluster_indices))\n",
    "            #     rows = (len(cluster_indices) + cols - 1) // cols\n",
    "            #     plt.figure(figsize=(cols * 2, rows * 2))\n",
    "            #     plt.suptitle(f\"Cluster {cluster_idx} - {len(cluster_indices)} images\")\n",
    "\n",
    "            #     for plot_idx, img_idx in enumerate(cluster_indices, start=1):\n",
    "            #         if plot_idx > cols * rows:\n",
    "            #             break\n",
    "\n",
    "            #         img = x[img_idx].permute(1, 2, 0).cpu().numpy()\n",
    "            #         img = to_pil_image(img)\n",
    "            #         plt.subplot(rows, cols, plot_idx)\n",
    "            #         plt.imshow(img)\n",
    "            #         plt.axis('off')\n",
    "\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.show()\n",
    "\n",
    "            ####################################################################\n",
    "\n",
    "            # # # Visualize cluster_labels\n",
    "            # import matplotlib.pyplot as plt\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "            # plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "            # plt.title('KMeans Clustering of Image Features')\n",
    "            # plt.xlabel('Feature 1')\n",
    "            # plt.ylabel('Feature 2')\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "\n",
    "            # from sklearn.manifold import TSNE\n",
    "\n",
    "            # # Apply t-SNE\n",
    "            # tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "            # X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "            # import matplotlib.pyplot as plt\n",
    "\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "\n",
    "            # # If you did clustering, color by cluster\n",
    "            # scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "\n",
    "            # # If you have true labels, you could color by those instead\n",
    "            # # scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
    "\n",
    "            # plt.colorbar(scatter)\n",
    "            # plt.title('t-SNE Visualization with Clusters')\n",
    "            # plt.xlabel('t-SNE dimension 1')\n",
    "            # plt.ylabel('t-SNE dimension 2')\n",
    "            # plt.show()\n",
    "\n",
    "            # cosine_sim = torch.mm(image_features, image_features.t())\n",
    "            # cosine_sim = image_features @ image_features.t()\n",
    "\n",
    "            # # kmeans clusters\n",
    "            # kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "            # kmeans.fit(cosine_sim.cpu().numpy())\n",
    "            # print(kmeans.labels_)\n",
    "\n",
    "            # show clusters\n",
    "\n",
    "            # #  2. Get similarities of the last image ([-1]) with all others\n",
    "            # last_img_similarities = cosine_sim[-1, :]  # (B,)\n",
    "\n",
    "            # # 3. Sort indices (descending order, excluding the last image itself)\n",
    "            # sorted_indices = torch.argsort(last_img_similarities, descending=True).cpu().numpy()\n",
    "            # # sorted_indices = sorted_indices[sorted_indices != len(x)-1]  # Remove self-comparison\n",
    "\n",
    "            # # 4. Visualize the last image + top-k most similar images\n",
    "            # k = 63  # Number of similar images to display\n",
    "            # cols = 8\n",
    "            # rows = (image_features.shape[0] + cols - 1) // cols\n",
    "            # plt.figure(figsize=(cols * 2, rows * 2))\n",
    "            # for i, idx in enumerate(sorted_indices[:k], start=2):\n",
    "            #     img = x[idx].permute(1, 2, 0).cpu().numpy()\n",
    "            #     img = TF.to_pil_image(img)\n",
    "\n",
    "            #     plt.subplot(rows, cols, i)\n",
    "            #     plt.imshow(img)\n",
    "            #     plt.title(f\"Sim: {last_img_similarities[idx]:.3f}\")\n",
    "            #     plt.axis('off')\n",
    "\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "            # Perform KMeans clustering\n",
    "            # kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "            # kmeans.fit(cosine_sim.cpu().numpy())\n",
    "            # labels = kmeans.labels_\n",
    "            # print(labels)\n",
    "\n",
    "            # exit()\n",
    "\n",
    "        return int(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3b3b4",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=DEVICE  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=200, comment=\"\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe0528",
   "metadata": {},
   "source": [
    "## C. (stupid) Adaptive Layer Norm \n",
    "\n",
    "dire che ho provato anche a fare layernorm learnable e effettivamente funziona meglio, ma che rottura di coglioni, lo volevo senza backprop.\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efd950",
   "metadata": {},
   "source": [
    "## D. TNT\n",
    "\n",
    "TODO: add amount of learnable parameters wrt TPT\n",
    "\n",
    "Implementation of TNT [[6](#ref-tnt2023)]. It's pretty straightforward, it's CLIP with learnable random noise on the augmented images (noise shape: `(3, height, width)`).\n",
    "\n",
    "We expect this to be slightly faster than TPT, and have slightly better performances, as in the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce6417",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c698216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.tnt_steps = tnt_steps\n",
    "        self.top_k = top_k\n",
    "        self.eps = epsilon\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.temperature = temperature\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "        self.noise = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise = None\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)[-1:]\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "            probs = F.softmax(logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d77520",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNT(\n",
    "    clip_model,  # type:ignore\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tnt\", visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f184e",
   "metadata": {},
   "source": [
    "## E. TNT + Top 1\n",
    "\n",
    "It's the same as TNT, but with the top 1 + original image logits as final prediction.\n",
    "\n",
    "We expect this to be slightly more accurate than TNT.\n",
    "\n",
    "**Diff**:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "```python\n",
    "with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "```\n",
    "\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad412d5e",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNTTop1(TNT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "        tnt_steps: int = 3,\n",
    "        top_k: float = 0.1,\n",
    "        epsilon: float = 1 / 255,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float = 1.0,\n",
    "        temperature: float = 7e-3,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            class_labels=class_labels,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            tnt_steps=tnt_steps,\n",
    "            top_k=top_k,\n",
    "            epsilon=epsilon,\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if self.noise is None:\n",
    "            self.noise = torch.randn_like(\n",
    "                x[0], requires_grad=True, device=self.device\n",
    "            )  # , dtype=torch.float16)\n",
    "            self.noise.data = self.noise.clamp(-self.eps, self.eps)\n",
    "\n",
    "        self.noise.requires_grad = True\n",
    "\n",
    "        with torch.autocast(self.device):  # , dtype=torch.float16):\n",
    "            for _ in range(self.tnt_steps):\n",
    "                # x_aug = x + torch.clamp(self.noise, 0, 1)[None, ...]\n",
    "                x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "\n",
    "                image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "                logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "                # Select top-k logits\n",
    "                top_logits, top_idx = self.select_confident_samples(\n",
    "                    logits, top=self.top_k\n",
    "                )\n",
    "                top_features = image_features[top_idx]\n",
    "\n",
    "                # Entropy loss\n",
    "                prob = F.softmax(top_logits, dim=1).mean(dim=0)\n",
    "                entropy_loss = -(prob * prob.log()).sum()\n",
    "\n",
    "                # Inter-view consistency loss\n",
    "                pairwise_dist = torch.cdist(top_features, top_features, p=2)\n",
    "                inter_view_loss = pairwise_dist.sum()\n",
    "\n",
    "                # Total loss\n",
    "                loss = self.alpha * entropy_loss + self.beta * inter_view_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update noise\n",
    "                with torch.no_grad():\n",
    "                    grad = self.noise.grad\n",
    "                    self.noise -= self.lr * grad.sign()\n",
    "                    self.noise.clamp_(-self.eps, self.eps)\n",
    "                    self.noise.requires_grad = True\n",
    "                    self.noise.grad = None\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(self.device):\n",
    "            x_aug = x + self.noise[None, ...].clamp(0, 1)\n",
    "            image_features = self.model.encode_image(x_aug, normalize=True)\n",
    "            logits = self.logit_scale * image_features @ self.text_features.t()\n",
    "\n",
    "            selected_logits, _ = self.select_confident_samples(\n",
    "                logits[:-1], top=self.top_k\n",
    "            )\n",
    "            final_logits = torch.cat((selected_logits, logits[-1:]), dim=0)\n",
    "            probs = F.softmax(final_logits / self.temperature, dim=1).mean(dim=0)\n",
    "            pred_class = int(probs.argmax().item())\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3135d55",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd673ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type:ignore\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "for param in clip_model.parameters():  # type:ignore\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = TNT(\n",
    "    clip_model,\n",
    "    class_labels=dataset.class_code_to_label,\n",
    "    device=DEVICE,\n",
    "    tnt_steps=1,  # type:ignore\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "bench(\n",
    "    wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tnt-top1\", visualize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f2ffa",
   "metadata": {},
   "source": [
    "## F. TPS\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52100809",
   "metadata": {},
   "source": [
    "## G. FILM\n",
    "\n",
    "TODO: add what we expect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726c201",
   "metadata": {},
   "source": [
    "# 5. Thoughts and Conclusion\n",
    "\n",
    "Pro e contro di ogni uno\n",
    "\n",
    "TODO: plot accuracy/latency.\n",
    "\n",
    "quindi che usare backprop non conviene (citare frustaingly easy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2731d6f",
   "metadata": {},
   "source": [
    "# 6. Future Work\n",
    "\n",
    "- ai based augmentation (trying to optimize e.g. random crops)\n",
    "- do something stupid like merge TPT + TNT + frustatingly easy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29860d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19369d9",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
