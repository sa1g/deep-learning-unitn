{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b431a3b",
   "metadata": {},
   "source": [
    "_For clarity different models have been developed to handle different, and, or, similar algorithms, with the idea of having more readable code._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4204f07",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we focus on **Test-Time Adaptation (TTA)**, which has recently gained traction due to its ability to enhance model performance without requiring access to training data.\n",
    "\n",
    "In this project, we focus on **TTA for image classification**, particularly using **CLIP** [[2](#ref-clip2021)] with **TPT** [[3](#ref-tpt2022)]. Our approach involves adapting the model on **single-image test instances**, with the model being reset to its pre-trained state after each instance. This resembles **TTIA**, keeping the constraint of no retention of prior test-time knowledge (between batches, so between images).\n",
    "\n",
    "![](img/tpt.png \"Test-Time Prompt Tuning (TPT) for CLIP\")\n",
    "\n",
    "## A. TTIA\n",
    "\n",
    "> **Definition**: \"_Test-Time Instance Adaption, TTIA_ Given a classifier $f_\\mathcal{S}$ learned on the source domain $\\mathcal{D_s}$, and an unlabeled target instance $x_t \\in \\mathcal{D_T}$ under distribution shift, _test-time instance adaption_ aims to leverage the labeled knowledge implied in $\\mathcal{f_S}$ to infer the label of $x_t$ adaptively\" [[1](#ref-liang2025)]. In other words, TTIA aims to adapt the classifier $f_\\mathcal{S}$ to the target instance $x_t$ by leveraging the knowledge of the source domain $\\mathcal{D_S}$. [[1](#ref-liang2025)]\n",
    "\n",
    "TTIA differs from TTBA in that single-instance adaption is performed, instead of batch-wise adaption, giving an example the difference is between classifying a single frame of a video and classifying a sequence of frames. In both methods no memory of the previous test-time knowledge is retained.\n",
    "\n",
    "## B. Reproducibility\n",
    "\n",
    "- github link\n",
    "- seeding is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2481ae3",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Get datasets data, create datasets and dataloaderd. Seeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "# TODO: add packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "\n",
    "# Get datasets (ImageNet-A and \n",
    "# TODO: ImageNetV2)\n",
    "!gdown --fuzzy https://drive.google.com/file/d/1nfictDVptrdDwRNaxsBx0UIlP7tpDsN_/view?usp=sharing\n",
    "\n",
    "!curl https://raw.githubusercontent.com/modestyachts/ImageNetV2/refs/heads/master/data/metadata/class_info.json -o datasets/imagenetv2-matched-frequency-format-val/class_info.json\n",
    "\n",
    "!tar -xvf imagenet-a.tar -C datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as v2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import json\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import clip.model\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "import kornia.enhance as Ke\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding and reproducibility\n",
    "\n",
    "torch.manual_seed(456)\n",
    "torch.cuda.manual_seed(456)\n",
    "torch.randn(456).to(\"cuda\")\n",
    "np.random.seed(42)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "# https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "vis_net, basic_image_transformations = clip.load(\"ViT-B/16\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    def __init__(\n",
    "        self, root_dir=\"datasets/imagenet-a\", transform=basic_image_transformations\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load class code to name mapping from README.txt\n",
    "        self.class_code_to_name = self._load_class_mapping(\n",
    "            os.path.join(root_dir, \"README.txt\")\n",
    "        )\n",
    "\n",
    "        # Map class codes to integer labels\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_name\n",
    "            ]\n",
    "        )\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image paths and labels\n",
    "        self.samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_folder = os.path.join(root_dir, class_code)\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    self.samples.append((path, label))\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as f:\n",
    "            lines = f.readlines()[12:]  # Skip the first 12 lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"datasets/imagenetv2-matched-frequency-format-val\",\n",
    "        transform=basic_image_transformations,\n",
    "        use_imagenet_a_classes=True,\n",
    "        imagenet_a=None,\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.use_imagenet_a = use_imagenet_a_classes\n",
    "\n",
    "        if use_imagenet_a_classes:\n",
    "            assert (\n",
    "                type(imagenet_a) == ImageNetA\n",
    "            ), \"imagenet_a_classes set to TRUE without passing imagenet_a object\"\n",
    "            imagenet_a_class_code_to_idx = imagenet_a.class_code_to_idx\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, \"class_info.json\"),\n",
    "                use_imagenet_a_classes,\n",
    "                imagenet_a_class_code_to_idx,\n",
    "            )\n",
    "            self.class_code_to_name = copy.deepcopy(imagenet_a.class_code_to_name)\n",
    "\n",
    "        else:\n",
    "            self.v2id_to_info = self._load_class_mapping(\n",
    "                os.path.join(root_dir, \"class_info.json\"), use_imagenet_a_classes, None\n",
    "            )\n",
    "            self.class_code_to_name = {\n",
    "                idx: self.v2id_to_info[\"label\"] for idx in self.v2id_to_info.keys()\n",
    "            }\n",
    "\n",
    "        self.samples = []\n",
    "        for v2_class_code in self.v2id_to_info.keys():\n",
    "            class_folder = os.path.join(root_dir, str(v2_class_code))\n",
    "            for fname in os.listdir(class_folder):\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    path = os.path.join(class_folder, fname)\n",
    "                    if use_imagenet_a_classes:\n",
    "                        self.samples.append(\n",
    "                            (path, self.v2id_to_info[v2_class_code][\"label_id\"])\n",
    "                        )\n",
    "                    else:\n",
    "                        self.samples.append((path, v2_class_code))\n",
    "\n",
    "    def _load_class_mapping(\n",
    "        self,\n",
    "        infofile_path,\n",
    "        use_imagenet_a_classes,\n",
    "        imagenet_a_class_code_to_idx: dict[str, int],\n",
    "    ):\n",
    "        mapping = {}\n",
    "        with open(infofile_path) as f:\n",
    "            data = json.load(f)\n",
    "            for idx, item in enumerate(data):\n",
    "                if use_imagenet_a_classes:\n",
    "                    if item[\"wnid\"] in imagenet_a_class_code_to_idx.keys():\n",
    "                        mapping[item[\"cid\"]] = {\n",
    "                            \"label_id\": imagenet_a_class_code_to_idx[item[\"wnid\"]],\n",
    "                            \"ia_code\": item[\"wnid\"],\n",
    "                            \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\"),\n",
    "                        }\n",
    "                else:\n",
    "                    mapping[item[\"cid\"]] = {\n",
    "                        \"label\": item[\"synset\"][0].lower().replace(\" \", \"_\")\n",
    "                    }\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e503ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(dataset, train_percentage=0.5, validation_percentage=0.25):\n",
    "    # Load data\n",
    "\n",
    "    # Create train validation and test samples\n",
    "    num_samples = len(dataset)\n",
    "    training_sample = int(num_samples * train_percentage + 1)\n",
    "    validation_sample = int(num_samples * validation_percentage)\n",
    "    test_sample = num_samples - training_sample - validation_sample\n",
    "\n",
    "    training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [training_sample, validation_sample, test_sample]\n",
    "    )\n",
    "\n",
    "    return (training_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    training_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    transform=None,\n",
    "    num_workers=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the dataset, split it into train/val/test and return a DataLoader for each.\n",
    "    \"\"\"\n",
    "\n",
    "    if not transform:\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def embed_dataset_classnames(dataset: ImageNetA, model, templates=[\"a photo of a {}.\"]):\n",
    "    \"\"\"\n",
    "    Embed the classnames in the prompt template.\n",
    "    Return the classnames and the normalized textual features.\n",
    "    \"\"\"\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    classnames = dataset.class_code_to_name.values()\n",
    "\n",
    "    texts_z_views = []\n",
    "    for template in templates:\n",
    "        descriptions = [template.format(c) for c in classnames]\n",
    "        text_tokens = clip.tokenize(descriptions).to(DEVICE)\n",
    "\n",
    "        # Get the normalized textual features\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "            texts_z_views.append(texts_z)\n",
    "\n",
    "    # Evaluate the mean representation\n",
    "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
    "\n",
    "    # Renormalise\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return classnames, texts_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a082f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a = ImageNetA()\n",
    "dataset_v2 = ImageNetV2(imagenet_a=dataset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74884d0c",
   "metadata": {},
   "source": [
    "# 2. Reproducing Coop\n",
    "\n",
    "### Base Model (Coop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c15192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(model_class, dataset):\n",
    "    classnames, _ = embed_dataset_classnames(dataset, vis_net)\n",
    "    n_ctx = 4\n",
    "    ctx_init = \"\"\n",
    "    class_token_position = \"end\"\n",
    "    csc = False\n",
    "    coop_net = model_class(\n",
    "        classnames=classnames,\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        csc=csc,\n",
    "    ).to(DEVICE)\n",
    "    return coop_net\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"./working_directory/model.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.text_projection\n",
    "        )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b24dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic mechanics are taken from the Lab Number 3 of AY 24/25\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(\n",
    "                clip_model.token_embedding.weight.device\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx_init_state = ctx_vectors.detach().clone()\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        print(classnames)\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        # print(\"+++\")\n",
    "        # print(\"Prompts:\")\n",
    "        # for p in prompts:\n",
    "        #     print(p)\n",
    "        # print(\"+++\")\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(\n",
    "            clip_model.token_embedding.weight.device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "        self.ctx_checkpoint = ctx_vectors.detach().clone()\n",
    "\n",
    "    def reset_ctx(self):  # https://discuss.pytorch.org/t/reset-model-weights/19180\n",
    "        with torch.no_grad():\n",
    "            self.ctx.copy_(self.ctx_checkpoint)\n",
    "\n",
    "        self.ctx.requires_grad = True\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        with torch.no_grad():\n",
    "            self.ctx_checkpoint.copy_(self.ctx)\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,  # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,  # (1, name_len, dim)\n",
    "                        ctx_i,  # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model: clip.model.CLIP\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model\n",
    "\n",
    "        self.prompt_learner = PromptLearner(\n",
    "            clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc\n",
    "        )\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def reset_ctx(self):\n",
    "        self.prompt_learner.reset_ctx()\n",
    "\n",
    "    def set_ctx_checkpoint(self):\n",
    "        self.prompt_learner.set_ctx_checkpoint()\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    net: OurCLIP,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    cost_function,\n",
    "    device=DEVICE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training step (for CoOp).\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        \"\"\" print(f\"input shape {inputs.shape}\")\n",
    "        print(f\"input type {inputs.dtype}\")\n",
    "        print(f\"labels{targets}\") \"\"\"\n",
    "\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        # max() returns (maximum_value, index_of_maximum_value)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        del inputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(inputs)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coop(\n",
    "    net,\n",
    "    dataset_splits,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "    skip_test=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    @param: dataset_class\n",
    "    @param: dataset_splits tuple that contains (training, validation, test)\"\"\"\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        dataset_splits[0],\n",
    "        dataset_splits[1],\n",
    "        dataset_splits[2],\n",
    "        transform=basic_image_transformations,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(\n",
    "        f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\"\n",
    "    )\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [{\"params\": net.parameters()}],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "    )\n",
    "\n",
    "    # Define the cost function\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Computes evaluation results before training\n",
    "    if not skip_test:\n",
    "        print(\"Before training:\")\n",
    "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(\n",
    "            net, train_loader, optimizer, cost_function\n",
    "        )\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    if not skip_test:\n",
    "        print(\"After training:\")\n",
    "\n",
    "        train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "        test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "        log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "        log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "        print(\n",
    "            f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\"\n",
    "        )\n",
    "        print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "\n",
    "    # Closes the logger\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_v2)\n",
    "splitted_datasets = get_dataset_split(dataset_v2)\n",
    "main_coop(coop_net, splitted_datasets, batch_size=16, skip_test=True)\n",
    "torch.save(coop_net.state_dict(), \"./working_directory/model_coop.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95816e54",
   "metadata": {},
   "source": [
    "# 3. Reproducing TPT\n",
    "\n",
    "We are always using OpenAI weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4942a35",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "As in the paper: random crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ed05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmix_transform():\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.4, 0.4, 0.4),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Basic original transform (non-augmented)\n",
    "original_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Wrapper Dataset that takes a base dataset + index of the sample to augment\n",
    "class AugmentSingleSampleDataset(Dataset):\n",
    "    def __init__(self, base_dataset, sample_idx, num_augments=63):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sample_idx = sample_idx\n",
    "        self.num_augments = num_augments\n",
    "        self.augmix_transform = get_augmix_transform()\n",
    "        self.original_transform = original_transform\n",
    "\n",
    "        # Extract the image once to avoid loading it 64 times\n",
    "        image, label = self.base_dataset[self.sample_idx]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            self.image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            self.image = image\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_augments + 1  # 63 augments + 1 original\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            image = self.original_transform(self.image)\n",
    "        else:\n",
    "            image = self.augmix_transform(self.image)\n",
    "        return image, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a76f89",
   "metadata": {},
   "source": [
    "### TPT Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52835f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_confident_samples(logits, top_p):\n",
    "    \"\"\"\n",
    "    Select the p-percentile of samples with lowest entropy, i.e. highest confidence.\n",
    "    \"\"\"\n",
    "    assert 0 <= top_p < 1, \"The value must be between 0 and 1\"\n",
    "    batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "    idx = torch.argsort(batch_entropy, descending=False)[\n",
    "        : int(batch_entropy.size()[0] * top_p)\n",
    "    ]\n",
    "    return logits[idx], idx\n",
    "\n",
    "\n",
    "def compute_avg_entropy(outputs):\n",
    "    \"\"\"\n",
    "    Compute marginal entropy of samples and return the average.\n",
    "    \"\"\"\n",
    "    # Calculate probabilities from logits\n",
    "    probs = outputs.softmax(dim=1)\n",
    "    # To avoid log(0), clamp probabilities to a minimum value\n",
    "    probs = probs.clamp(min=1e-9)\n",
    "    entropy = -(probs * probs.log()).sum(dim=1)\n",
    "    return entropy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_tpt(\n",
    "    net, dataset, optimizer, optimizer_state_dict, log_writer, num_aug=63, batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    @param net takes a OurClip model type\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(\n",
    "        range(len(dataset)),\n",
    "        desc=\"TPT_testing\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "        total=len(dataset),\n",
    "    )\n",
    "    # Iterate over the indices of the test set\n",
    "    try:\n",
    "        for sample_idx in pbar:  # Iterate through indices\n",
    "            net.reset_ctx()\n",
    "            optimizer.load_state_dict(optimizer_state_dict)\n",
    "            # print(f\"after Reset{torch.cuda.mem_get_info()}\")\n",
    "\n",
    "            # Create augmented dataset for the current sample\n",
    "            aug_data = AugmentSingleSampleDataset(\n",
    "                dataset, sample_idx, num_augments=num_aug\n",
    "            )  # Pass the base dataset and index\n",
    "\n",
    "            # Create a DataLoader for the augmented samples of this single image\n",
    "            aug_dataloader = torch.utils.data.DataLoader(\n",
    "                aug_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                worker_init_fn=seed_worker,\n",
    "                generator=g,\n",
    "            )\n",
    "\n",
    "            # Process the augmented images for this sample\n",
    "            all_outputs = []\n",
    "            for images, labels in aug_dataloader:\n",
    "                try:\n",
    "                    with autocast():\n",
    "                        # print(f\"size batch {len(images)}\")\n",
    "                        images = images.to(DEVICE)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        outputs = net(images)  # Use the provided net\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "                        # cpu_outputs = outputs.to(\"cpu\")\n",
    "                        all_outputs.append(outputs)\n",
    "                        # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "                        \"\"\" del images\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        print(torch.cuda.mem_get_info(), LINE()) \"\"\"\n",
    "\n",
    "                except:\n",
    "                    torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n",
    "                    raise\n",
    "\n",
    "            # Get the original label for this sample\n",
    "            original_image, target = dataset[sample_idx]\n",
    "            original_image = original_image.unsqueeze(0).to(DEVICE)\n",
    "            # print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "            # Make target a tensor and move to device\n",
    "            target = torch.tensor([target]).to(DEVICE)\n",
    "\n",
    "            # Concatenate outputs from all batches for this sample\n",
    "            all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "            # Select confident samples and compute average entropy\n",
    "            top_outputs, _ = select_confident_samples(all_outputs, 0.2)\n",
    "            loss = compute_avg_entropy(top_outputs)\n",
    "            # Loss computation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with autocast():\n",
    "                outputs = net(original_image)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += original_image.shape[0]\n",
    "            # Note: the .item() is needed to extract scalars from tensors\n",
    "            cumulative_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(target).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "    except:\n",
    "        raise\n",
    "    finally:\n",
    "        del all_outputs\n",
    "        del aug_data\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpt_test(net, dataset: Dataset, run_name=\"tpt1\", num_aug=63, batch_size=64):\n",
    "\n",
    "    # Create a logger for the experiment\n",
    "    log_writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    net.set_ctx_checkpoint()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "        # print(f\"{name}is in {param.requires_grad}\")\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = get_optimizer(net, 0.002, 0.0005, 0.9)\n",
    "    # , weight_decay=wd, momentum=momentum)\n",
    "    optimizer = torch.optim.AdamW([{\"params\": net.parameters()}], lr=0.005)\n",
    "    optimizer_state_dict = deepcopy(optimizer.state_dict())\n",
    "    print(torch.cuda.mem_get_info(), LINE())\n",
    "\n",
    "    print(\"Test tpt:\")\n",
    "    test_loss, test_accuracy = test_step_tpt(\n",
    "        net,\n",
    "        dataset,\n",
    "        optimizer,\n",
    "        optimizer_state_dict,\n",
    "        log_writer,\n",
    "        num_aug=num_aug,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Closes the logger\n",
    "    log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75daade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_net = new_model(OurCLIP, dataset_a)\n",
    "coop_net = load_model(coop_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt_test(coop_net, dataset_a, batch_size=64, num_aug=63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23602fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clear memory. (delete everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bfde4",
   "metadata": {},
   "source": [
    "## Simpler TPT with OpenCLIP\n",
    "\n",
    "TPT + \"one-shot\" Coop inspiration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc64a5d",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader\n",
    "\n",
    "Why another one? eheh we didn't communicate enough. They pratically do the same thing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for the ImageNet-A dataset.\n",
    "\n",
    "    Set the `transform` parameter so that images work with your model.\n",
    "    Example usage:\n",
    "    ```python\n",
    "        model, transform = clip.load(\"ViT-B/32\")\n",
    "        dataset = ImageNetADataset(<path>, transform=transform)\n",
    "    ```\n",
    "    ----\n",
    "\n",
    "    The dataset is organized into subdirectories, each named with a class code (e.g., \"n01614925\").\n",
    "    Each subdirectory contains images belonging to that class. The dataset also includes a README.txt file that maps class codes to human-readable names.\n",
    "\n",
    "    The dataset is expected to be structured as follows:\n",
    "    ```\n",
    "    datasets/imagenet-a/\n",
    "        n01440764/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        n01614925/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "            ...\n",
    "        ...\n",
    "        README.txt\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir=\"datasets/imagenet-a\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.__download_if_needed()\n",
    "\n",
    "        # Load mapping from class codes (e.g., \"n01614925\") to human-readable names\n",
    "        readme_path = os.path.join(root_dir, \"README.txt\")\n",
    "        self.class_code_to_label = self._load_class_mapping(readme_path)\n",
    "\n",
    "        # Filter valid class directories that match the mapping\n",
    "        self.class_codes = sorted(\n",
    "            [\n",
    "                d\n",
    "                for d in os.listdir(root_dir)\n",
    "                if os.path.isdir(os.path.join(root_dir, d))\n",
    "                and d in self.class_code_to_label\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Map class codes to indices\n",
    "        self.class_code_to_idx = {\n",
    "            code: idx for idx, code in enumerate(self.class_codes)\n",
    "        }\n",
    "\n",
    "        # Collect all image file paths and corresponding labels\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "        # Inverse mapping from label index to class name\n",
    "        self.idx_to_label = {\n",
    "            idx: self.class_code_to_label[code]\n",
    "            for code, idx in self.class_code_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def __download_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if the dataset is already downloaded. If not, download it.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found at {self.root_dir}. Please download it first.\"\n",
    "            )\n",
    "\n",
    "    def _load_class_mapping(self, readme_path):\n",
    "        \"\"\"\n",
    "        Load class code to human-readable name mapping from README.txt.\n",
    "        Skips header lines and parses lines in format: 'n01440764 tench'.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        with open(readme_path, \"r\") as file:\n",
    "            lines = file.readlines()[12:]  # Skip first 12 header lines\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    code, name = parts\n",
    "                    mapping[code] = name\n",
    "        return mapping\n",
    "\n",
    "    def _gather_samples(self):\n",
    "        \"\"\"\n",
    "        Walk through each class directory to gather image paths and corresponding labels.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for class_code in self.class_codes:\n",
    "            class_dir = os.path.join(self.root_dir, class_code)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    image_path = os.path.join(class_dir, filename)\n",
    "                    label = self.class_code_to_idx[class_code]\n",
    "                    samples.append((image_path, label))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load image and return dictionary containing image, label index, and class name.\n",
    "\n",
    "        Returns:\n",
    "            image (tensor)\n",
    "            label (tensor)\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path).float() / 255.0\n",
    "\n",
    "        if image.shape[0] == 1:  # Grayscale → RGB\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        elif image.shape[0] == 4:  # RGBA → RGB\n",
    "            image = image[:3, :, :]\n",
    "\n",
    "        elif image.shape[0] != 3:\n",
    "            raise ValueError(f\"Unsupported number of channels: {image.shape[0]}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"\n",
    "        Get human-readable class name for a given index.\n",
    "        \"\"\"\n",
    "        return self.idx_to_label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92b5f3",
   "metadata": {},
   "source": [
    "### Faster \"AugMix\"\n",
    "\n",
    "TODO: add motivation and benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugMixKornia(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        severity: int = 3,\n",
    "        width: int = 3,\n",
    "        depth: int = -1,\n",
    "        alpha: float = 1.0,\n",
    "        mixture_width: int = 3,\n",
    "        chain_depth: int = 3,\n",
    "        all_ops: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        AugMix implementation using Kornia with closer fidelity to the original paper.\n",
    "\n",
    "        Args:\n",
    "            severity: Severity level of augmentations (1-10)\n",
    "            width: Width of augmentation chain (not used directly, kept for compatibility)\n",
    "            depth: Depth of augmentation chain (-1 for random between 1-3)\n",
    "            alpha: Dirichlet distribution parameter for mixing weights\n",
    "            mixture_width: Number of augmentation chains to mix\n",
    "            chain_depth: Number of operations in each chain\n",
    "            all_ops: Whether to use all augmentation operations\n",
    "            device: Device to run on (cuda/cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.severity = severity\n",
    "        self.alpha = alpha\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth if depth <= 0 else depth\n",
    "        self.all_ops = all_ops\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define augmentation operations\n",
    "        self.augmentations = self._get_augmentations()\n",
    "\n",
    "    def _get_augmentations(self) -> List[nn.Module]:\n",
    "        \"\"\"Create a list of augmentation operations that will be randomly applied\"\"\"\n",
    "        severity_factor = self.severity / 10.0\n",
    "\n",
    "        if self.all_ops:\n",
    "            # Full set of augmentations similar to original AugMix\n",
    "            return [\n",
    "                # AutoContrast\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                # Equalize\n",
    "                Ke.equalize,\n",
    "                # Posterize\n",
    "                K.RandomPosterize(bits=max(1, 8 - self.severity), p=1.0),\n",
    "                # Rotate\n",
    "                K.RandomRotation(\n",
    "                    degrees=(-30 * severity_factor, 30 * severity_factor), p=1.0\n",
    "                ),\n",
    "                # Solarize\n",
    "                K.RandomSolarize(\n",
    "                    thresholds=0.5, additions=(0.0, 0.1 * self.severity), p=1.0\n",
    "                ),\n",
    "                # Shear\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    shear=(-15 * severity_factor, 15 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # Translate\n",
    "                K.RandomAffine(\n",
    "                    degrees=0,\n",
    "                    translate=(0.1 * severity_factor, 0.1 * severity_factor),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                # ColorJitter\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity,\n",
    "                    contrast=0.1 * self.severity,\n",
    "                    saturation=0.1 * self.severity,\n",
    "                    hue=0.1,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        else:\n",
    "            # Simplified version\n",
    "            return [\n",
    "                K.ColorJitter(\n",
    "                    brightness=0.1 * self.severity, contrast=0.1 * self.severity, p=1.0\n",
    "                ),\n",
    "                Ke.equalize,\n",
    "                K.RandomAffine(\n",
    "                    degrees=(-15 * severity_factor, 15 * severity_factor), p=1.0\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "    def _apply_augmentation_chain(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a random sequence of augmentations to an image.\n",
    "\n",
    "        Args:\n",
    "            image: Input image tensor (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented image tensor (C, H, W)\n",
    "        \"\"\"\n",
    "        # Randomly select augmentations for this chain\n",
    "        op_indices = np.random.choice(\n",
    "            len(self.augmentations), size=self.chain_depth, replace=True\n",
    "        )\n",
    "\n",
    "        augmented = image  # Don't clone immediately\n",
    "        for op_idx in op_indices:\n",
    "            augmented = self.augmentations[op_idx](augmented)\n",
    "\n",
    "        return augmented.squeeze(0)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply AugMix to a batch of images.\n",
    "\n",
    "        Args:\n",
    "            images: Input batch of images (B, C, H, W) or (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Augmented batch (same shape as input)\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            # Input validation\n",
    "            if not isinstance(images, torch.Tensor):\n",
    "                images = K.image_to_tensor(images)\n",
    "\n",
    "            if images.dim() == 3:\n",
    "                images = images.unsqueeze(0)\n",
    "\n",
    "            # Move to device if needed\n",
    "            if images.device != self.device:\n",
    "                images = images.to(self.device)\n",
    "\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            # Sample mixing weights from Dirichlet distribution\n",
    "            weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet(\n",
    "                        [self.alpha] * self.mixture_width, size=batch_size\n",
    "                    )\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, mixture_width)\n",
    "\n",
    "            # Sample weights for mixing with original\n",
    "            mix_weights = (\n",
    "                torch.from_numpy(\n",
    "                    np.random.dirichlet([self.alpha, self.alpha], size=batch_size)\n",
    "                )\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )  # Shape (B, 2)\n",
    "\n",
    "            # Generate augmented versions for each mixture component\n",
    "            # Pre-allocate memory for augmented versions\n",
    "            augmented = torch.empty(\n",
    "                (self.mixture_width, batch_size, *images.shape[1:]), device=self.device\n",
    "            )\n",
    "\n",
    "            for i in range(self.mixture_width):\n",
    "                augmented[i] = self._apply_augmentation_chain(images)\n",
    "\n",
    "            # Weighted sum of augmented versions\n",
    "            mixed = torch.einsum(\"mbchw,bm->bchw\", augmented, weights).to(self.device)\n",
    "\n",
    "            # Final mix with original image\n",
    "            result = (\n",
    "                mix_weights[:, 0:1, None, None] * images\n",
    "                + mix_weights[:, 1:2, None, None] * mixed\n",
    "            )\n",
    "\n",
    "            result = result.squeeze(0) if result.shape[0] == 1 else result\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def kornia_random_crop(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies random crop to a batch of images using Kornia's RandomResizedCrop.\n",
    "    Preserves the original image size while randomly cropping a portion.\n",
    "    \"\"\"\n",
    "    b, c, h, w = images.shape\n",
    "\n",
    "    # Create random crop transform that:\n",
    "    # 1. Crops between 50% and 100% of original area\n",
    "    # 2. Maintains original aspect ratio\n",
    "    # 3. Resizes back to original dimensions\n",
    "    transform = K.RandomResizedCrop(\n",
    "        size=(h, w),\n",
    "        # scale=(0.5, 1.0),  # Crop between 50% and 100% of original area\n",
    "        # ratio=(1.0, 1.0),  # Maintain original aspect ratio\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "        same_on_batch=False,  # Different crop for each image in batch\n",
    "    )\n",
    "\n",
    "    return transform(images)\n",
    "\n",
    "\n",
    "kornia_preprocess = nn.Sequential(\n",
    "    K.SmallestMaxSize(\n",
    "        224,\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    K.CenterCrop(\n",
    "        size=(224, 224),\n",
    "        resample=kornia.constants.Resample.BICUBIC,\n",
    "    ),\n",
    "    kornia.enhance.Normalize(\n",
    "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]),\n",
    "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_transform, custom_transform=None, n_views=63, device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_transform = model_transform\n",
    "        self.custom_transform = custom_transform\n",
    "        self.n_views = n_views\n",
    "        self.device = device\n",
    "\n",
    "        self.eval()\n",
    "        # self.model_transform.eval()\n",
    "        # self.custom_transform.eval() if custom_transform is not None else None\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model transform and custom transform to the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            image = image.to(self.device)\n",
    "\n",
    "            if self.custom_transform is not None:\n",
    "                with torch.no_grad():\n",
    "                    views = torch.empty(\n",
    "                        (self.n_views + 1, *image.shape), device=self.device\n",
    "                    )\n",
    "                    views[:-1] = self.custom_transform(\n",
    "                        image.repeat(self.n_views, 1, 1, 1)\n",
    "                    )\n",
    "                    views[-1] = image\n",
    "                    return self.model_transform(views)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    return self.model_transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9910555",
   "metadata": {},
   "source": [
    "### Image-A Builder\n",
    "\n",
    "Build the dataset and dataloader with image-agumentation at data-loading time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImagenetA(\n",
    "    augmenter: ImageTransform,\n",
    "    root_dir=\"datasets/imagenet-a\",\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the ImageNet-A dataset. Defaults to 1 element per batch.\n",
    "    Non modifiable. No shuffling.\n",
    "    Args:\n",
    "        augmenter (callable):\n",
    "        root_dir (str): Root directory of the ImageNet-A dataset.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (DataLoader): DataLoader for the ImageNet-A dataset.\n",
    "        dataset (ImageNetADataset): The underlying dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle the batch of images and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        images = batch[0][0]\n",
    "\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(0)\n",
    "\n",
    "        labels = batch[0][1]\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    dataset = ImageNetADataset(root_dir=root_dir, transform=augmenter)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bdaf1",
   "metadata": {},
   "source": [
    "### Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str,\n",
    "    comment: str,\n",
    "    reduce: Optional[int | None] = None,\n",
    "    visualize: Optional[bool] = False,\n",
    "):\n",
    "    \"\"\"Benchmark the model on the dataset.\n",
    "\n",
    "    The model must return logits.\n",
    "    \"\"\"\n",
    "\n",
    "    board = SummaryWriter(comment=comment)\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    times = []\n",
    "\n",
    "    total_tqdm = reduce if reduce is not None else len(dataloader)\n",
    "    # ░▒█\n",
    "    # ascii=\" ▖▘▝▗▚▞█\"\n",
    "    # ascii=' >='\n",
    "    start_event, end_event = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n",
    "        enable_timing=True\n",
    "    )\n",
    "    for image, label in tqdm(dataloader, total=total_tqdm, ascii=\" ▖▘▝▗▚▞█\"):\n",
    "        image = image.to(device)\n",
    "\n",
    "        start_event.record()\n",
    "        pred_class = model(image)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        times.append(start_event.elapsed_time(end_event))\n",
    "\n",
    "        # del image\n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total += 1\n",
    "        correct += int((pred_class == label))\n",
    "\n",
    "        if reduce:\n",
    "            if total > reduce:\n",
    "                break\n",
    "\n",
    "        # break\n",
    "        board.add_scalar(\"accuracy\", correct / total, total)\n",
    "        board.add_scalar(\"dbg/label/predict_class\", pred_class, total)\n",
    "        board.add_scalar(\"dbg/label/label\", label, total)\n",
    "\n",
    "        running_accuracy = correct / total\n",
    "\n",
    "        if visualize:\n",
    "            print(f\"[{label} || {pred_class}] | Acc: [{running_accuracy*100:.2f}%]\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    latency = (np.array(times).sum() / total).item()  # ms\n",
    "\n",
    "    board.add_scalar(\"metrics/latency (ms)\", latency)\n",
    "    board.add_scalar(\"metrics/accuracy\", accuracy)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Latency: {latency:.2f} ms\")\n",
    "\n",
    "    return accuracy, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7683b",
   "metadata": {},
   "source": [
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: from 5_tpt.py\n",
    "@dataclass(frozen=True)\n",
    "class CLIPModels:\n",
    "    ViTB32: str = \"ViT-B/32\"\n",
    "\n",
    "\n",
    "class TPTPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        clip_model: open_clip.model.CLIP,\n",
    "        arch: CLIPModels = CLIPModels.ViTB32,\n",
    "        base_prompt: str = \"a photo of a [CLS]\",\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer(arch)\n",
    "\n",
    "        self.__init_ctx_from_prompt(\n",
    "            tokenizer=tokenizer,\n",
    "            token_embedding=clip_model.token_embedding,\n",
    "            base_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    def __init_ctx_from_prompt(\n",
    "        self, tokenizer, token_embedding, base_prompt: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the context tokens from the base prompt.\n",
    "\n",
    "        We need to make sure that the CLS token is NOT \"exploded\" in the prompt.\n",
    "\n",
    "        The idea is to have prompts tuned without having to manually manage where the CLS token is.\n",
    "\n",
    "        To do this we need to keep the CLS token position in the prompt, and update it accordingly\n",
    "        when needed.\n",
    "\n",
    "        I'm splitting the prompt into prefix and suffix, using [CLS] as a separator.\n",
    "        They are trained as two different parameters, and then concatenated together.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the base prompt into prefix and suffix\n",
    "        promt_prefix = base_prompt.split(\"[CLS]\")[0]\n",
    "        promt_suffix = base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # \"Clean\" PAD, SOT and EOT tokens\n",
    "        c_token_sot = torch.tensor([[tokenizer.sot_token_id]]).to(self.device)\n",
    "        c_token_eot = torch.tensor([[tokenizer.eot_token_id]]).to(self.device)\n",
    "        c_token_pad = torch.tensor([[0]]).to(self.device)  # PAD\n",
    "\n",
    "        # Tokenize prefix, suffix and class names\n",
    "        tokenized_prefix = tokenizer(promt_prefix).to(self.device)\n",
    "        tokenized_suffix = tokenizer(promt_suffix).to(self.device)\n",
    "\n",
    "        # remove PAD, SOT and EOT tokens\n",
    "        # Extract \"clean\" tokens\n",
    "        c_tokenized_prefix = tokenized_prefix[\n",
    "            (tokenized_prefix != c_token_sot)\n",
    "            & (tokenized_prefix != c_token_eot)\n",
    "            & (tokenized_prefix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "        c_tokenized_suffix = tokenized_suffix[\n",
    "            (tokenized_suffix != c_token_sot)\n",
    "            & (tokenized_suffix != c_token_eot)\n",
    "            & (tokenized_suffix != c_token_pad)\n",
    "        ].to(self.device)\n",
    "\n",
    "        tokenized_class_names = tokenizer(self.class_names).to(self.device)\n",
    "\n",
    "        # BASE full prompt\n",
    "        # [CLS] + prefix + class_name + suffix + EOT\n",
    "        # pre-computed as it's used for all classes and images :)\n",
    "        self.tokenized_initial_full_prompt = tokenizer(\n",
    "            [base_prompt.replace(\"[CLS]\", c) for c in self.class_names]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get base embeddings\n",
    "        with torch.no_grad():\n",
    "            self.embedded_sot = token_embedding(c_token_sot)\n",
    "            self.embedded_eot = token_embedding(c_token_eot)\n",
    "            self.embedded_pad = token_embedding(c_token_pad)\n",
    "            self.embedded_prefix = token_embedding(c_tokenized_prefix)\n",
    "            self.embedded_suffix = token_embedding(c_tokenized_suffix)\n",
    "            embedded_class_names = token_embedding(tokenized_class_names)\n",
    "            self.embedded_max_len = embedded_class_names.shape[1]\n",
    "\n",
    "        # Setup clean embedded_class_names (list)\n",
    "        # Mask to filter out SOT/EOT/PAD tokens (shape [200, 77])\n",
    "        mask = (\n",
    "            (tokenized_class_names != c_token_sot)\n",
    "            & (tokenized_class_names != c_token_eot)\n",
    "            & (tokenized_class_names != c_token_pad)\n",
    "        )\n",
    "\n",
    "        # Apply mask to embeddings (for each class)\n",
    "        clean_embeddings = []\n",
    "        for i in range(embedded_class_names.shape[0]):\n",
    "            # masked_select would flatten, so we use boolean indexing\n",
    "            clean_embed = embedded_class_names[i][mask[i]]  # [num_valid_tokens, 512]\n",
    "            clean_embeddings.append(\n",
    "                clean_embed.unsqueeze(0)\n",
    "            )  # [1, num_valid_tokens, 512]\n",
    "\n",
    "        self.embedded_class_names = clean_embeddings\n",
    "\n",
    "        for i, embed in enumerate(clean_embeddings):\n",
    "            self.register_buffer(f\"class_embed_{i}\", embed)\n",
    "\n",
    "        # Create \"init\" states and set learnable parameters\n",
    "        self.init_state_prefix = self.embedded_prefix.detach().clone()\n",
    "        self.init_state_suffix = self.embedded_suffix.detach().clone()\n",
    "        self.embedded_prefix = nn.Parameter(self.embedded_prefix)\n",
    "        self.embedded_suffix = nn.Parameter(self.embedded_suffix)\n",
    "        self.register_parameter(\"embedded_prefix\", self.embedded_prefix)\n",
    "        self.register_parameter(\"embedded_suffix\", self.embedded_suffix)\n",
    "\n",
    "    def forward(self) -> torch.Tensor:\n",
    "        prompts = []\n",
    "        for i in range(len(self.class_names)):\n",
    "\n",
    "            # embeddeD_max_len: 77\n",
    "            # embedded_prefix: torch.Size([4, 512])\n",
    "            # embedded_class_names: torch.Size([1, 1, 512])\n",
    "            # embedded_suffix: torch.Size([0, 512]\n",
    "\n",
    "            padding_size = (\n",
    "                self.embedded_max_len\n",
    "                - self.embedded_prefix.shape[0]\n",
    "                - getattr(self, f\"class_embed_{i}\").shape[1]\n",
    "                - self.embedded_suffix.shape[0]\n",
    "            ) - 2  # # -2 for SOT and EOT\n",
    "\n",
    "            ## embedded sot shape: torch.Size([1, 1, 512])\n",
    "            ## embedded prefix shape: torch.Size([1, 4, 512])\n",
    "            ## embedded class names shape: torch.Size([1, 1, 1, 512])\n",
    "            ## embedded suffix shape: torch.Size([1, 0, 512])\n",
    "            ## embedded eot shape: torch.Size([1, 1, 512])\n",
    "            ## effective padding shape: torch.Size([1, 70, 512])\n",
    "            ## Prompt shape: torch.Size([1, 77, 512])\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.embedded_sot,\n",
    "                    self.embedded_prefix.unsqueeze(0),\n",
    "                    # self.embedded_class_names[i],\n",
    "                    getattr(self, f\"class_embed_{i}\"),\n",
    "                    self.embedded_suffix.unsqueeze(0),\n",
    "                    self.embedded_eot,\n",
    "                    self.embedded_pad.repeat(1, padding_size, 1),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "        # Must have shape torch.Size([200, 77, 512]) (classes, feature1, feature2)\n",
    "        return prompts\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        # TODO: check, doin without `data`\n",
    "\n",
    "        # self.embedded_prefix.data.copy_(self.init_state_prefix)\n",
    "        # self.embedded_suffix.data.copy_(self.init_state_suffix)\n",
    "        with torch.no_grad():\n",
    "            self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "            self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "\n",
    "class TPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names: List[str],\n",
    "        arch: CLIPModels,\n",
    "        pretrained: str,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        clip_model: open_clip.model.CLIP\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "            model_name=arch,\n",
    "            pretrained=pretrained,\n",
    "            device=device,\n",
    "            force_quick_gelu=True,\n",
    "        )\n",
    "\n",
    "        self.model = clip_model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(arch)\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.visual: open_clip.transformer.VisionTransformer = clip_model.visual\n",
    "        self.visual.eval()\n",
    "\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.attn_mask = clip_model.attn_mask\n",
    "        self.text_pool_type = clip_model.text_pool_type\n",
    "\n",
    "        for _, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.prompt_learner = TPTPromptLearner(\n",
    "            arch=arch, class_names=class_names, clip_model=clip_model\n",
    "        )\n",
    "\n",
    "    def _pool(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.visual.attn_pool is not None:\n",
    "            if self.visual.attn_pool_contrastive is not None:\n",
    "                # This is untested, WIP pooling that should match paper\n",
    "                x = self.visual.ln_post(\n",
    "                    x\n",
    "                )  # TBD LN first or separate one after each pool?\n",
    "                tokens = self.visual.attn_pool(x)\n",
    "                if self.visual.attn_pool_type == \"parallel\":\n",
    "                    pooled = self.visual.attn_pool_contrastive(x)\n",
    "                else:\n",
    "                    assert self.visual.attn_pool_type == \"cascade\"\n",
    "                    pooled = self.visual.attn_pool_contrastive(tokens)\n",
    "            else:\n",
    "                # this is the original OpenCLIP CoCa setup, does not match paper\n",
    "                x = self.visual.attn_pool(x)\n",
    "                x = self.visual.ln_post(x)\n",
    "                pooled, tokens = self.visual._global_pool(x)\n",
    "        elif self.visual.final_ln_after_pool:\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "            pooled = self.visual.ln_post(pooled)\n",
    "        else:\n",
    "            x = self.visual.ln_post(x)\n",
    "            pooled, tokens = self.visual._global_pool(x)\n",
    "\n",
    "        return pooled, tokens, x\n",
    "\n",
    "    def _forward_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.visual._embeds(x)\n",
    "        x = self.visual.transformer(x)\n",
    "\n",
    "        pooled, tokens, x = self._pool(x)\n",
    "\n",
    "        if self.visual.proj is not None:\n",
    "            pooled = pooled @ self.visual.proj\n",
    "        if self.visual.output_tokens:\n",
    "            return pooled, tokens, x\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def __encode_image(\n",
    "        self, image, normalize: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pooled_pre_norm = self._forward_image(image)\n",
    "        return F.normalize(pooled_pre_norm, dim=-1) if normalize else pooled_pre_norm\n",
    "\n",
    "    def __encode_text(self, text=None, normalize: bool = False):\n",
    "        cast_dtype = self.transformer.get_cast_dtype()\n",
    "\n",
    "        x = self.prompt_learner().to(cast_dtype)\n",
    "\n",
    "        text = self.prompt_learner.tokenized_initial_full_prompt\n",
    "\n",
    "        x = x + self.positional_embedding.to(cast_dtype)\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        x = text_global_pool(x, text, self.text_pool_type)\n",
    "        if self.text_projection is not None:\n",
    "            if isinstance(self.text_projection, nn.Linear):\n",
    "                x = self.text_projection(x)\n",
    "            else:\n",
    "                x = x @ self.text_projection\n",
    "\n",
    "        return F.normalize(x, dim=-1) if normalize else x\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference function for the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images.\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Logits from the CLIP model.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.__encode_image(image, normalize=True)\n",
    "        text_features = self.__encode_text(normalize=True)\n",
    "\n",
    "        logit_scale = self.model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, image_features\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the prompt learner to its initial state.\n",
    "        \"\"\"\n",
    "        self.prompt_learner.reset()\n",
    "\n",
    "\n",
    "class TPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained: str,\n",
    "        arch: CLIPModels,\n",
    "        class_names: List[str],\n",
    "        tta_steps: int = 1,\n",
    "        lr: float = 0.0001,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "        model = TPTModel(\n",
    "            class_names=class_names,\n",
    "            arch=arch,\n",
    "            pretrained=pretrained,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # # # TEST - learnable layer norm\n",
    "        # self.model.visual.ln_post.requires_grad_(True)\n",
    "        # self.model.ln_final.requires_grad_(True)\n",
    "\n",
    "        # Get all trainable parameters (filter by requires_grad)\n",
    "        trainable_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        # Initialize optimizer with trainable parameters\n",
    "        self.optim = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.optim_init = deepcopy(self.optim.state_dict())\n",
    "\n",
    "        # Initialize backup lists\n",
    "        self.ln_backup = {\n",
    "            \"weights\": [],  # For gamma (scale)\n",
    "            \"biases\": [],  # For beta (shift)\n",
    "        }\n",
    "\n",
    "        # Backup all LN params in text encoder\n",
    "        for block in self.model.transformer.resblocks:\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_1.weight.data.detach().clone()\n",
    "            )  # gamma for ln_1\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_1.bias.data.detach().clone()\n",
    "            )  # beta for ln_1\n",
    "            self.ln_backup[\"weights\"].append(\n",
    "                block.ln_2.weight.data.detach().clone()\n",
    "            )  # gamma for ln_2\n",
    "            self.ln_backup[\"biases\"].append(\n",
    "                block.ln_2.bias.data.detach().clone()\n",
    "            )  # beta for ln_2\n",
    "\n",
    "        # Backup final LN\n",
    "        self.ln_backup[\"weights\"].append(\n",
    "            self.model.ln_final.weight.data.detach().clone()\n",
    "        )\n",
    "        self.ln_backup[\"biases\"].append(self.model.ln_final.bias.data.detach().clone())\n",
    "\n",
    "    def set_tta_steps(self, tta_steps: int) -> None:\n",
    "        \"\"\"\n",
    "        Set the number of TTA steps.\n",
    "\n",
    "        Args:\n",
    "            tta_steps (int): Number of TTA steps.\n",
    "        \"\"\"\n",
    "        self.tpt_steps = tta_steps\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        selected_idx = None\n",
    "\n",
    "        for step in range(self.tpt_steps):\n",
    "            with torch.cuda.amp.autocast():\n",
    "\n",
    "                logits, image_features = self.model(input)\n",
    "\n",
    "                # Select the most confident samples\n",
    "                if selected_idx is not None:\n",
    "                    logits = logits[selected_idx]\n",
    "                else:\n",
    "                    logits, selected_idx = self.__select_confident_samples(logits)\n",
    "\n",
    "                # Compute the average entropy loss\n",
    "                loss = self.__avg_entropy_loss(logits)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optim)\n",
    "            self.scaler.update()\n",
    "\n",
    "        # Actual inference\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "            # take only the last image of the input\n",
    "            # input = input[-1].unsqueeze(0)\n",
    "            logits, _ = self.model(input)\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = marginal_prob.argmax().item()\n",
    "\n",
    "        self.__reset()\n",
    "\n",
    "        return pred_class\n",
    "\n",
    "    def __select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def __avg_entropy_loss(self, outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model's outputs.\n",
    "        Args:\n",
    "            outputs (torch.Tensor): The model's outputs.\n",
    "        Returns:\n",
    "            torch.Tensor: The average entropy.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L46)\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def __reset(self) -> None:\n",
    "        \"\"\"Full reset of prompt learner and optimizer state\"\"\"\n",
    "        # 1. Reset prompt embeddings\n",
    "        for p in self.model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        self.model.reset()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     self.embedded_prefix.copy_(self.init_state_prefix)\n",
    "        #     self.embedded_suffix.copy_(self.init_state_suffix)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            idx = 0\n",
    "            # Reset LN params in text encoder\n",
    "            for block in self.model.transformer.resblocks:\n",
    "                block.ln_1.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_1.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "                block.ln_2.weight.data.copy_(self.ln_backup[\"weights\"][idx].clone())\n",
    "                block.ln_2.bias.data.copy_(self.ln_backup[\"biases\"][idx].clone())\n",
    "                idx += 1\n",
    "\n",
    "            # # Reset final LN\n",
    "            self.model.ln_final.weight.data.copy_(self.ln_backup[\"weights\"][-1].clone())\n",
    "            self.model.ln_final.bias.data.copy_(self.ln_backup[\"biases\"][-1].clone())\n",
    "\n",
    "        # # 2. Reset optimizer state\n",
    "        self.optim.load_state_dict(deepcopy(self.optim_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e0463",
   "metadata": {},
   "source": [
    "### Running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f25939",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_random_crop,\n",
    "    n_views=63,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "dataloader, dataset = ImagenetA(augmenter, num_workers=5)\n",
    "\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=DEVICE,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()\n",
    "\n",
    "wrapper_clip = TPT(\n",
    "    arch=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    class_names=dataset.class_code_to_label.values(),\n",
    "    tta_steps=1,\n",
    "    lr=5e-3,\n",
    ")\n",
    "\n",
    "bench(wrapper_clip, dataloader, DEVICE, reduce=None, comment=\"tpt\", visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6daa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: zero-shot CLIP (openclip and clip) code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb02ea",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "TODO: compare TPT (clip) with TPT (openclip). Show that the difference is minimal.\n",
    "\n",
    "TODO: compare clip and openclip on zero-shot classification.\n",
    "\n",
    "OBV everything in the same table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffa1bf",
   "metadata": {},
   "source": [
    "# 4. Trying to get a better at TTA (our contribution)\n",
    "\n",
    "- augmix (as a note criticizing TPT's paper.)\n",
    "- augment top 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79099c",
   "metadata": {},
   "source": [
    "$\\delta = \\Alpha + 3* \\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981b2a7",
   "metadata": {},
   "source": [
    "## A. Augment Top 1 🚀\n",
    "\n",
    "desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae26cb0",
   "metadata": {},
   "source": [
    "## B. Self-Supervised Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe0528",
   "metadata": {},
   "source": [
    "## C. (stupid) Adaptive Layer Norm 😥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027b398",
   "metadata": {},
   "source": [
    "## C. TPT with Top 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efd950",
   "metadata": {},
   "source": [
    "## D. TNT (without Top 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f184e",
   "metadata": {},
   "source": [
    "## E. TNT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f2ffa",
   "metadata": {},
   "source": [
    "## F. TPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52100809",
   "metadata": {},
   "source": [
    "## G. FILM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726c201",
   "metadata": {},
   "source": [
    "# 5. Thoughts and Conclusion\n",
    "\n",
    "Pro e contro di ogni uno\n",
    "\n",
    "TODO: plot accuracy/latency.\n",
    "\n",
    "quindi che usare backprop non conviene (citare frustaingly easy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2731d6f",
   "metadata": {},
   "source": [
    "# 6. Future Work\n",
    "\n",
    "- ai based augmentation (trying to optimize e.g. random crops)\n",
    "- do something stupid like merge TPT + TNT + frustatingly easy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29860d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19369d9",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
