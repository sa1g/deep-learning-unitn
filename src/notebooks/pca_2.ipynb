{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64459876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.manual_seed(456)\n",
    "torch.cuda.manual_seed(456)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "\n",
    "\n",
    "from src.augmix import (\n",
    "    AugMixKornia,\n",
    "    ImageTransform,\n",
    "    kornia_preprocess,\n",
    "    kornia_random_crop,\n",
    ")\n",
    "from src.data import ImagenetA\n",
    "from src.utils import bench\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def plot_image_grid(images, title, ncols=4):\n",
    "    \"\"\"\n",
    "    Plot a grid of images.\n",
    "\n",
    "    Args:\n",
    "        images: Tensor or numpy array of shape (N, C, H, W) or (N, H, W, C)\n",
    "        title: Title for the entire figure\n",
    "        ncols: Number of columns in the grid\n",
    "    \"\"\"\n",
    "    # Convert to numpy and handle channel dimension\n",
    "    if not isinstance(images, np.ndarray):\n",
    "        images = images.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # Handle (N, C, H, W) -> (N, H, W, C) if needed\n",
    "    if images.shape[1] in [1, 3]:  # Assuming 1 or 3 channels\n",
    "        images = images.transpose(0, 2, 3, 1)\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    nrows = int(np.ceil(n_images / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 3 * nrows))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_images:\n",
    "            ax.imshow(images[i])\n",
    "            ax.set_title(f\"Image {i + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "augmenter = ImageTransform(\n",
    "    model_transform=kornia_preprocess,\n",
    "    custom_transform=kornia_random_crop,\n",
    "    n_views=63,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "dataloader, dataset = ImagenetA(augmenter, num_workers=6)\n",
    "\n",
    "# Load the CLIP model\n",
    "clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "    # model_name=\"ViT-B-32\", pretrained=\"datacomp_xl_s13b_b90k\", device=device#, force_quick_gelu=True\n",
    "    model_name=\"ViT-B-16\",\n",
    "    pretrained=\"openai\",\n",
    "    device=device,\n",
    "    force_quick_gelu=True,\n",
    ")\n",
    "clip_model.eval()  # type: ignore\n",
    "clip_model.visual.output_tokens = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        class_labels: dict,\n",
    "        prompt: str = \"a photo of a {}\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "        self.model: open_clip.model.CLIP = model\n",
    "        self.logit_scale = model.logit_scale.data.exp()\n",
    "        # self.logit_scale = model.log\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prompts = torch.cat(\n",
    "                [self.tokenizer(prompt.format(c)) for c in class_labels.values()]\n",
    "            ).to(device)\n",
    "            self.text_features = model.encode_text(prompts, normalize=True)\n",
    "\n",
    "    def select_confident_samples(\n",
    "        self, logits: torch.Tensor, top: float = 0.1\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects the top-k samples with the lowest entropy from the logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits from the model.\n",
    "            top (float): The fraction of samples to select.\n",
    "                For example, if top=0.1, it selects the top 10% of samples.\n",
    "        Returns:\n",
    "            torch.Tensor: The selected logits.\n",
    "            torch.Tensor: The indices of the selected samples.\n",
    "\n",
    "        [Reference](https://github.com/azshue/TPT/blob/63ecbace79694205d7884e63fdc3137a200f0b0e/tpt_classification.py#L41C5-L41C11)\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> int:\n",
    "        with torch.no_grad():  # , torch.autocast(\"cuda\"):\n",
    "            clip_model.visual.output_tokens = True  # type: ignore\n",
    "\n",
    "            # Patch embedding: (batch_size, num_patches + 1, embed_dim)\n",
    "            # on 224x224 images: [1, 50, 768]\n",
    "            image_features, tokens = self.model.visual(x)\n",
    "\n",
    "            image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "            filtered_logits = (\n",
    "                self.logit_scale * image_features[:-1:] @ self.text_features.T\n",
    "            )\n",
    "\n",
    "            selected_logits, idx = self.select_confident_samples(\n",
    "                logits=filtered_logits, top=0.1\n",
    "            )\n",
    "\n",
    "            # selected_tokens = torch.cat((tokens[idx], tokens[-1:]), dim=0) # semplice\n",
    "            selected_tokens = tokens[idx]  # coso_separato\n",
    "\n",
    "            # Calcualte PCA (n_components=1) of each of the selected tokens\n",
    "            # Assuming patch_embeddings is a tensor of shape [64, 50, 768]\n",
    "            batch_size, num_patches, feat_dim = selected_tokens.shape\n",
    "            PATCH_RES = int(num_patches**0.5)  # Assuming num_patches is a square number\n",
    "\n",
    "            # Center the data (subtract mean)\n",
    "            patch_embeddings_centered = selected_tokens - selected_tokens.mean(\n",
    "                dim=1, keepdim=True\n",
    "            )\n",
    "\n",
    "            U, S, V = torch.pca_lowrank(patch_embeddings_centered, q=1, center=False)\n",
    "\n",
    "            # Project the data (get the first principal component scores)\n",
    "            fg_scores = torch.matmul(\n",
    "                patch_embeddings_centered, V[:, :, -1:]\n",
    "            )  # [64, 50, 1]\n",
    "\n",
    "            # Min-max scaling per image\n",
    "            min_vals = fg_scores.min(dim=1, keepdim=True)[0]\n",
    "            max_vals = fg_scores.max(dim=1, keepdim=True)[0]\n",
    "            fg_scores_scaled = (fg_scores - min_vals) / (\n",
    "                max_vals - min_vals + 1e-8\n",
    "            )  # [64, 50, 1]\n",
    "\n",
    "            # Reshape to [64, PATCH_RES, PATCH_RES]\n",
    "            fg_scores_reshaped = fg_scores_scaled.view(batch_size, PATCH_RES, PATCH_RES)\n",
    "\n",
    "            # Create mask\n",
    "            positive_mask = fg_scores_reshaped > 0.6\n",
    "            # negative_mask = fg_scores_reshaped < 0.4\n",
    "\n",
    "            # print(\n",
    "            #     f\"mask shape: {positive_mask.shape}    fg_scores shape: {fg_scores.shape}\"\n",
    "            # )\n",
    "\n",
    "            # Apply mask on the input (x) on the specific indices\n",
    "            positive_masked = x[idx].clone()\n",
    "            # positive_masked = torch.cat((positive_masked, x[-1:]), dim=0) # Semplice\n",
    "            negative_masked = positive_masked.clone()\n",
    "\n",
    "            positive_mask = F.interpolate(\n",
    "                positive_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "                size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "                mode=\"nearest\",\n",
    "            )\n",
    "\n",
    "            # negative_mask = F.interpolate(\n",
    "            #     negative_mask.unsqueeze(1).float(),  # Add channel dimension\n",
    "            #     size=(x.shape[2], x.shape[3]),  # Resize to input image size\n",
    "            #     mode=\"nearest\",\n",
    "            # )\n",
    "\n",
    "            positive_masked = positive_masked * positive_mask  # .bool()\n",
    "            negative_masked = negative_masked * (1 - positive_mask)  # .bool()\n",
    "\n",
    "            # # visualize the original image\n",
    "            # plt.imshow(x[-1].cpu().numpy().transpose(1, 2, 0))\n",
    "            # plt.title(\"Original Image\")\n",
    "            # plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "\n",
    "            # # for i in range(positive_masked.shape[0]):\n",
    "            # #     img = positive_masked[i].cpu().numpy().transpose(1, 2, 0)\n",
    "            # #     plt.imshow(img)\n",
    "            # #     plt.title(f\"Masked Image {i + 1}\")\n",
    "            # #     plt.axis(\"off\")\n",
    "            # #     plt.show()\n",
    "\n",
    "            # plot_image_grid(positive_masked, \"Positive Masked Images\")\n",
    "            # plot_image_grid(negative_masked, \"Negative Masked Images\")\n",
    "            # plot_image_grid(torch.cat((x[idx], x[-1:]), dim=0), \"Original\")\n",
    "\n",
    "            # # Calculate the logits for the positive and negative masked images\n",
    "            clip_model.visual.output_tokens = False  # type: ignore\n",
    "\n",
    "            pos_logits = self.model.encode_image(positive_masked, normalize=True)\n",
    "            neg_logits = self.model.encode_image(negative_masked, normalize=True)\n",
    "            # orig_logits = F.normalize(selected_logits, dim=-1)\n",
    "\n",
    "            # # # pos_logits = torch.cat(\n",
    "            # # #     (pos_logits, image_features[-1:]), dim=0\n",
    "            # # # )  # coso_separato\n",
    "            # # # neg_logits = torch.cat(\n",
    "            # # #     (neg_logits, image_features[-1:]), dim=0\n",
    "            # # # )  # coso_separato\n",
    "\n",
    "            # # # # # Calculate the cosine similarity between the positive and negative logits\n",
    "            # # # pos_logits = pos_logits @ self.text_features.T * self.logit_scale\n",
    "            # # # neg_logits = neg_logits @ self.text_features.T * self.logit_scale\n",
    "            # # # # orig_logits = orig_logits @ self.text_features.T * self.logit_scale\n",
    "\n",
    "            # # # pos_marginal_prob = F.softmax(pos_logits, dim=1).mean(0)\n",
    "            # # # neg_marginal_prob = F.softmax(neg_logits, dim=1).mean(0)\n",
    "\n",
    "            # # # # # Get the logits with the highest average confidence\n",
    "\n",
    "            # # # if pos_marginal_prob.max() > neg_marginal_prob.max():\n",
    "            # # #     pred_class = pos_marginal_prob.argmax().item()\n",
    "            # # # else:\n",
    "            # # #     pred_class = neg_marginal_prob.argmax().item()\n",
    "\n",
    "            logits = torch.cat((pos_logits, neg_logits, image_features[-1:]), dim=0)\n",
    "            logits = logits @ self.text_features.T * self.logit_scale\n",
    "\n",
    "            logits = self.select_confident_samples(logits, top=0.1)[0]\n",
    "\n",
    "            marginal_prob = F.softmax(logits, dim=1).mean(0)\n",
    "            pred_class = int(marginal_prob.argmax().item())\n",
    "\n",
    "        return pred_class\n",
    "\n",
    "\n",
    "# Create a ClipSkeleton instance\n",
    "wrapper_clip = ClipWrapper(\n",
    "    clip_model, class_labels=dataset.class_code_to_label, device=device  # type: ignore\n",
    ").to(device)\n",
    "\n",
    "bench(\n",
    "    wrapper_clip,\n",
    "    dataloader,\n",
    "    device,\n",
    "    reduce=1000,\n",
    "    comment=\"pca_no_bool_solo_da_positive\",\n",
    "    visualize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf79a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataloader:\n",
    "    x = image[-1]\n",
    "\n",
    "    #  # visualize the original image\n",
    "\n",
    "    x = x.numpy().transpose(1, 2, 0)\n",
    "    plt.imshow(x)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "    # plt.imshow(x[-1].cpu().numpy().astype(np.uint8).transpose(1, 2, 0))\n",
    "    # plt.title(\"Original Image\")\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a21c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
